# (PART) Cours II: modélisation {-}

# Régression linéaire I {#lm}

```{r setup, include=FALSE, echo=FALSE, message=FALSE, results='hide'}
knitr::opts_chunk$set(comment = '#', fig.align = "center")
SciViews::R
library(modelr)
```

##### Objectifs {-}

- Retrouver ses marques avec R, RStudio et la SciViews Box et découvrir les fonctions supplémentaires de la nouvelle version.

- Découvrir la régression linaire de manière intuitive.

- Découvrir les outils de diagnostic de la régression linéaire, en particulier l'analyse des résidus.


##### Prérequis {-}

Avant de nous lancer tête baissée dans de la matière nouvelle, nous allons **installer la dernière version de la SciViews Box**. Une nouvelle version est disponible chaque année début septembre. Reportez-vous à l'appendice \@ref(svbox) pour son installation et pour la migration éventuelle de vos projets depuis la version précédente. Une fois la box installée, consacrez un petit quart d'heure à repérer les icônes nouvelles dans le Dock et dans le menu `Applications`. Vous retrouverez R et RStudio, mais dans des version plus récentes qui apportent également leur lot de nouveautés. Lancez RStudio et repérez ici aussi les nouveaux onglets et les nouvelles entrées de menu. Aidez-vous de l'aide en ligne ou de recherches sur le net pour vous familiariser avec ces nouvelles fonctionnalités.

Une fois la nouvelle SciViews Box fonctionnelle sur votre ordinateur, vous allez réaliser une séance d'exercice couvrant les points essentiels des notions abordées dans le livre [science des données biologiques partie 1](http://biodatascience-course.sciviews.org/sdd-umons/), histoire de rafraîchir vos connaissances. Les tutoriaux `learnr` auxquels vous êtes maintenant habitués seront là pour vous aider à auto-évaluer votre progression. Pour le cours 2, ces tutoriaux sont dans le package `BioDataScience2` que vous venez normalement d'installer si vous avez bien suivi toutes les instructions de configuration de votre SciViews Box (sinon, vérifiez votre configuration).

```{block2, type='bdd'}
Démarrez la SciViews Box et RStudio. Dans la fenêtre **Console** de RStudio, entrez l'instruction suivante suivie de la touche `Entrée` pour ouvrir le tutoriel concernant les bases de R\ :

    BioDataScience2::run("01a_rappel")

N’oubliez pas d’appuyer sur la touche `ESC` pour reprendre la main dans R à la fin d’un tutoriel dans la console R.
```


## Modèle 

**Qu'est-ce qu'un "modèle" en science des données et en statistique\ ?** Il s'agit d'une représentation *simplifiée* sous forme mathématique du mécanisme responsable de la distribution des observations.

Rassurez-vous, dans ce module, le côté mathématique du problème sera volontairement peu développé pour laisser une large place à une compréhension *intuitive* du modèle. Les seules notions clés à connaître ici concernent l'équation qui définit une droite quelconque dans le plan $xy$\ :

$$y = a \ x + b$$

Cette équation comporte\ :

- deux **variables** $x$ et $y$ qui sont matérialisées par les axes des abscisses et des ordonnées dans le plan $xy$. Ces variables prennent des valeurs bien définies pour les *observations* réalisées sur chaque *individu* du jeu de données.

- deux **paramètres** $a$ et $b$, respectivement la pente de la droite ($a$) et son ordonnée à l'origine ($b$). Ecrit de la sorte, $a$ et $b$ peuvent prendre n'importe quelle valeur et l'équation définit de manière généraliste *toutes* les droites possibles qui existent dans le plan $xy$. *Paramétrer* ou *paramétriser* le modèle consiste à définir **une et une seule droite** en fixant les valeurs de $a$ et de $b$. Par exemple, si je décide de fixer $a = 0.35$ et $b = -1.23$, mon équation définit maintenant **une droite bien précise** dans le plan $xy$\ :

$$y = 0,35 \ x - 1.23$$

```{block2, type='warning'}
La distinction entre *variable* et *paramètre* dans les équations précédentes semble difficile pour certaines personnes. C'est pourtant crucial de pouvoir le faire pour bien comprendre la suite. Alors, c'est le bon moment de **relire attentivement** ce qui est écrit ci-dessus et de le mémoriser avant d'aller plus avant\ !
```


### Pourquoi modéliser ?

Le but de la modélisation consiste à découvrir l'équation mathématique de la droite (ou plus généralement, de la fonction) qui décrit au mieux la forme du nuage de points matérialisant les observations dans le plan $xy$ (ou plus généralement dans un *hyper-espace* représenté par les différentes variables mesurées). Cette équation mathématique peut ensuite être utilisée de différentes façons, toutes plus utiles les unes que les autres\ :

- Aide à la **compréhension du mécanisme sous-jacent** qui a généré les données. Par exemple, si une droite représente bien la croissance pondérale d'un organisme dans le plan représenté par le logarithme du poids (`P`) en ordonnée et le temps (`t`) en abscisse, nous pourrons déduire que la croissance de cet organisme est probablement un mécanisme de type exponentiel (puisqu'une transformation inverse, c'est-à-dire logarithmique, linéarise alors le nuage de points). Attention\ ! Le modèle *n'est pos* le mécanisme sous-jacent de génération des données, mais utilisé habilement, ce modèle peut donner des *indices utiles* pour aider à découvrir ce mécanisme.

- Effectuer des **prédictions**. Le modèle paramétré pourra être utilisé pour prédire, par exemple, le poids probable d'un individu de la même population après un certain laps de temps.

- **Comparer différents modèles**. En présence de plusieurs populations, nous pourrons ajuster un modèle linéaire pour chacune d'elles et comparer ensuite les pentes des droites pour déterminer quelle population a le meilleur ou le moins bon taux de croissance.

- **Explorer les relations** entre variables. Sans aucunes connaissances sur le contexte qui a permit d'obtenir nos données, un modèle peut fournir des informations utiles pour orienter les recherches futures.

Idéalement, un modèle devrait pouvoir servir à ces différentes applications. En pratique, comme le modèle est forcément une simplification de la réalité, des compromis doivent être concédés pour arriver à cette simplification. En fonction de son usage, les compromis possibles vont différer. Il s'en suit une *spécialisation* des modèles en modèles **mécanistiques** qui décrivent particulièrement bien le mécanisme sous-jacent (fréquents en physique, par exemple), les modèles **prédictifs** conçus pour calculer des nouvelles valeurs (que l'intelligence artificielle affectionne particulièrement), les modèles **comparatifs**, et enfin, les modèles **exploratroires** (utilisés dans la phase initiale de découverte et de description des données). Retenez simplement qu'un même modèle est rarement efficace sur les quatre tableaux simultanément.


### Quand modéliser ?

A chaque fois que deux ou plusieurs variables (quantitatives dans le cas de la régression) forment un nuage de points qui présente une forme particulière non sphérique, autrement dit, qu'une **corrélation significative** existe dans les données, un modèle peut être utile.

Etant donné deux variables quantitatives, trois niveaux d'association de force croissante peuvent être définies entre ces deux variables\ :

- La **corrélation** quantifie juste l'allongement dans une direction préférentielle du nuage de points à l'aide des coefficients de corrélation linéaire de **Pearson** ou non linéaire de **Spearman**. Ce niveau d'association a été traité dans le [module 12 du cours 1](http://biodatascience-course.sciviews.org/sdd-umons/association-de-deux-variables.html). Il est purement descriptif et n'implique aucunes autres hypothèses sur les données observées.

- La **relation** considère que la corrélation observée entre les deux variables est issue d'un mécanisme sous-jacent qui nous intéresse. Un **modèle mathématique** de l'association entre les deux variables matérialise de manière éventuellement simplifiée, ce mécanisme. Il permet de réaliser ensuite des calculs utiles. Nous verrons plus loin que des contraintes plus fortes doivent être supposées concernant le distribution des deux variables.

- La **causalité** précise encore le mécanisme sous-jacent dans le sens qu'elle exprime le fait que c'est la variation de l'une de ces variables qui est directement ou indirectement la **cause** de la variation de la seconde variable. Bien que des outils statistiques existent pour inférer une causalité (nous ne les aborderons pas dans ce cours), la causalité est plutôt étudiée via l'**expérimentation**\ : le biologiste *contrôle* et *fait varier* la variable supposée causale, toutes autres conditions par ailleurs invariables dans l'expérience. Il mesure alors et constate si la seconde variable répond ou non à ces variations^[En biologie, le vivant peut être étudié essentiellement de deux manières complémentaires\ : par l'**observation** du monde qui nous entoure sans interférer, ou le moins possible, et par l'**expérimentation** où le biologiste fixe alors très précisément les conditions dans lesquelles il étudie ses organismes cibles. Les deux approches se prêtent à la modélisation *mais seule l'expérimentation permet d'inférer avec certitude la causalité*.] et en déduit une causalité éventuelle.

La distinction entre ces trois degrés d'association de deux variables est cruciale. Il est fréquent d'observer une confusion entre corrélation (ou relation) et causalité chez ceux qui ne comprennent pas bien la différence. Cela peut mener à des **interprétations complètement erronées\ !** Comme ceci est à la fois crucial mais subtil, voici une vidéo issue de la série "les statistiques expliquées à mon chat" qui explique clairement le problème. Une troisième **variable confondante** peut en effet expliquer une corrélation, rendant alors la relation et/ou la causalité entre les deux variables fallacieuse...

```{r, echo=FALSE}
vembedr::embed_youtube("aOX0pIwBCvw", width = 770, height = 433)
```


### Entraînement et confirmation

En statistique, une règle universelle veut qu'une observation ne peut servir qu'une seule fois. Ainsi, toutes les données utilisées pour calculer le modèle ne *peuvent pas servir simultanément* à la confirmer. Il faut échantillonner d'autres valeurs pour effectuer cette confirmation. Il s'en suit une spécialisation des jeux de données en\ :

- jeu d'**entraînement** qui sert à établir le modèle

- jeu de **confirmation** ou de **test** qui sert à vérifier que le modèle est *génaralisable* car il est capable de prédire le comportement d'un *autre* jeu de données indépendant issu de la même population statistique.

```{block2, type='note'}
C'est une pratique cruciale de toujours confirmer son modèle, et donc, de prendre soin de séparer ses données en jeu d'entraînement et de test. Les bonnes façons de faire cela seront abordées au cours 3 dans la partie consacrée à l'apprentissage machine. Ici, nous nous focaliserons uniquement sur l'établissement du modèle dans la phase d'entraînement. Par conséquent, nous utiliserons toutes nos données pour cet entraînement, mais qu'il soit d'emblée bien clair qu'une confirmation du modèle est une seconde phase également indispensable.
```


## Régression linéaire simple

Nous allons découvrir les bases de la régression linéaire de façon intuitive. Nous utilisons le jeu de données `trees` qui rassemble la mesure du diamètre, de la hauteur et du volume de bois de cerisiers noirs. 

```{r}
# importation des données
trees <- read("trees", package = "datasets", lang = "fr")
```

Rapellons-nous que dans le [chapitre 12 du livre science des données 1](http://biodatascience-course.sciviews.org/sdd-umons/association-de-deux-variables.html), nous avons étudié l'association de deux variables quantitatives (ou numériques). Nous utilisons donc une matrice de corrélation afin de mettre en évidence la corrélation entre nos trois variables qui composent le jeu de donnée `trees`.

La fonction `correlation()` nous renvoie un tableau de la matrice de correlation avec l'indice de Pearson (corrélation linéaire) par défaut. C'est précisement ce coefficient qui nous intéresse dans le cadre d'une régression *linéaire* comme description préalable des données autant que pour nous guider dans le choix de nos variables.

```{r}
(trees_corr <- correlation(trees))
```

Nous pouvons également observer cette matrice sous la forme d'un graphique plus convivial.

```{r}
plot(trees_corr, type = "lower")
```

Cependant, n'oubliez pas qu'il est indispensable de visualiser les nuages de points pour ne pas tomber dans le piège mis en avant par le [jeu de données artificiel appelé “quartet d’Anscombe”](http://biodatascience-course.sciviews.org/sdd-umons/association-de-deux-variables.html#importance-des-graphiques) qui montre très bien comment des données très différentes peuvent avoir même moyenne, même variance et même coefficient de corrélation. Un graphique de type matrice de nuages de points est tout indiqué ici.

```{r}
GGally::ggscatmat(as.data.frame(trees), 1:3)
```

Nous observons une plus forte corrélation linéaire entre le volume et le diamètre. Intéressons nous à cette association.

```{r}
chart(trees, volume ~ diameter) +
  geom_point()
```

Si vous deviez ajouter une droite permettant de représenter au mieux les données, où est ce que vous la placeriez ? 

Pour rappel, une droite respecte l'équation mathématique suivante\ :

$$y = a \ x + b$$ 

dont `a` est la pente (*slope* en anglais) et `b` est l'ordonnée à l'origine (*intercept* en anglais).

```{r}
# Sélection de pentes et d'ordonnées à l'origine
models <- tibble(
  model = paste("model", 1:4, sep = "-"),
  slope = c(5, 5.5, 6, 0),
  intercept = c(-0.5, -0.95, -1.5, 0.85)
)

chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(data = models,
    aes(slope = slope, intercept = intercept, color = model)) + 
  labs( color = "Modèle")
```

Nous avons quatre droites candidates pour représenter au mieux les observations. Quel est la meilleure d'entre elles selon vous\ ? 


### Quantifier l'ajustement d'un modèle

Nous voulons identifier la meilleure régression, c'est-à-dire la régression le plus proche de nos données. Nous avons besoin d'une règle qui va nous permettre de quantifier la distance de nos observations à notre modèle afin d'obtenir la régression avec la plus faible distance possible de l'ensemble de nos observations. 

Décomposons le problème étape par étape et intéressons nous au `model-1` (droite en rouge sur le graphique précédent).

- Calculer les valeurs de $y_i$ **prédites par le modèle** que nous noterons par convention $\hat y_i$ (prononcez "y chapeau" ou *"y hat"* en anglais) pour chaque observation $i$.


```{r}
# Calculer la valeur de y pour chaque valeur de x suivant le model souhaité
# Création de notre fonction 
model <- function(slope, intercept, x) {
  prediction <- intercept + slope * x
  attributes(prediction) <- NULL
  prediction
}
# Application de notre fonction 
yhat <- model(slope = 5, intercept = -0.5, x = trees$diameter)
# Affichage des résultats
yhat
```

- Calculer la distance entre les observations $y_i$ et les prédictions par notre modèle $\hat y_i$, soit $y_i - \hat y_i$

Les distances que nous souhaitons calculer, sont appelées les **résidus** du modèle et sont notés $\epsilon_i$ (epsilon). Nous pouvons premièrement visualiser ces résidus graphiquement (ici en rouge par rapport à `model-1`)\ :

```{r, echo = FALSE}
chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(slope = 5, intercept = -0.5) +
  geom_segment(
    aes(x = diameter, y = volume, 
      xend = diameter, yend = yhat,
      color = "red")) +
  guides(color = FALSE)
```

Nous pouvons ensuite facilement calculer leurs valeurs comme ci-dessous\ : 

```{r}
# Calculer la distance entre y et y barre
# Création de notre fonction de calcul des résidus
distance <- function(observations, predictions) {
  residus <- observations - predictions
  attributes(residus) <- NULL
  residus
}
# Utilisation de la fonction
resid <- distance(observations = trees$volume, predictions = yhat)
# Impression des résultats
resid
```

- Définir une règle pour obtenir une valeur unique qui résume l'ensemble des distances de nos observations par rapport aux prédictions du modèle. Une première idée serait de sommer l'ensemble de nos résidus comme ci-dessous\ :

```{r}
sum(resid)
```

Appliquons ces calculs sur nos quatre modèles afin de les comparer... Le modèle pour lequel notree critère serait le plus proche de zéro serait alors considéré comme le meilleur.

```{r, echo = FALSE}
dist_calc <- list()
for (i in 1:nrow(models)) {
  dist_calc[i] <- sum(
  distance(
    observations = trees$volume, 
    predictions =  model(slope = models$slope[i], intercept = models$intercept[i],
      x = trees$diameter)))
}
names(dist_calc) <- models$model
knitr::kable(as_tibble(dist_calc))
```

Selon notre méthode, il en ressort que le modèle 4 est le plus approprié pour représenter au mieux nos données. Qu'en pensez vous\ ?

```{r, echo = FALSE}
chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + 
  labs( color = "Modèle")
```

Intuitivement, nous nous aperçevons que le modèle 4 est loin d'être le meilleur. **Nous pouvons en déduire que la somme des résidus n'est pas un bon critère pour ajuster un modèle linéaire.** 

Le problème lorsque nous sommons des résidus, est la présence de résidus positifs et de résidus négatifs (ici par rapport à `model-4`).

```{r, echo = FALSE}
yhat4 <- model(
  slope = models$slope[4],
  intercept = models$intercept[4],
  x = trees$diameter)

chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(data = models, 
    aes(slope = slope[4], intercept = intercept[4])) +
  geom_segment(
    aes(x = diameter, y = volume, 
      xend = diameter, yend = yhat4,
      colour = "red")) + # Does not work, why? colour = yhat4 > 0)) +
  geom_label(aes(x = 0.3, y = 1.5),
    label = "Résidus positifs", color = "red") +
  geom_label(aes(x = 0.45, y = 0.5), 
    label = "Résidus négatifs", color = "red") +
  guides(colour = FALSE)
```

Ainsi avec notre première méthode naïve de somme des résidus, il suffit d'avoir autant de résidus positifs que négatifs pour avoir un résultat proche de zéro. Mais cela n'implique pas que les observations soient prochent de la droite pour autant. Avez-vous une autre idée que de sommer les résidus\ ?

- **Sommer le carré des résidus** aurait des propriétés intéressantes car d'une part les carrés de nombres positifs et négatifs sont tous positifs, et d'autre part, plus une observation est éloignée plus sa distance au carré pèse fortement dans la somme^[Utiliser le carré des résidus a aussi d'autres propriétés statistiques intéressantes qui rapprochent ce calcul de la variance (qui vaut la somme de la distance au carré à la moyenne pour une seule variables numérique).].

Nous obtenons les résultats suivants\ :

```{r, echo = FALSE}
dist_calc <- list()
for (i in 1:nrow(models)) {
  dist_calc[i] <- sum(
  distance(
    observations = trees$volume, 
    predictions =  model(slope = models$slope[i], intercept = models$intercept[i],
      x = trees$diameter))^2)
}
names(dist_calc) <- models$model
knitr::kable(as_tibble(dist_calc))
```

- **Sommer les valeurs absolues des résidus** mène également à des contributions toutes positives, mais sans pénaliser outre mesure les observations les plus éloignées.

Nous obtenons les résultats suivants\ :

```{r, echo = FALSE}
dist_calc <- list()
for (i in 1:nrow(models)) {
  dist_calc[i] <- sum(abs(
  distance(
    observations = trees$volume, 
    predictions =  model(slope = models$slope[i], intercept = models$intercept[i],
      x = trees$diameter))))
}
names(dist_calc) <- models$model
knitr::kable(as_tibble(dist_calc))
```

Nous avons trouvé deux solutions intéressantes pour quantifier la distance de notre droite par rapport à nos observations. En effet, dans les deux cas, la valeur minimale est obtenue pour le `model-2` (en vert sur le graphique) qui est visuellement le meilleur des quatre.

```{block2, type='note'}
La méthode utilisant les carrés des résidus s'appelle une **régression par les moindres carrés**. Notre objectif est donc de trouver les meilleures valeurs des paramètres $a$ et $b$ de la droite pour minimiser ce critère. Il en résulte une fonction dite **objective** qui dépend de $x$ et de nos paramètres $a$ et $b$ à minimiser. Cette approche s'appelle la **régression par les moindres carrés** et elle est la plus utilisée.

L'approche utilisant la somme de la valeur absolue des résidus est également utilisable (et elle est d'ailleurs préférable en présence de valeurs extrêmes potentiellement suspectes). Elle s'apppelle **régression par la médiane**, un cas particulier de la **régression quantile**, une approche intéressante dans le cas de non normalité des résidus et/ou de présence de valeurs extrêmes suspectes.
```


### Trouver la meilleure droite 

```{block2, type='bdd'}
Essayez de trouver le meilleur modèle par vous-même dans une application interactive "shiny". Démarrez la SciViews Box et RStudio. Dans la fenêtre **Console** de RStudio, entrez l'instruction suivante suivie de la touche `Entrée` pour ouvrir l'application :

    BioDataScience2::app("01a_lin_mod") # TODO

Méthode alternative\ :

    shiny::runApp(system.file("shiny/01a_lin_mod", package = "BioDataScience2"))

N’oubliez pas d’appuyer sur la touche `ESC` pour reprendre la main dans R lorsque vous aurez fini avec l'application shiny.
```

Nous pouvons nous demander si notre modèle 2 qui est la meilleure droite de nos quatre modèles est le **meilleur modèle possible dans l'absolu**. Pour se faire nous allons devoir définir unez technique d'optimisation qui nous permet de déterminer quelle est la droite qui minimise notre fonction objective. Dans la suite, nous garderonsq le critère des moindres carrés (des résidus).

Imaginons que nous n'avons pas quatre mais 5000 modèles linéaires avec des pentes et des ordonnées à l'origine différentes. Quelle est la meilleure droite\ ? 

```{r}
set.seed(34643)
models1 <- tibble(
  intercept = runif(5000, -5, 4),
  slope = runif(5000, -5, 15))

chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(aes(intercept = intercept, slope = slope),
    data = models1, alpha = 1/6) 
```

Nous voyons sur le graphique qu'un grand nombre de droites différentes sont testées, mais nous ne distinguons pas grand chose de plus. Cependant, sur ces  `r nrow(models1)`  modèles, nous pouvons maintenant calculer la somme des carrés des résidus et ensuite déterminer quel est le meilleur d'entre eux.

```{r}
# Fonction de calcul de la somme des carrés des résidus
measure_distance <- function(slope, intercept, x, y) {
   ybar <- x * slope + intercept
   resid <- y - ybar
   sum(resid^2)
}
# Test de la fonction
#measure_distance(slope = 0, intercept = 0.85, x = trees$diameter, y = trees$volume)

# Fonction adaptée pour être employé avec purrr:map() pour distribuer le calcul
trees_dist <- function(intercept, slope) {
  measure_distance(slope = slope, intercept = intercept,
    x = trees$diameter, y = trees$volume)
}

models1 <- models1 %>%
  mutate(dist = purrr::map2_dbl(intercept, slope, trees_dist))
```

Si nous réalisons un graphique de valeurs de pentes, d'ordonnées à l'origine et de la valeur de la fonction objective (distance) en couleur, nous obtenons le graphique ci-dessous.

```{r}
plot <- chart(models1, slope ~ intercept %col=%  dist) +
  geom_point() +
  geom_point(data = filter(models1, rank(dist) <= 10),
    shape = 1, color = 'red', size = 3) +
  labs( y = "Pente", x = "Ordonnée à l'origine", color = "Distance") +
  scale_color_viridis_c(direction = -1)
plotly::ggplotly(plot)
```

Les 10 valeurs les plus faibles sont mises en évidence sur le graphique par des cercles rouges. Le modèle optimal que nous recherchons se trouve dans cette région.

```{r}
best_models <- models1 %>.%
  filter(., rank(dist) <= 10)
```


Nous pouvons afficher les `r nrow(best_models)` meilleurs modèles sur notre graphique\ :

```{r}
chart(trees, volume ~ diameter) +
  geom_abline(data = best_models, 
    aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) +
  geom_point() +
  labs(color = "Distance")  +
  scale_color_viridis_c(direction = -1)
```

**En résumé, nous avons besoin d'une fonction qui calcule la distance d'un modèle par rapport à nos observations et d'un algorithme pour la minimiser.** 

Il n'est cependant pas nécessaire de chercher pendant des heures la meilleure fonction et le meilleur algorithme. Il existe dans R, un outil spécifiquement conçu pour adapter des modèles linéaires sur des observations, la fonction `lm()`. Dans le cas particulier de la régression par les moindres carrés, la solution s'obtient très facilement par un simple calcul\ :

$$a = \frac{cov_{x, y}}{var_x} \ \ \ \textrm{et} \ \ \ b = \bar y - a \ \bar x$$

où $\bar x$ et $\bar y$ sont les moyennes pour les deux variables, $cov$ est la covariance et $var$ est la variance.

```{block2, type='note'}
Vous avez à votre disposition des snippets dédiés aux modèles linéaires (tapez `...`, ensuite choisissez `models`, ensuite `models : linear` et choisissez le snippet qui vous convient dans la liste.
```

```{r}
(lm. <- lm(data = trees, volume ~ diameter))
```

Nous pouvons reporter ces valeurs sur notre graphique afin d'observer ce résultat par rapport à nos modèles aléatoires. nous avons en rouge la droite calculée par la fonction `lm()`. 

```{r}
chart(trees, volume ~ diameter) +
  geom_abline(data = best_models, 
    aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) +
  geom_point() + 
  geom_abline(
    aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), 
    color = "red", size = 1.5) +
  labs( color = "Distance")  +
  scale_color_viridis_c(direction = -1)
```


### La fonction `lm()`

Suite à notre découverte de manière intuitive de la régression linéaire simple, nous pouvons récapituler quelques points clés\ : 

- Une droite suit l'équation mathématique suivante :

$$y = a \ x + b$$
dont $a$ est la pente (*slope* en anglais) et $b$ est l'ordonnée à l'origine (*intercept* en anglais), tous deux les **paramètres** du modèle, alors que $x$ et $y$ en sont les **variables**.

- La distance entre une valeur observée $y_i$ et une valeur prédite $\hat y_i$ se nomme le résidu ($\epsilon_i$) et **se mesure toujours parallèlement** à l'axe $y$. Cela revient à considérer que toute l'erreur du modèle se situe sur $y$ et non sur $x$. Cela donne l'équation complète de notre modèle statistique\ :

$$y_i = a \ x_i + b + \epsilon_i$$

avec $y_i$ est la valeur mesurée pour le point i sur l'axe y, $a$ est la pente, $x_i$  est la valeur mesurée pour le point i sur l'axe x, `b` est l'ordonnée à l'origine et $\epsilon_i$ les résidus. On peut montrer (nous ne le ferons pas ici pour limiter les développements mathématiques) que le choix des moindres carrés des résidus comme fonction objective revient à considérer que nos résidus suivent **une distribution normale** centrée autour de zéro et avec un écart type $\sigma$ constant/ :

$$\epsilon_i \approx N(0, \sigma)$$

- Dans le cas de la régression linéaire simple, la meilleure droite s'obtient très facilement par la minimisation de la somme des carrés des résidus. En effet, la pente $a = \frac{cov_{x, y}}{var_x}$ et l'ordonnée à l'origine $b = \bar y - a \ \bar x$.

- La fonction `lm()` permet de faire ce calcul très facilement dans R.


```{r}
# Régression linéaire
lm. <- lm(data = trees, volume ~ diameter)

# Graphique de nos observations et de la droite obtenue avec la fonction lm()
chart(trees, volume ~ diameter) +
  geom_point() + 
  geom_abline(
    aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), 
    color = "red", size = 1.5) +
  labs( color = "Modèle")  +
  scale_color_viridis_c(direction = -1)
```

La fonction `lm()` crée un objet spécifique qui contient de nombreuses informations pour pouvoir ensuite analyser notre modèle linéaire. La fonction `class()` permet de mettre en avant la classe de notre objet.

```{r}
class(lm.)
```


### Résumé avec `summary()`

Avec la fonction `summary()` nous obtenons un résumé condensé des informations les plus utiles pour interpréter notre régression linéaire. 

```{r}
summary(lm.)
```


- Call\ : 

Il s'agit de la formule employée dans la fonction `lm()`. C'est à dire le volume en fonction du diamètre du jeu de données `trees`.

- Residuals\ :

La tableau nous fournit un résumé via les 5 nombres de l'ensemble des résidus (que vous pouvez récupérer à partir de `lm.$residuals`).

```{r}
fivenum(lm.$residuals)
```


- Coefficients\ :

Il s'agit des résultats associés à la pente et à l'ordonnée à l'origine dont les valeurs estimées des paramètres *(Estimate*). Les mêmes valeurs peuvent être obtenues à partir de `lm.$coefficients`\ :

```{r}
lm.$coefficients
```

On retrouve également les écart-types calculés sur ces valeurs (*Std.Error*) qui donnent une indication de la précision de leur estimation, les valeurs des distibutions de Student sur les valeurs estimées (*t value*) et enfin les valeurs p (*PR(>|t|)*) liées à un test de Student pour déterminer si le paramètre `p` correspondant est significativement différent de zéro (avec $H_0: p = 0$ et $H_a: p \neq 0$).

*Pour l'instant, nous nous contenterons d'interpréter et d'utiliser les informations issues de `summary()` dans sa partie supérieure. Le contenu des trois dernières lignes sera détaillé dans le module suivant.*

A partir des données de ce résumé, nous pouvons maintenant **paramétrer** l'équation de notre modèle\ :

$$y = ax + b$$

devient^[Lors de la paramétrisation du modèle, pensez à arrondir la valeur des paramètres à un nombre de chiffres significatifs raisonnables. Inutile de garder 5, ou même 3 chiffres derrière la virgule si vous n'avez que quelques dizaines d'obserrvations pour ajuster votre modèle.]\ :

$$volume \ de \ bois = 5.65 \ diamètre \ à \ 1.4 \ m  - 1.05 $$


## Outils de diagnostic

Une fois la meilleure droite de régression obtenue, le travail est loin d'être terminé. Il se peut que le nuage de point ne soit pas tout-à-fait linéaire, que sa dispersion ne soit pas homogène, que les résidus n'aient pas une distribution normale, qu'il existe des valeurs extrêmes aberrantes, ou qui tirent la droite vers elle de manière excessive.

Nous allons maintenant devoir diagnostiquer ces possibles problèmes. L'**analyse des résidus** permet de le faire. Ensuite, si deux ou plusieurs modèles sont utilisable, il nous faut décider lequel conserver. Enfin, nous pouvons aussi calculer et visualiser l'**enveloppe de confiance** 
du modèle et extraire une série de données de ce modèle.


### Analyse des résidus

Le tableau numérique obtenu à l'aide de `summary()` peut faire penser que l'étude d'une régression linéaire se limite à quelques valeurs numériques et divers tests d'hypothèses associés. C'est un premier pas, mais c'est oublier que la technique est **essentiellement visuelle**. Le graphique du nuage de points avec la droite superposée est un premier outil diagnostic visuel indispensable, mais il n'est pas le seul\ ! Plusieurs graphiques spécifiques existent pour mettre en évidence diverses propriétés des résidus qui peuvent révéler des problèmes. Leur inspection est indispensable et s'appelle l'**analyse des résidus**. Les différents graphiques sont faciles à obtenir à partir des snippets.

Le premier de ces graphique permet de vérifier la distribution homogène des résidus. Dans une bonne régression, nous aurons une distribution équilibrée de part et d'autre du zéro sur l'axe Y. Que pensez-vous de notre graphique d'anayse des résidus\ ? Nous avons une valeur plus éloignée du zéro qui est mise en avant par la courbe en bleu qui montre l'influence générale des résidus. 

```{r}
#plot(lm., which = 1)
lm. %>.%
  chart(broom::augment(.), .resid ~ .fitted) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(se = FALSE, method = "loess", formula = y ~ x) +
  labs(x = "Fitted values", y = "Residuals") +
  ggtitle("Residuals vs Fitted")
```

Le second graphique permet de vérifier la normalité des résidus (comparaison par graphique quantile-quantile à une distribution normale).

```{r}
#plot(lm., which = 2)
lm. %>.%
  chart(broom::augment(.), aes(sample = .std.resid)) +
  geom_qq() +
  geom_qq_line(colour = "darkgray") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals") +
  ggtitle("Normal Q-Q")
```

Le troisième graphique va standardiser les résidus, et surtout, en prendre la racine carrée. Cela a pour effet de superposer les résidus négatifs sur les résidus positifs. Nous y diagnostiquons beaucoup plus facilement des problèmes de distribution de ces résidus. A nouveau, nous pouvons observer qu'une valeur influence fortement la régression.

```{r}
#plot(lm., which = 3)
lm. %>.%
  chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) +
  geom_point() +
  geom_smooth(se = FALSE, method = "loess", formula = y ~ x) +
  labs(x = "Fitted values",
    y = expression(bold(sqrt(abs("Standardized residuals"))))) +
  ggtitle("Scale-Location")
```

Le quatrième graphique met en évidence l'influence des individus sur la régression linéaire. Effectivement, la régression linéaire est sensible aux valeurs extrêmes. Nous pouvons observer que nous avons une valeur qui influence fortement notre régression. On utilise pour ce faire la distance de Cook que nous ne détaillerons pas dans le cadre de ce cours.

```{r}
#plot(lm., which = 4)
lm. %>.%
  chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = "darkgray") +
  labs(x = "Obs. number", y = "Cook's distance") +
  ggtitle("Cook's distance")
```

Le cinquième graphique utilise l'effet de levier (*Leverage*) qui met également en avant l'influence des individus sur notre régression. Il répond à la question suivante\ : "est-ce qu'un ou plusieurs points sont tellement influents qu'ils tirent la régression vers eux de manière abusive\ ?" Nous avons à nouveau une valeur qui influence fortement notre modèle.

```{r}
#plot(lm., which = 5)
lm. %>.%
  chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) +
  geom_point() +
  geom_smooth(se = FALSE, size = 0.5, method = "loess", formula = y ~ x) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  labs(x = "Leverage", y = "Standardized residuals") +
  ggtitle("Residuals vs Leverage")
```

Le sixième graphique met en relation la distance de Cooks et l'effet de levier. Notre unique point d'une valeur supérieur à 0.5 m de diamètre influence fortement notre modèle.

```{r}
#plot(lm., which = 6)
lm. %>.%
  chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) +
  geom_point() +
  geom_vline(xintercept = 0, colour = NA) +
  geom_abline(slope = seq(0, 3, by = 0.5), colour = "darkgray") +
  geom_smooth(se = FALSE, size = 0.5, method = "loess", formula = y ~ x) +
  labs(x = expression("Leverage h"[ii]), y = "Cook's distance") +
  ggtitle(expression("Cook's dist vs Leverage h"[ii] / (1 - h[ii])))
```

A l'issue de l'analyse des résidus, nous abservons donc différents problèmes qui suggèrent que le modèle choisi n'est peut être pas le plus adapté. Nous comprendrons pourquoi plus loin.

##### A vous de jouer {-}

```{block2, type='bdd'}
Démarrez la SciViews Box et RStudio. Dans la fenêtre **Console** de RStudio, entrez l'instruction suivante suivie de la touche `Entrée` pour ouvrir le tutoriel concernant les bases de R\ :

    BioDataScience2::run("01b_reg_lin")

N’oubliez pas d’appuyer sur la touche `ESC` pour reprendre la main dans R à la fin d’un tutoriel dans la console R.
```


##### Pièges et astuces : extrapolation {-}

Notre régression linéaire a été réalisée sur des cerisiers noirs dont le diamètre est compris entre `r min(trees$diameter)`  et `r max(trees$diameter)` mètre. Pensez vous qu'il soit acceptable de prédire des volumes de bois pour des arbres dont le diamètre est inférieur ou supérieur à nos valeurs minimales et maximales mesurées (extrapolation)\ ?

Utilisons notre régression linéaire afin de prédire 10 volumes de bois à partir d'arbre dont le diamètre varie entre 0.1 et 0.8m.

```{r}
new <- data.frame(diameter = seq(0.1, 0.7, length.out = 8))
```

Ajoutons une variable `pred` qui contient les prédictions en volume de bois. Observez-vous un problème particulier sur base du tableau ci-dessous\ ?

```{r}
new %>.%
  modelr::add_predictions(., lm.) -> new
new
```

Il est peut-être plus simple de voir le problème sur un nuage de points. Pour un diamètre de `r new$diameter[2]` m de diamètre, le volume de bois est de `r round(new$pred[2], 2)` mis en avant par l'intersection des lignes pointillées bleues.

```{r}
chart(trees, volume~diameter) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "grey") +
  geom_vline(xintercept = new$diameter[2], linetype = "twodash", color = "blue") +
  geom_hline(yintercept = new$pred[2], linetype = "twodash", color = "blue") +
  geom_point() +
  geom_abline(aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2])) +
  geom_point(data = new, f_aes(pred~diameter), color = "red") 

```

Le volume de bois prédit est négatif\ ! Notre modèle est-il alors complètement faux\ ? Rappelons-nous qu'un modèle est nécessairement une vision simplifiée de la réalité. En particulier, notre modèle a été entraîné avec des données comprises dans un intervalle. Il est alors valable pour effectuer des **interpolations** à l'intérieur de cet intervalle, mais **ne peut pas être utilisé pour effectuer des extrapolations** en dehors, comme nous venons de le faire.

```{r}
trees %>.%
  modelr::add_predictions(., lm.) -> trees

chart(trees, volume~diameter) +
  geom_point() +
  geom_line(f_aes(pred ~ diameter))

```


##### Pièges et astuces : significativité fortuite {-}

Gardez toujours à l'esprit qu'il est possible que votre jeu de données donne une régression significative, mais purement **fortuite**. Les données supplémentaires de test devraient alors démasquer le problème. D'où l'importance de vérifier/valider votre modèle.

Le **principe de parcimonie** veut que l'on ne teste pas toutes les combinaisons possibles deux à deux des variables d'un gros jeu de données, mais que l'on restreigne les explorations à des relations qui ont un sens biologique afin de minimiser le risque d'obtenir une telle régression de manière fortuite.


### Enveloppe de confiance

De même que l'on peut définir un intervalle de confiance dans lequel la moyenne d'un échantillon se situe avec une probabilité donnée, il est aussi possible de calculer et de tracer une **enveloppe de confiance** qui indique la région dans laquelle le "vrai" modèle se trouve avec une probabilité donnée (généralement, on choisi cette probabilité à 95%). Voici ce que cela donne\ :


```{r}
lm. %>.% (function(lm, model = lm[["model"]], vars = names(model))
  chart(model, aes_string(x = vars[2], y = vars[1])) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x))(.)
```

Cette enveloppe de confiance est en réalité basée sur l'écart type *conditionnel* (écart type de $y$ sachant quelle est la valeur de $x$) qui se calcule comme suit\ :


$$s_{y|x}\ =\ \sqrt{ \frac{\sum_{i = 0}^n\left(y_i - \hat y_i\right)^2}{n-2}}$$

A partir de là, il est possible de définir également un intervalle de confiance conditionnel à $x$\ :

$$CI_{1-\alpha}\ =\ \hat y_i\ \ \pm \  t_{\frac{\alpha}{2}}^{n-2}  \frac{s_{y|x}\ }{\sqrt{n}}$$

C'est cet intervalle de confiance conditionnel qui est matérialisé par l'enveloppe de confiance autour de la droite de régression représentée sur le graphique.


### Extraire les données d'un modèle

La fonction `tidy()` du package `broom` extrait facilement et rapidement sous la forme d'un tableau différentes valeurs associées à votre régression linéaire.

```{r}
(DF <- broom::tidy(lm.))
```


Pour extraire facilement et rapidement sous la forme d'un tableau de données les paramètres de votre modèle vouys pouvez aussi utiliser la fonction `glance()`. 

```{r}
(DF <- broom::glance(lm.))

```

Vous avez des snippets à votre disposition pour ces deux fonctions\ :

`... -> models ..m -> .models tools .mt -> .mtmoddf` ou encore `... -> models ..m -> .models tools .mt -> .mtpardf`

##### A vous de jouer {-}

```{block2, type='bdd'}

Vous avez à votre disposition la première assignation GitHub Classroom : 
  
- <https://classroom.github.com/a/bvqsukEO>
  
```
