# (PART) Cours II: analyse et modélisation {-}

# Modèle linéaire {#lm}

```{r setup, include=FALSE, echo=FALSE, message=FALSE, results='hide'}
knitr::opts_chunk$set(comment = '#', fig.align = "center")
SciViews::R
library(modelr)
```

##### Objectifs {-}

- TODO

##### Prérequis {-}

Avant de poursuivre, vous allez réaliser une séance d'exercice couvrant les points essentiels des notions abordées dans le livre [science des données biologiques partie 1](http://biodatascience-course.sciviews.org/sdd-umons/). 

```{block2, type='bdd'}
Démarrez la SciViews Box et RStudio. Dans la fenêtre **Console** de RStudio, entrez l'instruction suivante suivie de la touche `Entrée` pour ouvrir le tutoriel concernant les bases de R :

    BioDataScience2::run("01a_rappel")

N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R
```


## La régression linéaire

Nous allons découvrir les base du modèle linéaire de façon intuitive. Nous utilisons le jeu de données `trees` qui rassemble la mesure du diamètre, de la hauteur et du volume de bois de cerisiers noirs. 

```{r}
# importation des données
trees <- read("trees", package = "datasets", lang = "fr")
```

Rapellons nous que dans le [chapitre 12 du livre science des données 1](http://biodatascience-course.sciviews.org/sdd-umons/association-de-deux-variables.html), nous avons étudié l'association de deux variables numériques. Nous utilisons donc une matrice de corrélation afin de mettre en évidence la corrélation entre nos 3 variables qui composent le jeu de donnée `trees`.

La fonction `correlation()` nous renvoie un tableau de la matrice de correlation avec l'indice de Pearson.

```{r}
(trees_corr <- correlation(trees))
```

Nous pouvons également observer cette matrice sous la forme d'un graphique plus convivial.

```{r}
plot(trees_corr, type = "lower")
```

Cependant, n'oubliez pas qu'il est indispensable de visualiser les nuages de points pour ne pas tobmer dans le piège mis en avant par le [jeu de données artificiel appelé “quartet d’Anscombe”](http://biodatascience-course.sciviews.org/sdd-umons/association-de-deux-variables.html#importance-des-graphiques) qui montre très bien comment des données très différentes peuvent avoir même moyenne, même variance et même coefficient de corrélation.

```{r}
GGally::ggscatmat(as.data.frame(trees), 1:3)
```

Nous observons une forte corrélation linéaire entre le volume et la hauteur des cerisiers noirs.  Interessons nous à cette association.

```{r}
chart(trees, volume ~ diameter) +
  geom_point()
```

Si vous deviez ajouter une droite permettant de représenter au mieux les données, où est ce que vous la placeriez ? 

Une droite respecte l'équation mathématique suivante: $$ y = ax + b$$ 

dont  `a` est la pente (*slope* en anglais) et `b` est l'ordonnée à l'origine (*intercept* en anglais).

```{r}
# Sélection de pente et d'ordonnée à l'origine
models <- tibble(
  model = paste("mod", 1:4, sep = "-"),
  slope = c(5, 5.5, 6, 0),
  intercept = c(-0.5, -0.95, -1.5, 0.85)
)

chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + 
  labs( color = "Modèle")
```

Nous avons 4 droites qui veulent représenter au mieux les observations. Quel est le meilleur modèle selon vous ? 

### Quantifier la qualité d'un modèle

Nous voulons identifier le meilleur modèle, c'est à dire le modèle le plus proche de nos données. Nous avons besoin d'une règle qui va nous permettre de quantifier la distance de nos observations à notre modèle afin d'obtenir le modèle avec la plus faible distance possible de l'ensemble de nos observations. 

Décomposons le problème étape par étape et intéressons nous au `mod-1`

- Connaitre les valeurs de y prédite par le modèle 


```{r}
# Calculer la valeur de y pour chaque valeur de x suivant le model souhaité
## Création de notre fonction 
model <- function(slope, intercept, x) {
  prediction <- intercept + slope * x
  attributes(prediction) <- NULL
  prediction
}
## Application de notre fonction 
mod1 <- model(slope = 5, intercept = -0.5, x = trees$diameter)
## Affichage des résultats
mod1
```

- Connaitre la distance entre les observations mesurées en y et les observations prédites en y par le modèle

La distance que nous souhaitons calculé, il s'agit des `résidus`. Nous pouvons premièrement visualiser ces résidus graphiquement :

```{r}
chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(slope = 5, intercept = -0.5) +
  geom_segment(
    aes(x = diameter, y = volume, 
      xend = diameter, yend = mod1))
```

Nous pouvons ensuite facilement calculer cette distance comme ci-dessous : 

```{r}
# Calculer la distance entre y observé et y prédit
## création de notre fonction
distance <- function(observation, prediction) {
  diff <- observation - prediction
  attributes(diff) <- NULL
  diff
}
## Application de la fonction
dist1 <- distance(observation = trees$volume, prediction = mod1)
## affichage des résultats
dist1
```

- Définir une règle pour obtenir une valeur unique de la distance de nos observations mesurées en y par rapport aux observations prédites en y par le modèle

Une première idée serait de sommer l'ensemble de nos distances comme ci-dessous :

```{r}
sum(dist1)
```

Appliquons la suite d'étapes ci-dessus pour nos 4 modèles afin de les comparer

```{r, echo = FALSE}
dist_calc <- list()
for(i in 1:nrow(models)) {
  dist_calc[i] <- sum(
  distance(
    observation = trees$volume, 
    prediction =  model(slope = models$slope[i], intercept = models$intercept[i], x = trees$diameter)))
}
names(dist_calc) <- models$model
knitr::kable(as_tibble(dist_calc))
```

Selon notre méthode, il en ressort que le modèle 4 est le modèle le plus approprié pour représenter au mieux nos données. Qu'en pensez vous ? 

```{r}
chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + 
  labs( color = "Modèle")
```

Intuitivement, nous nous apperçevons que le modèle 4 n'est pas le meilleur modèle. Nous pouvons en déduire que la somme des résidus n'est pas la meilleur fonction pour ajuster un modèle linéaire à nos observations. 

Le problème lorsque nous sommons des résidus, est la présence de résidus positifs et de résidus négatifs.

```{r}
mod2 <- model(slope = models$slope[4], intercept = models$intercept[4], x = trees$diameter)

chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(data = models, 
    aes(slope = slope[4], intercept = intercept[4])) +
  geom_segment(
    aes(x = diameter, y = volume, 
      xend = diameter, yend = mod2)) +
  geom_label(aes(x = 0.3, y = 1.5),
    label = "Résidus postifis", color = "red") +
  geom_label(aes(x = 0.45, y = 0.5), 
    label = "Résidus postifis", color = "blue")
```

Avez vous une autre idée  que de sommer les résidus ?

- Sommer le carré des résidus 

Nous obtenons les résultats suivants :

```{r, echo = FALSE}
dist_calc <- list()
for(i in 1:nrow(models)) {
  dist_calc[i] <- sum(
  distance(
    observation = trees$volume, 
    prediction =  model(slope = models$slope[i], intercept = models$intercept[i], x = trees$diameter))^2)
}
names(dist_calc) <- models$model
knitr::kable(as_tibble(dist_calc))
```

- Sommer les valeurs absolues des résidus 

Nous obtenons les résultats suivants :

```{r, echo = FALSE}
dist_calc <- list()
for(i in 1:nrow(models)) {
  dist_calc[i] <- sum(abs(
  distance(
    observation = trees$volume, 
    prediction =  model(slope = models$slope[i], intercept = models$intercept[i], x = trees$diameter))))
}
names(dist_calc) <- models$model
knitr::kable(as_tibble(dist_calc))
```

Nous avons trouvé deux solutions intéressantes pour quantifier la distance de notre modèle par rapport à nos observation.

### Trouver le meilleur modèle linéaire 

Nous pouvons maintenant nous demander si notre modèle 2 qui est le meilleur modèle de nos 4 modèles est le meilleur modèle possible pour se faire nous allons devoir optimiser notre modèle afin d'avoir le meilleur modèle possible.

Nous allons prendre la somme du carré des résidus comme fonction à minimiser.

Imaginons que nous avons pas 4 mais 500 modèle linéaire avec une pente et une ordonnée à l'origine différente, quelle est la meilleure droite ? 

```{r}

models1 <- tibble(
  intercept = runif(500, -5, 5),
  slope = runif(500, 3, 8))

chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(aes(intercept = intercept, slope = slope), data = models1, alpha = 1/6) 

```

Sur ces 500 modèles nous pouvons calculer la somme des carré des résidus et observer quelle est le meilleur modèle.

```{r}

```


**En résumé, nous avons besoin d'une fonction qui calcule la distance d'un modèle par rapport à nos observations et d'un algorihtme pour la minimiser.** 

Il n'est cependant pas nécessaire de chercher pendant des heures la meilleure fonction et le meilleur algorithme. Il existe dans R, un outil spécifiquement conçu pour adapter des modèles linéaires sur des observations, la fonction `lm()`. 

Un snippet est mis à votre disposition pour ne pas devoir retenir `... -> ...m -> -> .ml`

```{r}
summary(lm. <- lm(data = trees, volume ~ diameter))
```

```{r}
summary(lm. <- lm(data = trees, volume ~ diameter))
lm. %>.% (function (lm, model = lm[["model"]], vars = names(model))
  chart(model, aes_string(x = vars[2], y = vars[1])) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x))(.)
```

```{r}
chart(trees, volume ~ diameter) +
  geom_point() +
  geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + 
  labs( color = "Modèle")
```


Il existe une multitude de méthode pour obtenir une valeur unique exprimant cette distance


## test 

optimisation par essai erreur. 

```{r}
library(modelr)

models <- tibble(
  a1= runif(500, -20, 40),
  a2 = runif(500, -5, 5))

ggplot(sim1, aes(x,y)) +
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/6) +
  geom_point()

(lm. <- lm(data = sim1, formula = y ~ x))

model1 <- function(a,data) {
  a[1] +data$x * a[2]
}

model1(c(7, 1.5), sim1)

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

measure_distance(c(7, 1.5), sim1)

sim1_dist <- function(a1,a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>%
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

plotly::plot_ly(x=models$a1, y=models$a2, z=models$dist, type="scatter3d",  color = models$dist) 
```


```{r}
for(i in 1:nrow(models)) {
  dist_measure <- sum(abs(
  distance(
    observation = trees$volume, 
    prediction =  model(slope = models$slope[i], intercept = models$intercept[i], x = trees$diameter))))
  print(dist_measure)
}
```


```{r}
for(i in 1:nrow(models)) {
  dist_measure <- sqrt(mean(
  distance(
    observation = trees$volume, 
    prediction =  model(slope = models$slope[i], intercept = models$intercept[i], x = trees$diameter))^2))
  print(dist_measure)
}
```


```{block2, type='bdd'}
Essayez par vous même de trouver le meilleur modèle dans l'application shiny suivante :

Démarrez la SciViews Box et RStudio. Dans la fenêtre **Console** de RStudio, entrez l'instruction suivante suivie de la touche `Entrée` pour ouvrir le tutoriel concernant les bases de R :

    BioDataScience2::app("01a_modele_lineaire") TODO

N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R
```

```{r}
summary(lm(data = trees, formula = volume ~ diameter))
```

