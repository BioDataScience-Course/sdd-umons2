# Régression linéaire II {#lm2}

```{r setup, include=FALSE, echo=FALSE, message=FALSE, results='hide'}
knitr::opts_chunk$set(comment = '#', fig.align = "center")
SciViews::R
```


##### Objectifs {-}

- Savoir utiliser les outils de diagnostic de la régression linéaire correctement, en particulier l'analyse des résidus.

- Appréhender les différentes formes de régressions linéaires par les moindres carrés.

- Choisir sa régression linéaire de manière judicieuse.


##### Prérequis {-}

- Le module précédent est une entrée en matière indispensable qui est complétée par le contenu du présent module.


## Outils de diagnostic (suite)

La régression linéaire est une matière complexe et de nombreux outils existent pour vous aider à déterminer si le modèle que vous ajustez tient la route ou non. Il est très important de le vérifier avant d'utiliser un modèle. **Ajuster un modèle quelconque dans des données est à la portée de tout le monde, mais choisir un modèle pertinent et pouvoir expliquer pourquoi est nettement plus difficile\ !**


### Résumé avec `summary()`(suite)

Reprenons la sortie renvoyée par `summary()` appliqué à un objet `lm`.

```{r}
trees <- read("trees", package = "datasets", lang = "fr")
lm. <- lm(data = trees, volume ~ diameter)
summary(lm.)
```

Nous n'avons pas encore étudié la signification des trois dernières lignes de ce résumé. Voici de quoi il s'agit.

- Residual standard error\ :

Il s'agit de l'écart-type résiduel, considérant que les degrés de liberté du modèle est le nombre d'observations $n$ (ici 31) soustrait du nombre de paramètres à estimer (ici 2, la pente et l'ordonnée à l'origine de la droite). C'est donc une mesure globale de l'importance (c'est-à-dire de l'étendue) des résidus de manière générale.

$$\sqrt{\frac{\sum(y_i - ŷ_i)^2}{n-2}}$$

- Multiple R-squared\ :

Il s'agit de la valeur du **coefficient de détermination** du modèle noté *R^2* de manière générale ou *r^2^* dans le cas d'une régression linéaire simple. Il exprime la fraction de variance exprimée par le modèle. Autrement dit, le *R^2^* quantifie la capacité du modèle à *prédire* la valeur de $y$ connaissant la valeur $x$ pour le même individu. C'est dons une indication du *pouvoir prédictif* de notre modèle autant que de sa **qualité d'ajustement** (*goodness-of-fit* en anglais).

Souvenons-nous que la variance totale respecte la propiété d'additivité. La variance est composée au numérateur d'une somme de carrés, et au dénominateur de degrés de liberté. La somme des carrés totaux (de la variance) peut elle-même être décomposée en une **fraction expliquée** par notre modèle, et la fraction qui ne l'est pas (les **résidus**)\ :

$$SC(total) = SC(rég) + SC(résidus)$$

avec\ :

$$SC(total) = \sum_{i=0}^n(y_i - \bar y_i)^2$$

$$SC(rég) = \sum_{i=0}^n(ŷ_i - \bar y_i)^2$$

$$SC(résidus) = \sum_{i=0}^n(y_i - ŷ_i)^2$$

A partir de la décomposition de ces sommes de carrés, le coefficient *R^2^* (ou *r^2^*) se définit comme\ :

$$R^2 = \frac{SC(rég)}{SC(total)} = 1 - \frac{SC(résidus)}{SC(total)}$$

La valeur du *R^2^* est comprise entre 0 (lorsque le modèle est très mauvais et n'explique rien) et 1 (lorsque le modèle est parfait et "capture" toute la variance des données\ ; dans ce cas, tous les résidus valent zéro). Donc, **plus le coefficient *R^2^* se rapproche de un, plus le modèle explique bien les données et aura un bon pouvoir de prédiction**.

```{block2, type='warning'}

Dans R, le *R^2^* multiple se réfère simplement au *R^2^* (ou au *r^2^* pour les régressions linéaires simples) calculé de cette façon. L'adjectif **multiple** indique simplement que le calcul est valable pour une régression **multiple** telle que nous verrons plus loin.

Par contre, le terme au dénominateur considère en fait la somme des carrés totale **par rapport à un modèle de référence** lorsque la variable dépendante $y$ ne *dépend pas* de la ou des variables indépendantes $x_i$. Les équations indiquées plus haut sont valables lorsque l'ordonnée à l'origine *n'est pas* figée ($y = a \ x + b$). Dans ce cas, la valeur de référence pour $y$ est bien sa moyenne, $\bar y$.

D'un autre côté, si l'ordonnée à l'origine est fixée à zéro dans le modèle simplifié $y = a \ x$ (avec $b = 0$ obtenu en indiquant la formule `y ~ x + 0` ou `y ~ x - 1`), alors le zéro sur l'axe $y$ est considéré comme une valeur appartenant d'office au modèle et devient valeur de référence. Ainsi, dans les équations ci-dessus il faut remplacer $\hat y$ par 0 partout. Le *R^2^* est alors calculé différemment, et sa valeur peut brusquement augmenter si le nuage de points est très éloigné du zéro sur l'axe y. **Ne comparez donc jamais les *R^2^* obtenus avec et sans forçage à zéro de l'ordonnée à l'origine\ !**

```

- Adjusted R-squared\ :

La valeur du coefficient *R^2^* ajustée n'est pas utile dans le cadre de la régression linéaire simple, mais est indispensable avec la régression multiple. En effet, à chaque fois que vous rendez votre modèle plus complexe en ajoutant une ou plusieurs variables indépendantes, le modèle s'ajustera de mieux en mieux dans les données. C'est un phénomène que l'on appelle l'**inflation du *R^2^***. A la limite, si nous ajoutons une nouvelle variable fortement corrélée avec les précédentes^[La corrélation entre les prédicteurs dans un modèle linéaire multiple est un gros problème et doit être évité le plus possible. Cela s'appelle la **colinéarité** ou encore **multicollinéairité**. Ainsi, il est toujours préférable de choisir un ensemble de variables indépendantes peu corrélées entre elles dans un même modèle, mais ce n'est pas toujours possible.], l'apport en terme d'information nouvelle sera négligeable, mais le *R^2^* augmentera malgré tout un tout petit peu. 
Alors dans quel cas l'ajout d'une nouvelle variable est-il pertinent ou non\ ? Le *R^2^* ajusté apporte l'information désirée ici. TODO: continue explanation...

- F-statistic\ :

Tout comme pour l'ANOVA, le test de la significativité de la régression car  $MS(rég)/MS(résidus)$ suit une distribution *F* à respectivement 1 et $n-2$ degré de liberté, avec $MS$ les carrés moyens, c'est-à-dire les sommes des carrés $SC$ divisés par leurs degrés de liberté respectifs.

- p-value\ : 

Il s'agit de la valeur *p* associé à la statistique de *F*, donc à l'ANOVA associée à la régression linéaire. Pour cette ANOVA particulière, l'hypothèse nulle est que la droite n'apporte pas plus d'explication des valeurs de *y* à partir des valeurs de *x* que la valeur moyenne de *y* (ou zéro, dans le cas paerticulier d'un modèle dont l'ordonnée à l'origine est forcé à zéro). L'hypothèse alternative est donc que le modèle est significatif au seuil $\alpha$ considéré. **Donc, notre objectif est de rejetter *H~0~* pour cet test ANOVA** pour que le modèle ait un sens (valeur *p* plus petite quez le seuil $\alpha$ choisi).

Le tableau complet de l'ANOVA associée au modèle peut aussi être obtenu à l'aide de la fonction `anova()`\ :

```{r}
anova(lm.)
```

On y retrouve les mêmes informations, fortement résumées en une ligne à la fin de la sortie de `summary()`, mais ici sous une forme plus classique de tableau de l'analyse de la variance.


### Comparaison de régressions

Vous pouvez à présent comparer ces résultats avec un tableau et les six graphiques d'analyse des résidus sans la valeur supérieure à 0.5m de diamètre. **Attention, On ne peut supprimer une valeur sans raison valable.** La suppression de points aberrants doit en principe être faite avant de débuter l'analyse. La raison de la suppression de ce point est liée au fait qu'il soit seul et unique point supérieur à 0.5m de diamètre. Nous le faisons ici à titre de comparaison.

```{r}
trees_red <- filter(trees, diameter < 0.5)
lm1 <- lm(data = trees_red, volume ~ diameter)

chart(trees, volume ~ diameter) +
  geom_point() + 
  geom_abline(
    aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), 
    color = "red", size = 1.5) +
  labs( color = "Modèle")  +
  scale_color_viridis_c(direction = -1) +
  geom_abline(
    aes(intercept = lm1$coefficients[1], slope = lm1$coefficients[2]), 
    color = "blue", size = 1.5)
```

La droite en bleu correspond à la régression sans utiliser l'arbre de diamètre supérieur à 0,5m. Tentez d'analyser le tableau de notre régression en bleu (astuce\ : comparez avec ce que la régeression précédente donnait).

```{r}
summary(lm1)
```

Tentez d'analyser également les graphiques d'analyse des résidus ci-dessous.

```{r}
#plot(lm1, which = 1)
lm1 %>.%
  chart(broom::augment(.), .resid ~ .fitted) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(se = FALSE, method = "loess", formula = y ~ x) +
  labs(x = "Fitted values", y = "Residuals") +
  ggtitle("Residuals vs Fitted") 

#plot(lm1, which = 2)
lm1 %>.%
  chart(broom::augment(.), aes(sample = .std.resid)) +
  geom_qq() +
  geom_qq_line(colour = "darkgray") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals") +
  ggtitle("Normal Q-Q") 

#plot(lm1, which = 3)
lm1 %>.%
  chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) +
  geom_point() +
  geom_smooth(se = FALSE, method = "loess", formula = y ~ x) +
  labs(x = "Fitted values",
    y = expression(bold(sqrt(abs("Standardized residuals"))))) +
  ggtitle("Scale-Location") 

#plot(lm1, which = 4)
lm1 %>.%
  chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = "darkgray") +
  labs(x = "Obs. number", y = "Cook's distance") +
  ggtitle("Cook's distance") 

#plot(lm1, which = 5)
lm1 %>.%
  chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) +
  geom_point() +
  geom_smooth(se = FALSE, size = 0.5, method = "loess", formula = y ~ x) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  labs(x = "Leverage", y = "Standardized residuals") +
  ggtitle("Residuals vs Leverage")

#plot(lm1, which = 6)
lm1 %>.%
  chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) +
  geom_point() +
  geom_vline(xintercept = 0, colour = NA) +
  geom_abline(slope = seq(0, 3, by = 0.5), colour = "darkgray") +
  geom_smooth(se = FALSE, size = 0.5, method = "loess", formula = y ~ x) +
  labs(x = expression("Leverage h"[ii]), y = "Cook's distance") +
  ggtitle(expression("Cook's dist vs Leverage h"[ii] / (1 - h[ii])))
```

Au travers de cet exemple, nous constatons que la comparaison de modèles, dans le but de choisir le meilleur est un travail utile. Cela apparaitra d'autant plus utile que la situation va passablement se complexifier (dans le bon sens) avec l'introduction de la régression multiple et polynomiale ci-dessous. Heureusement, nous terminerons ce module avec la découverte d'une métrique qui va nous permettre d'effectuer le choix du meilleur modèle de manière fiable\ : le critère d'Akaike.


## Régression linéaire multiple 

Dans le cas de la régression linéaire simple, nous considèrions le modèle stqatistique suivant (avec $\epsilon$ représentant les résidus, terme statistique dans l'équation)\ :

$$y = a \ x + b + \epsilon $$ 

Dans le cas de la régression, nous introduirons *plusieurs* variables indépendantes notés $x_1$, $x_2$, ..., $x_n$\ :

$$y = a_1 \ x_1 + a_2 \ x_2 + ... + a_n \ x_n + b + \epsilon $$ 

La bonne nouvelle, c'est que tous les calculs, les métriques et les tests d'hypothèses relatifs à la régression linéaire simple se généraliser simplement et naturellement, tout comme nous sommes passés dans le cours SDD 1 de l'ANOVA à 1 facteur à un modèle plus complexe à 2 ou plusoieurs facteurs. Voyons tout de suite ce que cela donne si nous voulions utiliser **à la fois** le diamètree et la hauteur des cerisiers noirs pour prédire leur volume de bois\ :

```{r}
summary(lm2 <- lm(data = trees, volume ~ diameter + height))
```

D'un point de vue pratique, nous voyons que la formule qui spécifie le modèle peut très bien comporter plusieurs variables séparées par des `+`. Nous avons ici trois paramètres dans notre modèle\ : l'ordonnée à l'origine qui vaut -1,63, la pente relative au diamètre de 5,25, et la pente relative à la hauteur de 0,031. Le modèle `lm2` sera donc paramétré comme suit\ : volume de bois = 5,25 . diamètre + 0,031 . hauteur - 1,63.

Notons que la pente relative à la hauteur (0,031) n'est pas significativement différente de zéro au seuil $\alpha$ de 5% (mais l'est seulement pour $\alpha$ = 1%). En effet, la valeur *t* du test de Student associé (H~0~\ : le paramètre vaut zéro, H~1~\ : le paramètre est différent de zéro) vaut 2,574. Cela correspond à une valeur *p* du test de 0,0156, une valeur moyennement significative donc, matérialisée par une seule astérisque à la droite du tableau. Cela dénote un plus faible pouvoir de prédiction du volume de bois via la hauteur que via le diamètre de l'arbre. Nous l'avions déjà observé sur le graphique matrice de nuages de points réalisé initialement, ainsi que via les coefficients de correlation respectifs.

La représentation de cette régression nécessite un graphique à trois dimensions (diamètre, hauteur et volume) et le modèle représente en fait le meilleur **plan** dans cet espace à 3 dimensions. Pour un modèle comportant plus de deux variables indépendantes, il n'est plus possible de représenter graphiquement la régression.

```{r}
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```

```{r}
car::scatter3d(data = trees, volume ~ diameter + height, fit = "linear",
  residuals = TRUE, bg = "white", axis.scales = TRUE, grid = TRUE,
  ellipsoid = FALSE)
rgl::rglwidget(width = 800, height = 800)
```

Utilisez la souris pour zoomer (molette) et pour retourner le graphique (cliquez et déplacer la souris en maintenant le bouton enfoncé) pour comprendre ce graphique 3D. La régression est matérialisée par un plan en bleu. Les observations sont les boules jaunes et les résidus sont des traits cyans lorsqu'ils sont positifs et magenta lorsqu'ils sont négatifs.

Les graphes d'analyse des résidus sont toujours disponibles (nous ne représentons ici que les quatre premiers)\ :

```{r}
#plot(lm2, which = 1)
lm2 %>.%
  chart(broom::augment(.), .resid ~ .fitted) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(se = FALSE, method = "loess", formula = y ~ x) +
  labs(x = "Fitted values", y = "Residuals") +
  ggtitle("Residuals vs Fitted") 

#plot(lm2, which = 2)
lm2 %>.%
  chart(broom::augment(.), aes(sample = .std.resid)) +
  geom_qq() +
  geom_qq_line(colour = "darkgray") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals") +
  ggtitle("Normal Q-Q") 

#plot(lm2, which = 3)
lm2 %>.%
  chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) +
  geom_point() +
  geom_smooth(se = FALSE, method = "loess", formula = y ~ x) +
  labs(x = "Fitted values",
    y = expression(bold(sqrt(abs("Standardized residuals"))))) +
  ggtitle("Scale-Location") 

#plot(lm2, which = 4)
lm2 %>.%
  chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = "darkgray") +
  labs(x = "Obs. number", y = "Cook's distance") +
  ggtitle("Cook's distance") 
```

Est-ce que ce modèle est préférable à celui n'utilisant que le diamètre\ ? Le *R^2^ ajusté* est passé de 0,933 avec le modèle simple `lm.` utilisant uniquement le diamètre à 0,944 dans le présent modèle `lm2` utilisant le diamètre et la hauteur. Cela semble une amélioration, mais le test de significativité de la pente pour la hauteur ne nous indique pas un résultat très significatif. De plus, cela a un coût en pratique de devoir mesurer deux variables au lieu d'une seule pour estimer le volume de bois. Cela en vaut-il la peine\ ? Nous sommes encore une fois confrontés à la question de comparer deux modèles, cette fois-ci ayant une complexité croissante.

Dans le cas particulier de modèles **imbriqués** (un modèle *contient* l'autre, mais rajoute un ou plusieurs termes), une ANOVA est possible en décomposant la variance selon les composantes reprises respectivement par chacun des deux modèles. La fonction `anova()` est programmée pour faire ce calcul en lui indiquant chacun des deux objets contenant les modèles à comparer\ :

```{r}
anova(lm., lm2)
```

Notez que dans le cas de l'ajout d'*un seul* terme, la valeur *p* de cette ANOVA est identique à la valeur *p* de test de significativité du paramètre (ici, cette valeur *p* est de 0,0156 dans les deux cas). Donc, le choix peut se faire directement à partir de `summary()` pour ce terme unique. La conclusion est similaire\ : l'ANOVA donne un résultat seulement moyennent significatif entre les 2 modèles. Dans un cas plus complexe, la fonction `anova()` de comparaison pourra être utile. Enfin, tous les modèles ne sont pas nécessairement imbriqués. Dans ce cas, il nous faudra un autre moyen de les départager, ... mais avant d'aborder cela, étudions une variante intéressante de la régression multiple\ : la régression polynomiale.


## Régression linéaire polynomiale

TODO


## RMSE & critère d'Akaike

Le *R^2^*/*r^2^* (ajusté) n'est pas la seule mesure d'ajustement d'un modèle. Il existe d'autres indicateurs. Par exemple, l'**erreur quadratique moyenne**, (root mean square error, ou RMSE en anglais) est la racine carrée de la moyenne des résidus au carré. Elle représente en quelque sorte la distance "typique" des résidus. Comme cette distance est exprimée dans les mêmes unités que l'axe *y*, cette mesure est particulièrement parlante. Nous pouvons l'obtenir par exemple comme ceci\ :

```{r}
modelr::rmse(lm., trees)
```

Cela signifie que l'on peut s'attendre à ce que, en moyenne, les valeurs prédites de volume de bois s'écartent (dans un sens ou dans l'autre) de 0,117 m^3^ de la valeur effectivement observée. Evidemment, plus un modèle est bon, plus le RMSE est **faible**, contrairement au *R^2^* qui lui doit être **élevé**.

Si le *R^2^* comme le RMSE sont utiles pour quantifier la qualité d'ajustement d'*une* régression, ces mesures sont peu adaptées pour la comparaison de modèles entre eux. En effet, nous avons vu que plus le modèle est complexe, mieux il s'ajuste dans les données. Le *R^2^ ajusté* tente de remédier partiellement à ce problème, mais cette métrique reste peu fiable pour comparer des modèles très différents. Le **critère d'Akaike**, du nom du statisticien japonais qui l'a conçu, est une métrique plus adaptée à de telles comparaisons. Elle se base au départ sur encore une autre mesure de la qualité d'ajustement d'un modèle\ : la **log-vraisemblance**. Les explications relatives à cette mesure sont obligatoirement complexes d'un point de vue mathématique et nous vous proposons ici d'en retenir la définition sur un plan purement conceptuel. Un **estimateur de maximum de vraisemblance** est une mesure qui permet d'inférer le meilleur ajustement possible d'une loi de probabilité par rapport à des données. Dans le cas de la régression par les moindres carrés, la distribution de probabilité à ajuster est celle des résidus (pour rappel, il s'agit d'une distribution Normale de moyenne nulle et d'écart type constant $\sigma$). La **log-vraisemblance**, pour des raisons purement techniques est souvent préféré au maximum de vraissemblance. Il s'agit simplement du logarithme de sa valeur.

Donc, plus la log-vraisemblance est grande, mieux les données sont compatibles avec le modèle probabiliste considéré. **Pour un même jeu de données**, ces valeurs sont comparables entre elles... même pour des modèles très différents. Mais cela ne règle pas la question de la complexité du modèle. C'est ici qu'Akaike entre en piste. Il propose le critère suivant\ :

$$
\textrm{AIC} = -2 . \textrm{log-vraisemblance} + 2 . \textrm{nbrpar}
$$

- où **nbrpar** est le nombre de paramètres à estimer dans le modèle. Donc ici, nous prenons comme point de départ moins deux fois la log-vraisemblance, une valeur *a priori* à **minimiser**, mais nous lui ajoutons le second terme de **pénalisation** en fonction de la complexité du modèle valant 2 fois le nombre de paramètres du modèle. Notons d'ailleurs que le terme multiplicateur 2 ici est modifiable. Si nous voulons un modèle le moins complexe possible, nous pourrions très bien multiplier par 3 ou 4 pour pénaliser encore plus. Et si nous voulons être moins restrictifs, nous pouvons aussi diminuer ce facteur multiplicatif. Dans la pratique, le facteur 2 est quand même très majoritairement adapté par les praticiens, mais la possibilité de changer l'impact de complexité du modèle est inclue dans le calcul *de facto*.

Dès lors que ce critère peut être calculé (et R le fait pour pratiquement tous les modèles qu'il propose), une comparaison est possible avec pour objectif de sélectionner le, ou un des modèles qui a l'AIC **la plus faible**. N'oubliez toutefois pas de comparer *visuellement* les différents modèles ajustés et d'interpréter les graphiques d'analyse des résidus respectifs en plus des valeurs d'AIC. **C'est l'ensemble de ces outils qui vous orientent vers le meilleur modèle, pas l'AIC seul\ !**
