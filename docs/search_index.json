[
["index.html", "Science des données biologiques 2 Préambule", " Science des données biologiques 2 Philippe Grosjean &amp; Guyliann Engels 2019-09-11 Préambule Cet ouvrage interactif est le second d’une série de trois ouvrages traitant de la science des données biologiques. L’écriture de cette suite de livres a débuté au cours de l’année académique 2018-2019. Pour l’année académique 2019-2020, cet ouvrage interactif sera le support du cours suivant : Science des données II : Analyse et modélisation, UMONS dont le responsable est Grosjean Philippe Cet ouvrage est conçu pour être utilisé de manière interactive en ligne. En effet, nous y ajoutons des vidéos, des démonstrations interactives, et des exercices sous forme de questionnaires interactifs également. Ces différents éléments ne sont, bien évidemment, utilisables qu’en ligne. Le matériel dans cet ouvrage est distribué sous licence CC BY-NC-SA 4.0. "],
["vue-generale-des-cours.html", "Vue générale des cours", " Vue générale des cours Le cours de Science des données II: analyse et modélisation est dispensé aux biologistes de troisième Bachelier en Faculté des Sciences de l’Université de Mons à partir de l’année académique 2019-2020. La matière est divisée en 8 modules de sessions de 6h chacuns en présentiel. Il nécessitera environ un tiers de ce temps (voir plus, en fonction de votre rythme et de votre technique d’apprentissage) en travail à domicile. Cette matière fait suite au premier cours dont le contenu est considéré comme assimilé (voir https://biodatascience-course.sciviews.org/sdd-umons/). "],
["materiel-pedagogique.html", "Matériel pédagogique", " Matériel pédagogique Le matériel pédagogique, rassemblé dans ce syllabus interactif est aussi varié que possible. Vous pourrez ainsi piocher dans l’offre en fonction de vos envies et de votre profil d’apprenant pour optimiser votre travail. Vous trouverez: le présent ouvrage en ligne, des tutoriaux interactifs (réalisés avec un logiciel appelé learnr). Vous pourrez exécuter ces tutoriaux directement sur votre ordinateur, et vous aurez alors accès à des pages Web réactives contenant des explications, des exercices et des quizzs en ligne, des slides de présentations, des dépôts Github Classroom dans la section BioDataScience-Course pour réaliser et documenter vos travaux personnels. des renvois vers des documents externes en ligne, types vidéos youtube ou vimeo, des ouvrages en ligne en anglais ou en français, des blogs, des tutoriaux, des parties gratuites de cours Datacamp ou équivalents, des questions sur des sites comme “Stackoverflow” ou issues des “mailing lists” R, … Tout ce matériel est accessible à partir du site Web du cours, du présent syllabus interactif (et de Moodle pour les étudiants de l’UMONS). Ces derniers ont aussi accès au dossier SDD sur StudentTemp en Intranet à l’UMONS. Les aspects pratiques seront à réaliser en utilisant la ‘SciViews Box’, une machine virtuelle préconfigurée1. Enfin, vous pourrez poser vos questions par mail à l’adresse sdd@sciviews.org. Il existe tout de même des outils plus pointus pour obtenir de l’aide sur le logiciel R comme rseek.org, rdocumentation.org ou rdrr.io. Rien ne sert de chercher ’R’ dans Goggle.↩ "],
["comment-apprendre.html", "Comment apprendre?", " Comment apprendre? fortunes::fortune(&quot;brain surgery&quot;) # # I wish to perform brain surgery this afternoon at 4pm and don&#39;t know where # to start. My background is the history of great statistician sports # legends but I am willing to learn. I know there are courses and numerous # books on brain surgery but I don&#39;t have the time for those. Please direct # me to the appropriate HowTos, and be on standby for solving any problem I # may encounter while in the operating room. Some of you might ask for # specifics of the case, but that would require my following the posting # guide and spending even more time than I am already taking to write this # note. # -- I. Ben Fooled (aka Frank Harrell) # R-help (April 1, 2005) Version courte: en pratiquant, en faisant des erreurs ! Version longue: aujourd’hui –et encore plus à l’avenir– les données sont complexes et ne se manipulent plus simplement avec un tableur comme Microsoft Excel. Vous allez apprendre à maitriser des outils professionnels, ce qui sous-entend qu’ils sont très puissants mais aussi relativement complexes. La méthode d’apprentissage que nous vous proposons a pour objectif prioritaire de vous faciliter la tâche, quelles que soient vos aptitudes au départ. Envisagez votre voyage en science des données comme l’apprentissage d’une nouvelle langue. C’est en pratiquant, et en pratiquant encore sur le long terme que vous allez progresser. La formation s’étale sur quatre années, et est répartie en cinq cours de difficulté croissante pour vous aider dans cet apprentissage progressif et sur la durée. N’hésitez pas à expérimenter, tester, essayer des nouvelles idées (même au delà de ce qui sera demandé dans les exercices) et n’ayez pas peur de faire des erreurs. Vous en ferez, … beaucoup … nous vous le souhaitons! En fait, la meilleure manière d’apprendre, c’est justement en faisant des erreurs, et puis en mettant tout en oeuvre pour les comprendre et les corriger. Donc, si un message d’erreur, ou un “warning” apparait, ne soyez pas intimidé. Prenez une bonne respiration, lisez-le attentivement, essayez de le comprendre, et au besoin faites-vous aider: la solution est sur le Net, ‘Google1 est votre ami’! Il existe tout de même des outils plus pointus pour obtenir de l’aide sur le logiciel R comme rseek.org, rdocumentation.org ou rdrr.io. Rien ne sert de chercher ’R’ dans Goggle.↩ "],
["evaluation.html", "Evaluation", " Evaluation L’évaluation sera basée sur une somme de petites contributions qui matérialiseront votre progression sur le long terme. Avec cette évaluation, nous souhaitons vous gratifier chaque fois que vous franchirez des étapes, plutôt que de vous sanctionner lorsque vous bloquez. Donc, pour une note finale sur 20: 3 points pour la restitution des capsules et votre participation en présentiel. Au début de chaque séance, nous discuterons des notions que vous aurez à préparer par avance, et votre participation sera évaluée. 6 points pour un quizz final. 11 points pour l’évaluation d’un des rapports d’analyse de données (choisi au hasard en fin de cours). Enfin, vous pourrez éventuellement encore gagner un point bonus pour une participation remarquable, ou tout autre élément à valoriser (rapport particulièrement bien réalisé, aide des autres étudiants, etc.). Ceci étant à l’appréciation des enseignants. System information sessioninfo::session_info() # ─ Session info ────────────────────────────────────────────────────────── # setting value # version R version 3.5.3 (2019-03-11) # os Ubuntu 18.04.2 LTS # system x86_64, linux-gnu # ui X11 # language (EN) # collate en_US.UTF-8 # ctype en_US.UTF-8 # tz Europe/Madrid # date 2019-09-11 # # ─ Packages ────────────────────────────────────────────────────────────── # package * version date lib source # assertthat 0.2.1 2019-03-21 [2] CRAN (R 3.5.3) # bookdown 0.9 2018-12-21 [2] CRAN (R 3.5.3) # cli 1.1.0 2019-03-19 [2] CRAN (R 3.5.3) # crayon 1.3.4 2017-09-16 [2] CRAN (R 3.5.3) # digest 0.6.18 2018-10-10 [2] CRAN (R 3.5.3) # evaluate 0.13 2019-02-12 [2] CRAN (R 3.5.3) # fortunes 1.5-4 2016-12-29 [2] CRAN (R 3.5.3) # htmltools 0.3.6 2017-04-28 [2] CRAN (R 3.5.3) # inline 0.3.15 2018-05-18 [2] CRAN (R 3.5.3) # knitr 1.22 2019-03-08 [2] CRAN (R 3.5.3) # magrittr 1.5 2014-11-22 [2] CRAN (R 3.5.3) # Rcpp 1.0.1 2019-03-17 [2] CRAN (R 3.5.3) # rmarkdown 1.12 2019-03-14 [2] CRAN (R 3.5.3) # sessioninfo 1.1.1 2018-11-05 [2] CRAN (R 3.5.3) # stringi 1.4.3 2019-03-12 [2] CRAN (R 3.5.3) # stringr 1.4.0 2019-02-10 [2] CRAN (R 3.5.3) # withr 2.1.2 2018-03-15 [2] CRAN (R 3.5.3) # xfun 0.6 2019-04-02 [2] CRAN (R 3.5.3) # yaml 2.2.0 2018-07-25 [2] CRAN (R 3.5.3) # # [1] /home/sv/R/x86_64-pc-linux-gnu-library/3.5 # [2] /usr/local/lib/R/site-library # [3] /usr/lib/R/site-library # [4] /usr/lib/R/library "],
["lm.html", "Module 1 Modèle linéaire", " Module 1 Modèle linéaire Objectifs Découvrir le modèle linaire de manière intuitive Prérequis Avant de poursuivre, vous allez réaliser une séance d’exercice couvrant les points essentiels des notions abordées dans le livre science des données biologiques partie 1. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01a_rappel&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R "],
["modele.html", "1.1 Modèle", " 1.1 Modèle TODO Un modèle a pour objectif de fournir une description des données. On retrouve deux grandes catégories au sein des modèles, les modèles prédictifs et les modèles exploratoires. une observation peut être employé pour explorer et pour confirmer un modèle mais pas les deux. Pour réaliser une analyse confirmatoire, il est proposé de divisé les observations en 2 groupes jeu d’entrainement jeu de confirmation Qu’est ce qu’un modèle TODO "],
["regression-lineaire-simple-par-intuition.html", "1.2 Régression linéaire simple par intuition", " 1.2 Régression linéaire simple par intuition Nous allons découvrir les base de la régression linéaire de façon intuitive. Nous utilisons le jeu de données trees qui rassemble la mesure du diamètre, de la hauteur et du volume de bois de cerisiers noirs. # importation des données trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) Rapellons nous que dans le chapitre 12 du livre science des données 1, nous avons étudié l’association de deux variables numériques. Nous utilisons donc une matrice de corrélation afin de mettre en évidence la corrélation entre nos 3 variables qui composent le jeu de donnée trees. La fonction correlation() nous renvoie un tableau de la matrice de correlation avec l’indice de Pearson. (trees_corr &lt;- correlation(trees)) # Matrix of Pearson&#39;s product-moment correlation: # (calculation uses everything) # diameter height volume # diameter 1.000 0.519 0.967 # height 0.519 1.000 0.597 # volume 0.967 0.597 1.000 Nous pouvons également observer cette matrice sous la forme d’un graphique plus convivial. plot(trees_corr, type = &quot;lower&quot;) Cependant, n’oubliez pas qu’il est indispensable de visualiser les nuages de points pour ne pas tobmer dans le piège mis en avant par le jeu de données artificiel appelé “quartet d’Anscombe” qui montre très bien comment des données très différentes peuvent avoir même moyenne, même variance et même coefficient de corrélation. GGally::ggscatmat(as.data.frame(trees), 1:3) Nous observons une forte corrélation linéaire entre le volume et la hauteur des cerisiers noirs. Interessons nous à cette association. chart(trees, volume ~ diameter) + geom_point() Si vous deviez ajouter une droite permettant de représenter au mieux les données, où est ce que vous la placeriez ? Pour rappel, une droite respecte l’équation mathématique suivante: \\[y = ax + b\\] dont a est la pente (slope en anglais) et b est l’ordonnée à l’origine (intercept en anglais). # Sélection de pente et d&#39;ordonnée à l&#39;origine models &lt;- tibble( model = paste(&quot;mod&quot;, 1:4, sep = &quot;-&quot;), slope = c(5, 5.5, 6, 0), intercept = c(-0.5, -0.95, -1.5, 0.85) ) chart(trees, volume ~ diameter) + geom_point() + geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + labs( color = &quot;Modèle&quot;) Nous avons 4 droites qui veulent représenter au mieux les observations. Quel est la meilleure régression selon vous ? 1.2.1 Quantifier la qualité d’un modèle Nous voulons identifier la meilleure régression, c’est à dire la régression le plus proche de nos données. Nous avons besoin d’une règle qui va nous permettre de quantifier la distance de nos observations à notre modèle afin d’obtenir la régression avec la plus faible distance possible de l’ensemble de nos observations. Décomposons le problème étape par étape et intéressons nous au mod-1 Connaitre les valeurs de y prédite par le modèle # Calculer la valeur de y pour chaque valeur de x suivant le model souhaité ## Création de notre fonction model &lt;- function(slope, intercept, x) { prediction &lt;- intercept + slope * x attributes(prediction) &lt;- NULL prediction } ## Application de notre fonction mod1 &lt;- model(slope = 5, intercept = -0.5, x = trees$diameter) ## Affichage des résultats mod1 # [1] 0.555 0.590 0.620 0.835 0.860 0.870 0.895 0.895 0.910 0.920 0.935 # [12] 0.950 0.950 0.985 1.025 1.140 1.140 1.190 1.240 1.255 1.280 1.305 # [23] 1.340 1.530 1.570 1.695 1.720 1.775 1.785 1.785 2.115 Connaitre la distance entre les observations mesurées en y et les observations prédites en y par la régression Les distances que nous souhaitons calculer, sont les résidus. Nous pouvons premièrement visualiser ces résidus graphiquement : chart(trees, volume ~ diameter) + geom_point() + geom_abline(slope = 5, intercept = -0.5) + geom_segment( aes(x = diameter, y = volume, xend = diameter, yend = mod1)) Nous pouvons ensuite facilement calculer cette distance comme ci-dessous : # Calculer la distance entre y observé et y prédit ## création de notre fonction distance &lt;- function(observation, prediction) { diff &lt;- observation - prediction attributes(diff) &lt;- NULL diff } ## Application de la fonction dist1 &lt;- distance(observation = trees$volume, prediction = mod1) ## affichage des résultats dist1 # [1] -0.263 -0.298 -0.331 -0.371 -0.328 -0.312 -0.453 -0.380 -0.270 -0.357 # [11] -0.250 -0.355 -0.344 -0.382 -0.484 -0.511 -0.183 -0.414 -0.512 -0.550 # [21] -0.303 -0.407 -0.312 -0.445 -0.364 -0.126 -0.143 -0.124 -0.327 -0.341 # [31] 0.065 Définir une règle pour obtenir une valeur unique de la distance de nos observations mesurées en y par rapport aux observations prédites en y par la régression Une première idée serait de sommer l’ensemble de nos distances comme ci-dessous : sum(dist1) # [1] -10.175 Appliquons la suite d’étapes ci-dessus pour nos 4 modèles afin de les comparer mod-1 mod-2 mod-3 mod-4 -10.175 -1.441 10.393 0.135 Selon notre méthode, il en ressort que le modèle 4 est le modèle le plus approprié pour représenter au mieux nos données. Qu’en pensez vous ? chart(trees, volume ~ diameter) + geom_point() + geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + labs( color = &quot;Modèle&quot;) Intuitivement, nous nous apperçevons que le modèle 4 n’est pas la meilleuire solution pour représenter nos observartions. Nous pouvons en déduire que la somme des résidus n’est pas la meilleur fonction pour ajuster une droite avec nos observations. Le problème lorsque nous sommons des résidus, est la présence de résidus positifs et de résidus négatifs. mod2 &lt;- model(slope = models$slope[4], intercept = models$intercept[4], x = trees$diameter) chart(trees, volume ~ diameter) + geom_point() + geom_abline(data = models, aes(slope = slope[4], intercept = intercept[4])) + geom_segment( aes(x = diameter, y = volume, xend = diameter, yend = mod2)) + geom_label(aes(x = 0.3, y = 1.5), label = &quot;Résidus postifis&quot;, color = &quot;red&quot;) + geom_label(aes(x = 0.45, y = 0.5), label = &quot;Résidus négatifs&quot;, color = &quot;blue&quot;) Avez vous une autre idée que de sommer les résidus ? Sommer le carré des résidus Nous obtenons les résultats suivants : mod-1 mod-2 mod-3 mod-4 3.842211 0.4931095 3.929195 6.498511 Sommer les valeurs absolues des résidus Nous obtenons les résultats suivants : mod-1 mod-2 mod-3 mod-4 10.305 3.186 10.393 11.525 Nous avons trouvé deux solutions intéressantes pour quantifier la distance de notre droite par rapport à nos observations. Essayez de trouver le meilleur modèle par vous même dans l’application shiny Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::app(&quot;01a_lin_mod&quot;) # TODO méthode alternative shiny::runApp(system.file(&quot;shiny/01a_lin_mod&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R 1.2.2 Trouver la meilleure droite Nous pouvons maintenant nous demander si notre modèle 2 qui est le meilleur modèle de nos 4 modèles est le meilleur modèle possible pour se faire nous allons devoir optimiser notre modèle afin d’avoir le meilleur modèle possible. Nous allons prendre la somme du carré des résidus comme fonction à minimiser. Imaginons que nous avons pas 4 mais 500 modèle linéaire avec une pente et une ordonnée à l’origine différente, quelle est la meilleure droite ? models1 &lt;- tibble( intercept = runif(5000, -10, 10), slope = runif(5000, 2, 8)) chart(trees, volume ~ diameter) + geom_point() + geom_abline(aes(intercept = intercept, slope = slope), data = models1, alpha = 1/6) Sur ces 4 modèles, nous pouvons calculer la somme des carrés des résidus et observer quel est le meilleur modèle. # fonction de calcule de la somme des carrés des résidus measure_distance &lt;- function(slope, intercept, x, y){ predict &lt;- x * slope + intercept dist &lt;- y - predict sum(dist ^ 2) } # test de la fonction #measure_distance(slope = 0, intercept = 0.85, x = trees$diameter, y = trees$volume) # fonction adaptée pour être employé avec purrr:map trees_dist &lt;- function(intercept, slope){ measure_distance(slope = slope, intercept = intercept, x = trees$diameter, y = trees$volume) } models1 &lt;- models1 %&gt;% mutate(dist = purrr::map2_dbl(intercept, slope, trees_dist)) si nous réalisons un graphique de valeurs de pentes, d’ordonnée à l’origine et des distances calculées, nous obtenons le graphique ci-dessous. plot &lt;- chart(models1, slope ~ intercept %col=% dist) + geom_point() + geom_point(data = filter(models1, rank(dist) &lt;= 10), shape = 1, color = &#39;red&#39;, size = 3) + labs( y = &quot;Pente&quot;, x = &quot;Ordonnée à l&#39;origine&quot;, color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) #plot plotly::ggplotly(plot) rem : Les 10 valeurs les plus faibles sont mis en évidence par un cercle rouge best_models &lt;- models1 %&gt;.% filter(., rank(dist) &lt;= 5) Nous pouvons afficher les 5 meilleurs modèles sur notre graphique chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + labs(color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) En résumé, nous avons besoin d’une fonction qui calcule la distance d’un modèle par rapport à nos observations et d’un algorihtme pour la minimiser. Il n’est cependant pas nécessaire de chercher pendant des heures la meilleure fonction et le meilleur algorithme. Il existe dans R, un outil spécifiquement conçu pour adapter des modèles linéaires sur des observations, la fonction lm(). Vous avez à votre disposition des snippets dédiés au focntions liés modèles linéaires. pour ne pas devoir retenir ... -&gt; ...m -&gt; -&gt; .ml (lm. &lt;- lm(data = trees, volume ~ diameter)) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Coefficients: # (Intercept) diameter # -1.047 5.652 Nous pouvons reporter ces valeurs sur notre graphique afin d’observer ce résultat par rapport à nos modèles aléatoires. nous avons en rouge la droite calculée par la fonction lm(). chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) 1.2.3 La fonction lm() Suite à notre découverte de manière intuitive de la régression linéaire simple, nous pouvons ressortir quelques points clés : Une droite suit l’équation mathématique suivante : \\[y = ax + b\\] dont a est la pente (slope en anglais) et b est l’ordonnée à l’origine (intercept en anglais). La distance entre une valeur observée et une valeur prédite se nomme le résidu (\\(\\epsilon\\)) \\[y_i = ax_i + b + \\epsilon_i\\] dont \\(y_i\\) est la valeur mesurée pour le point i sur l’axe y, \\(a\\) est la pente (slope en anglais), \\(x_i\\) est la valeur mesurée pour le point i sur l’axe x, b est l’ordonnée à l’origine (intercept en anglais) et \\(\\epsilon_i\\) est la distance entre la valeur prédite par la droite et la valeur de \\(y_i\\). La meilleure droite s’obtient par la minimisation de la somme des carrés des résidus (régression par les moindres carrés). La fonction lm() permet de calculer la meilleure droite possible #trees %&gt;.% # filter(., diameter &lt; 0.5) -&gt; trees # Modèle linéaire lm. &lt;- lm(data = trees, volume ~ diameter) # graphique de nos observations et de la droite obtenue avec la fonction lm() chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) La fonction lm() crée un objet avec de nombreuses informations calculées et mis à notre disposition pour analyser notre modèle linéaire. La fonction class() permet de mettre en avant la classe de notre objet. class(lm.) # [1] &quot;lm&quot; Attention, nous devons toujours garder un esprit critique. Que pensez vous du graphique suivant ? On peut mettre en avant que nous avons une seule valeur dont le diamètre est supérieur à 0.5m de diamètre. 1.2.4 Tableau d’interprétation Avec la fonction summary() nous obtenons le tableau d’analyse du modèle linéaire. summary(lm.) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.231211 -0.087021 0.003533 0.100594 0.271725 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.04748 0.09553 -10.96 7.85e-12 *** # diameter 5.65154 0.27649 20.44 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1206 on 29 degrees of freedom # Multiple R-squared: 0.9351, Adjusted R-squared: 0.9329 # F-statistic: 417.8 on 1 and 29 DF, p-value: &lt; 2.2e-16 Call: Il s’agit de la formule employée dans la fonction lm(). C’est à dire le volume en fonction du diamètre du cerisier noirs Residuals: La tableau nous fournit un résumé via les 5 nombres de l’ensemble des résidus (dont vous pouvez faire appel à l’ensemble des résidus avec lm.$residuals) fivenum(lm.$residuals) # 20 7 12 9 31 # -0.231210947 -0.087020695 0.003532709 0.100594122 0.271724973 Coefficients: Il s’agit des résultats associés à la pente et à l’ordonnée à l’origine dont les valeurs estimées des paramètres (Estimate) lm.$coefficients # (Intercept) diameter # -1.047478 5.651535 On retrouve également les écart-types sur ces valeurs (Std.Error), les valeurs des distibutions de Student sur les valeurs estimées (t value) et enfin la valeur de valeurs de p (PR(&gt;|t|)). Nous pouvons donc écrire l’équation de notre régression linéaire : \\[y = ax + b\\] \\[volume \\ de \\ bois = 5.65 \\times diamètre \\ à \\ 1.4 \\ m - 1.05 \\] Residual standard error: Il s’agit de l’écart-type résiduel \\[\\sqrt{\\frac{\\sum(y_i - ŷ_i)^2}{n-2}}\\] Multiple R-squared: Il s’agit de la valuer du coefficient \\(R^2\\) qui exprime la fraction de variance exprimé par le modèle. Souvenons nous que la variance totale respecte la propiété d’additivité. La variance conditionnelle \\(s^2_{y\\left|x\\right|}\\) peut être décomposé comme étant \\[SS(total) = SS(reg) + SS(résidus)\\] \\[SS(total) = \\sum(y_i - \\bar y_i)^2\\] \\[SS(reg) = \\sum(ŷ_i - \\bar y_i)^2\\] \\[SS(residus) = \\sum(y_i - ŷ_i)^2\\] \\[R^2 = \\frac{SS(reg)}{SS(total)}\\] Dès lors la valeur du \\(R^2\\) ne peut être compris que entre 0 et 1. Adjusted R-squared: La valuer du coefficient \\(R^2\\) ajustée. Le calcul de cette valeur sera abordé dans la suite de ce livre. F-statistic Tout comme pour l’ANOVA, le test de la significativité de la régression car \\(MS(reg)/MS(résidus)\\) suit une distribution F à respectivement 1 et \\(n-2\\) degré de liberté. p-value: Il s’agit de la valeur de p associé à la statistique de F. 1.2.5 Graphiques d’interprétation Nous avons en plus à notre disposition 6 graphiques pour étudier la qualité de notre régression. Le premier graphique permet de vérifier la distribution homogène des résidus. Dans une bonne régression, nous aurons une distribution équilibrée de part et d’autre du 0. Que pensez vous de notre graphique d’anayse des résidus ? Nous avons une valeur plus éloignée du 0 qui est mis en avant par la courbe en bleue qui montre l’influence des résidus. #plot(lm., which = 1) lm. %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) Ce second graphique permet de vérifier la normalité des résidus par rapport à une distribution normale. #plot(lm., which = 2) lm. %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) Ce troisième graphique va standardiser les résidus afin de pouvoir comparer les résidus positif et les résidus négatifs. A nouveau, nous pouvons observer qu’une valeur influence fortement la courbe bleue. #plot(lm., which = 3) lm. %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) Ce quatrième graphique met en évidence l’influence des individus sur la régression linéaire. Effectivement, la régression linéaire est sensible aux valeurs extrêmes. Nous pouvons observer que nous avons une valeur qui influence fortement notre régression. On utilise pour ce faire la distance de Cook que nous ne détailleront pas dans le cadre de ce cours. #plot(lm., which = 4) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Ce cinquième grapique utilise l’effet de levier (Leverage) qui met également en avant l’influence des individus sur notre régression. Nous avons à nouvea une valeur qui influence notre modèle. #plot(lm., which = 5) lm. %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) Ce sixième graphique met en relation la distance de Cooks et l’effet de levier. Notre unique point d’une valeur supérieur à 0.5 m de diamètre influence fortement notre modèle. …. #plot(lm., which = 6) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii]/(1-h[ii]))) 1.2.6 Comparaison de régressions Vous pouvez à présent comparer ces résultats avec un tableau et les 6 graphiques sans la valeur supérieur à 0.5m de diamètre. Attention, On ne peut supprimer une valeur sans raison valable. La points à supprimer doivent théoriquement être fait avant de débuter l’analyse. La raison de la suppression de ce point est lié au fait qu’il soit seul et unique point supérieur à 0.5m de diamètre. trees_red &lt;- filter(trees, diameter &lt; 0.5) lm1 &lt;- lm(data = trees_red, volume ~ diameter) chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) + geom_abline( aes(intercept = lm1$coefficients[1], slope = lm1$coefficients[2]), color = &quot;blue&quot;, size = 1.5) Tentez d’analyser le tableau de notre régression summary(lm1) # # Call: # lm(formula = volume ~ diameter, data = trees_red) # # Residuals: # Min 1Q Median 3Q Max # -0.215129 -0.068502 -0.001149 0.070522 0.181398 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -0.94445 0.09309 -10.15 6.98e-11 *** # diameter 5.31219 0.27540 19.29 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1082 on 28 degrees of freedom # Multiple R-squared: 0.93, Adjusted R-squared: 0.9275 # F-statistic: 372.1 on 1 and 28 DF, p-value: &lt; 2.2e-16 Tentez d’analyser ces graphiques ci-dessous #plot(lm1, which = 1) lm1 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm1, which = 2) lm1 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm1, which = 3) lm1 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm1, which = 4) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) #plot(lm1, which = 5) lm1 %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) -&gt; a #plot(lm1, which = 6) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii]/(1-h[ii]))) -&gt; b 1.2.7 Piège et astuces extrapolation regression peut être sifgnificatif de manière fortuite tester n’importe quelle variable avec n’improte quelle autre variable (parcimonie) calcule de l’écart type \\[s_{y|x}\\ =\\ \\sqrt{ \\frac{\\sum_{ }^{ }\\left(y_i-ŷ_i\\right)^2}{n-2}}\\] calcul de l IC \\[CI_{1-\\alpha}\\ =\\ ŷ_i\\ \\ \\pm t_{\\frac{\\alpha}{2}}^{n-2} \\frac{s_{y\\left|x\\right|}\\ }{\\sqrt{n}}\\] "],
["regression-lineaire-multiple.html", "1.3 Régression linéaire multiple", " 1.3 Régression linéaire multiple TODO "],
["regression-lineaire-polynomiale.html", "1.4 Régression linéaire polynomiale", " 1.4 Régression linéaire polynomiale TODO "],
["les-variables-facteurs.html", "1.5 Les variables facteurs", " 1.5 Les variables facteurs Le modèle linéaire permet d’analyser une relation linéaire entre deux variables. Jusqu’à présente, nous avons utilisé deux variables quantitative (si vous avez des doutes concernant les types de variables, relisez la section suivante : Type de variables ) . Dans le premier ouvrage, vous avez découvert l’analyse de variance dans le chapitre 10, il est indispensable de relire avec attention cette section au minimum. Avez vous remarqué une ressemblance particulière entre la regression linéaire que nous avons réalisé précédement et l’analyse de variance ? Les plus observateurs auront mis en avant la que la fonction est la même pour réaliser une régression linéaire et une analyse de variance. La fonction lm() est capable de traiter aussi bien des variables réponses qualitatives que quantitatives. 1.5.1 Matrice de contraste "],
["modele-lineaire.html", "1.6 Modèle linéaire", " 1.6 Modèle linéaire Le modèle linaire regroupe l’ensemble des régressions présentées précédement. Nous savons dorénavant la raison d’avoir une seule fonction unique pour l’ensemble des procédures ci dessus , la fonction lm() "],
["comparaison-des-modeles.html", "1.7 Comparaison des modèles", " 1.7 Comparaison des modèles Critère d’Akaike "],
["glm.html", "Module 2 Modèle linéaire généralisé", " Module 2 Modèle linéaire généralisé Objectifs TODO Prérequis TODO "],
["non-lineaire.html", "Module 3 Modèle non linéaire", " Module 3 Modèle non linéaire Objectifs TODO Prérequis TODO "],
["robuste-survie.html", "Module 4 Régression robuste/quantile &amp; analyse de survie", " Module 4 Régression robuste/quantile &amp; analyse de survie Objectifs TODO Prérequis TODO "],
["hierarchique.html", "Module 5 Classification hiérarchique", " Module 5 Classification hiérarchique Objectifs TODO Prérequis TODO "],
["k-moyenne-som.html", "Module 6 K-moyenne &amp; SOM", " Module 6 K-moyenne &amp; SOM Objectifs TODO Prérequis TODO "],
["k-moyennes.html", "6.1 K-moyennes", " 6.1 K-moyennes "],
["cartes-auto-adaptatives.html", "6.2 Cartes auto adaptatives", " 6.2 Cartes auto adaptatives La méthode des cartes auto-adaptatives se nomme self-organizing map (SOM) en anglais. "],
["acp-afc.html", "Module 7 ACP &amp; AFC", " Module 7 ACP &amp; AFC Objectifs TODO Prérequis TODO "],
["analyse-en-composantes-principales.html", "7.1 Analyse en composantes principales", " 7.1 Analyse en composantes principales "],
["analyse-factorielle-des-correspondances.html", "7.2 Analyse factorielle des correspondances", " 7.2 Analyse factorielle des correspondances "],
["afm.html", "Module 8 AFM", " Module 8 AFM Objectifs TODO Prérequis TODO "],
["analyse-factorielle-multiple-afm.html", "8.1 Analyse factorielle multiple (AFM)", " 8.1 Analyse factorielle multiple (AFM) L’analyse factorielle multiple (AFM) se nomme principal component analysis (PCA) en anglais. "],
["svbox.html", "A Installation de la SciViews Box", " A Installation de la SciViews Box Pour ce cours SDD 2, nous utiliserons la même SciViews Box… mais actualisée (version de l’année). Vous allez donc devoir installer la nouvelle version. La procédure n’a changé que sur des points de détails. Référez-vous à l’appendice A1 du cours SDD 1. Vous pouvez conserver l’ancienne SciViews Box en parallèle avec cette nouvelle version, mais vérifiez si vous avez assez d’espace sur le disque dur pour contenir les deux simultanément. Comptez par sécurité 20Go par version. Si vous manquez de place, vous pouvez éliminer l’ancienne version avant d’installer la nouvelle (vos projets ne seront pas effacés). TODO: procédure pour éliminer une ancienne version. "],
["migration-des-projets.html", "A.1 Migration des projets", " A.1 Migration des projets Concernant les projets réalisés dans une version précédente de la SciViews Box, ceux-ci restent disponibles, même si vous éliminez l’ancienne. Plusieurs cas de figure se présentent : Vous conserver deux ou plusieurs version de la SciViews Box en parallèle. Dans ce cas, nous conseillons fortement de garder chaque projet accessible à partir de la version dans laquelle il a été créé. Seulement les projets que vous décidez de migrer explicitement (voir ci-dessous) seront à déplacer dans le dossier shared de la nouvelle SciViews Box. Vous aurez à faire cette manipulation, par exemple, si vous devez recommencer un cours l’année suivante afin d’être en phase (même version de la svbox) par rapport à vos nouveaux collègues. Vous ne conservez que la dernière version de la SciViews Box, mais ne devez pas accéder fréquemment vos anciens projets, et dans ce cas, vous pouvez réinstaller temporairement l’ancienne version de svbox. Dans ce cas, ne migrez pas vos anciens projets. Éliminez simplement l’ancienne svbox, tout en laisant vos projets intacts dans son répertoire shared. Lors de la réinstallation de l’ancienne svbox, vous retrouverez alors tous vos anciens projets intactes. Vous ne conservez pas d’ancienne version de la svbox et vous ne souhaitez pas devoir la réinstaller. Il est possible de migrer vos anciens projets en les déplaçant de l’ancien répertoire shared vers le nouveau. Soyez toutefois conscients que vos documents R Markdown et scripts R ne fonctionneront pas forcément dans la nouvelle svbox et qu’une adaptation sera peut-être nécessaire ! "],
["configuration-git-et-github.html", "A.2 Configuration Git et Github", " A.2 Configuration Git et Github A chaque nouvelle installation de la SciViews Box, vous devez la reconfigurer via la boite de dialogue SciViews Box Configuration. En particulier, il est très important d’indiquer correctement votre identifiant et email Git (zone encadrée en rouge dans la copie d’écran ci-dessous). Assurez-vous (si ce n’est déjà fait) que vous possédez un compte Github valide. Vous pouvez cliquer sur le bouton Go to Github par facilté dans la même boite de dialogue. Choisissez de manière judicieuse votre login. Vous pourriez être amenés à l’utiliser bien plus longtemps que vous ne le pensez, y compris plus tard dans votre carrière. Donc, lisez les conseils ci-dessous (inspirés et adaptés de Happy Git and Github for the UseR - Register a Github Account : Incluez votre nom réel. Les gens aiment savoir à qui ils ont affaire. Rendez aussi votre nom/login facile à deviner et à retenir. Philippe Grosjean a comme login phgrosjean, par exemple. Vous pouvez réutiliser votre login d’autres contextes, par exemple Twitter ou Slack (ou Facebook). Choisissez un login que vous pourrez échanger de manière confortable avec votre futur boss. Un login plus court est préférable. Soyez unique dans votre login, mais à l’aide d’aussi peu de caractères que possible. Github propose parfois des logins en auto-complétion. Examinez ce qu’il propose. Rendez votre login invariable dans le temps. Par exemple, n’utilisez pas un login lié à votre université (numéro de matricule, ou nom de l’université inclue dans le login). Si tout va bien votre login vous suivra dans votre carrière, … donc, potentiellement loin de l’université où vous avez fait vos études. N’utilisez pas de logins qui sont aussi des mots ayant une signification particulière en programmation, par exemple, n’utilisez pas NA, même si c’est vos initiales ! Une fois votre compte Github créé, et votre login/email pour votre identification Git correctement enregistrés dans la SciViews Box, vous devez pouvoir travailler, faire des “pushs”, des “pulls” et des “commits”2. Cependant, RStudio vous demandera constamment vos logins et mots de passe… à la longue, c’est lassant ! La procédure ci-dessous vous enregistre une fois pour toutes sur votre compte Github dans RStudio. A.2.1 Compte Github dans RStudio RStudio offre la possibilité d’enregistrer une clé publique/privée dans votre SciViews Box afin de vous enregistrer sur Github de manière permanente. L’avantage, c’est que vous ne devrez plus constamment entrer votre login et mot de passe à chaque opération sur Github ! Nous vous le conseillons donc vivement. Entrez dans Rstudio Server, et allez dans le menu Tools -&gt; Global Options.... Ensuite, cliquez dans la rubrique Git/SVN dans la boite de dialogue. Ensuite, cliquez sur le bouton Create RSA key.... La phrase de passe n’est pas nécessaire (il est même préférable de la laisser vide si vous voulez utiliser Github sans rien devoir taper à chaque fois). Cliquez sur le bouton Create. Vous obtenez alors une fenêtre similaire à celle ci-dessous (bien sûr avec des données différentes). Ceci confirme que votre clé cryptographique a été créée localement. Fermez cette fenêtre pour revenir à la boite de dialogue de configuration de RStudio Server. Dans la boite de dialogue de configuration de RStudio Server, section Git/SVN cliquez sur le lien View public key qui apparait une fois la clé créée : La clé apparait dans une fenêtre, déjà présélectionnée. Copiez-là dans le presse-papier (Ctrl-C ou clic bouton droit et sélection de Copy dans le menu contextuel), puis fermez cette fenêtre. Dans votre navigateur web favori, naviguez vers https://github.com, loggez-vous, et accédez aux paramètres de votre compte Github (menu déroulant en haut à droite, entrée Settings) : Dans les paramètres de votre compte, cliquez sur la rubrique SSH and GPG keys, ensuite sur le bouton vert New SSH key Collez-y votre clé à partir du presse-papier dans la zone Key. Vous pouvez lui donner un nom évocateur dans le champ Title. Ensuite, cliquez sur Add SSH key. Déloggez, puis reloggez-vous dans RStudio Server pour que les changements soient pris en compte. La prochaine action sur Github depuis RStudio pourrait encore déclencher la demande de votre login et mot de passe, mais ensuite, les opérations devraient se faire directement. Si vous éprouvez toujours des difficultés à faire collaborer R et RStudio avec Git et Github, voyez https://happygitwithr.com (en anglais) qui explique les différentes procédures bien plus en détails. Vérifiez toujours lors de votre premier commit que Github vous reconnait bien. Pour cela, naviguez vers le dépôt où vous avez commité avec votre explorateur web, et vérifiez l’identité prise en compte lors de votre commit.↩ "],
["references.html", "Références", " Références Il existe tout de même des outils plus pointus pour obtenir de l’aide sur le logiciel R comme rseek.org, rdocumentation.org ou rdrr.io. Rien ne sert de chercher ’R’ dans Goggle.↩ Vérifiez toujours lors de votre premier commit que Github vous reconnait bien. Pour cela, naviguez vers le dépôt où vous avez commité avec votre explorateur web, et vérifiez l’identité prise en compte lors de votre commit.↩ "]
]
