[
["index.html", "Science des données biologiques 2 Préambule", " Science des données biologiques 2 Philippe Grosjean &amp; Guyliann Engels 2019-09-19 Préambule Cet ouvrage interactif est le second d’une série de trois ouvrages traitant de la science des données biologiques. L’écriture de cette suite de livres a débuté au cours de l’année académique 2018-2019. Pour l’année académique 2019-2020, cet ouvrage interactif sera le support du cours suivant : Science des données II : Analyse et modélisation, UMONS dont le responsable est Grosjean Philippe Cet ouvrage est conçu pour être utilisé de manière interactive en ligne. En effet, nous y ajoutons des vidéos, des démonstrations interactives, et des exercices sous forme de questionnaires interactifs également. Ces différents éléments ne sont, bien évidemment, utilisables qu’en ligne. Le matériel dans cet ouvrage est distribué sous licence CC BY-NC-SA 4.0. "],
["vue-generale-des-cours.html", "Vue générale des cours", " Vue générale des cours Le cours de Science des données II: analyse et modélisation est dispensé aux biologistes de troisième Bachelier en Faculté des Sciences de l’Université de Mons à partir de l’année académique 2019-2020. La matière est divisée en huit modules de 6h chacun en présentiel. Il nécessitera environ un tiers de ce temps (voir plus, en fonction de votre rythme et de votre technique d’apprentissage) en travail à domicile. Cette matière fait suite au premier cours dont le contenu est considéré comme assimilé (voir https://biodatascience-course.sciviews.org/sdd-umons/). La première moitié du cours est consacrée à la modélisation, un domaine particulièrement important de la science des données qui étend les concepts déjà vu au cours 1 d’analyse de variance et de corrélation entre deux variables. Ces quatre modules formeront aussi un socle sur lequel nous pourrons élaborer les techniques d’apprentissage machine (classification supervisée), et puis ensuite l’apprentissage profond à la base de l’intelligence artificielle qui seront abordées plus tard dans le cours 3. Cette partie est dense, mais ultra importante ! La seconde moitié s’intéressera à l’exploration des données, encore appelée analyse des données qui vise à découvrir des caractéristiques intéressantes dans des très gros jeux de données. Ces techniques sont d’autant plus utiles que les données volumineuses deviennent de plus en plus courantes en biologie. "],
["materiel-pedagogique.html", "Matériel pédagogique", " Matériel pédagogique Le matériel pédagogique, rassemblé dans ce syllabus interactif est aussi varié que possible. Vous pourrez ainsi piocher dans l’offre en fonction de vos envies et de votre profil d’apprenant pour optimiser votre travail. Vous trouverez: le présent ouvrage en ligne, des tutoriaux interactifs (réalisés avec un logiciel appelé learnr). Vous pourrez exécuter ces tutoriaux directement sur votre ordinateur, et vous aurez alors accès à des pages Web réactives contenant des explications, des exercices et des quizzs en ligne, des slides de présentations, des dépôts Github Classroom dans la section BioDataScience-Course pour réaliser et documenter vos travaux personnels. des renvois vers des documents externes en ligne, types vidéos youtube ou vimeo, des ouvrages en ligne en anglais ou en français, des blogs, des tutoriaux, des parties gratuites de cours Datacamp ou équivalents, des questions sur des sites comme “Stackoverflow” ou issues des “mailing lists” R, … Tout ce matériel est accessible à partir du site Web du cours, du présent syllabus interactif (et de Moodle pour les étudiants de l’UMONS). Ces derniers ont aussi accès au dossier SDD sur StudentTemp en Intranet à l’UMONS. Les aspects pratiques seront à réaliser en utilisant la ‘SciViews Box’, une machine virtuelle préconfigurée. Nous installerons ensemble la nouvelle version de cette SciViews Box au premier cours. Il est donc très important que vous soyez présent à ce cours, et vous pouvez venir aussi si vous le souhaitez avec votre propre ordinateur portable comme pour le cours 1. Enfin, vous pourrez poser vos questions par mail à l’adresse sdd@sciviews.org. System information sessioninfo::session_info() # ─ Session info ────────────────────────────────────────────────────────── # setting value # version R version 3.5.3 (2019-03-11) # os Ubuntu 18.04.2 LTS # system x86_64, linux-gnu # ui X11 # language (EN) # collate en_US.UTF-8 # ctype en_US.UTF-8 # tz Europe/Brussels # date 2019-09-19 # # ─ Packages ────────────────────────────────────────────────────────────── # package * version date lib source # assertthat 0.2.1 2019-03-21 [2] CRAN (R 3.5.3) # bookdown 0.9 2018-12-21 [2] CRAN (R 3.5.3) # cli 1.1.0 2019-03-19 [2] CRAN (R 3.5.3) # colorspace 1.4-1 2019-03-18 [2] CRAN (R 3.5.3) # crayon 1.3.4 2017-09-16 [2] CRAN (R 3.5.3) # digest 0.6.18 2018-10-10 [2] CRAN (R 3.5.3) # dplyr 0.8.0.1 2019-02-15 [2] CRAN (R 3.5.3) # evaluate 0.13 2019-02-12 [2] CRAN (R 3.5.3) # farver 1.1.0 2018-11-20 [2] CRAN (R 3.5.3) # gganimate 1.0.3 2019-04-02 [2] CRAN (R 3.5.3) # ggplot2 3.1.1 2019-04-07 [2] CRAN (R 3.5.3) # glue 1.3.1 2019-03-12 [2] CRAN (R 3.5.3) # gtable 0.3.0 2019-03-25 [2] CRAN (R 3.5.3) # hms 0.4.2 2018-03-10 [2] CRAN (R 3.5.3) # htmltools 0.3.6 2017-04-28 [2] CRAN (R 3.5.3) # inline 0.3.15 2018-05-18 [2] CRAN (R 3.5.3) # knitr 1.22 2019-03-08 [2] CRAN (R 3.5.3) # lazyeval 0.2.2 2019-03-15 [2] CRAN (R 3.5.3) # magick 2.0 2018-10-05 [2] CRAN (R 3.5.3) # magrittr 1.5 2014-11-22 [2] CRAN (R 3.5.3) # munsell 0.5.0 2018-06-12 [2] CRAN (R 3.5.3) # pillar 1.3.1 2018-12-15 [2] CRAN (R 3.5.3) # pkgconfig 2.0.2 2018-08-16 [2] CRAN (R 3.5.3) # plyr 1.8.4 2016-06-08 [2] CRAN (R 3.5.3) # prettyunits 1.0.2 2015-07-13 [2] CRAN (R 3.5.3) # progress 1.2.0 2018-06-14 [2] CRAN (R 3.5.3) # purrr 0.3.2 2019-03-15 [2] CRAN (R 3.5.3) # R6 2.4.0 2019-02-14 [2] CRAN (R 3.5.3) # Rcpp 1.0.1 2019-03-17 [2] CRAN (R 3.5.3) # rlang 0.3.4 2019-04-07 [2] CRAN (R 3.5.3) # rmarkdown 1.12 2019-03-14 [2] CRAN (R 3.5.3) # rstudioapi 0.10 2019-03-19 [2] CRAN (R 3.5.3) # scales 1.0.0 2018-08-09 [2] CRAN (R 3.5.3) # sessioninfo 1.1.1 2018-11-05 [2] CRAN (R 3.5.3) # stringi 1.4.3 2019-03-12 [2] CRAN (R 3.5.3) # stringr 1.4.0 2019-02-10 [2] CRAN (R 3.5.3) # tibble 2.1.1 2019-03-16 [2] CRAN (R 3.5.3) # tidyselect 0.2.5 2018-10-11 [2] CRAN (R 3.5.3) # tweenr 1.0.1 2018-12-14 [2] CRAN (R 3.5.3) # withr 2.1.2 2018-03-15 [2] CRAN (R 3.5.3) # xfun 0.6 2019-04-02 [2] CRAN (R 3.5.3) # yaml 2.2.0 2018-07-25 [2] CRAN (R 3.5.3) # # [1] /home/sv/R/x86_64-pc-linux-gnu-library/3.5 # [2] /usr/local/lib/R/site-library # [3] /usr/lib/R/site-library # [4] /usr/lib/R/library "],
["lm.html", "Module 1 Régression linéaire", " Module 1 Régression linéaire Objectifs Retrouver ses marques avec R, RStudio et la SciViews Box et découvrir les fonctions supplémentaires de la nouvelle version. Découvrir la régression linaire de manière intuitive. Appréhender les différentes formes de régressions linéaires par les moindres carrés. Choisir sa régression linéaire de manière judicieuse. Savoir utiliser les outils de diagnostic de la régression linéaire, en particulier l’analyse des résidus. Prérequis Avant de nous lancer tête baissée dans de la matière nouvelle, nous allons installer la dernière version de la SciViews Box. Une nouvelle version est disponible chaque année début septembre. Reportez-vous à l’appendice A pour son installation et pour la migration éventuelle de vos projets depuis la version précédente. Une fois la box installée, consacrez un petit quart d’heure à repérer les icônes nouvelles dans le Dock et dans le menu Applications. Vous retrouverez R et RStudio, mais dans des version plus récentes qui apportent également leur lot de nouveautés. Lancez RStudio et repérez ici aussi les nouveaux onglets et les nouvelles entrées de menu. Aidez-vous de l’aide en ligne ou de recherches sur le net pour vous familiariser avec ces nouvelles fonctionnalités. Une fois la nouvelle SciViews Box fonctionnelle sur votre ordinateur, vous allez réaliser une séance d’exercice couvrant les points essentiels des notions abordées dans le livre science des données biologiques partie 1, histoire de rafraîchir vos connaissances. Les tutoriaux learnr auxquels vous êtes maintenant habitués seront là pour vous aider à auto-évaluer votre progression. Pour le cours 2, ces tutoriaux sont dans le package BioDataScience2 que vous venez normalement d’installer si vous avez bien suivi toutes les instructions de configuration de votre SciViews Box (sinon, vérifiez votre configuration). Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01a_rappel&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["modele.html", "1.1 Modèle", " 1.1 Modèle Qu’est-ce qu’un “modèle” en science des données et en statistique ? Il s’agit d’une représentation simplifiée sous forme mathématique du mécanisme responsable de la distribution des observations. Rassurez-vous, dans ce module, le côté mathématique du problème sera volontairement peu développé pour laisser une large place à une compréhension intuitive du modèle. Les seules notions clés à connaître ici concernent l’équation qui définit une droite quelconque dans le plan \\(xy\\) : \\[y = a \\ x + b\\] Cette équation comporte : deux variables \\(x\\) et \\(y\\) qui sont matérialisées par les axes des abscisses et des ordonnées dans le plan \\(xy\\). Ces variables prennent des valeurs bien définies pour les observations réalisées sur chaque individu du jeu de données. deux paramètres \\(a\\) et \\(b\\), respectivement la pente de la droite (\\(a\\)) et son ordonnée à l’origine (\\(b\\)). Ecrit de la sorte, \\(a\\) et \\(b\\) peuvent prendre n’importe quelle valeur et l’équation définit de manière généraliste toutes les droites possibles qui existent dans le plan \\(xy\\). Paramétrer ou paramétriser le modèle consiste à définir une et une seule droite en fixant les valeurs de \\(a\\) et de \\(b\\). Par exemple, si je décide de fixer \\(a = 0.35\\) et \\(b = -1.23\\), mon équation définit maintenant une droite bien précise dans le plan \\(xy\\) : \\[y = 0,35 \\ x - 1.23\\] La distinction entre variable et paramètre dans les équations précédentes semble difficile pour certaines personnes. C’est pourtant crucial de pouvoir le faire pour bien comprendre la suite. Alors, c’est le bon moment de relire attentivement ce qui est écrit ci-dessus et de le mémoriser avant d’aller plus avant ! 1.1.1 Pourquoi modéliser ? Le but de la modélisation consiste à découvrir l’équation mathématique de la droite (ou plus généralement, de la fonction) qui décrit au mieux la forme du nuage de points matérialisant les observations dans le plan \\(xy\\) (ou plus généralement dans un hyper-espace représenté par les différentes variables mesurées). Cette équation mathématique peut ensuite être utilisée de différentes façons, toutes plus utiles les unes que les autres : Aide à la compréhension du mécanisme sous-jacent qui a généré les données. Par exemple, si une droite représente bien la croissance pondérale d’un organisme dans le plan représenté par le logarithme du poids (P) en ordonnée et le temps (t) en abscisse, nous pourrons déduire que la croissance de cet organisme est probablement un mécanisme de type exponentiel (puisqu’une transformation inverse, c’est-à-dire logarithmique, linéarise alors le nuage de points). Attention ! Le modèle n’est pos le mécanisme sous-jacent de génération des données, mais utilisé habilement, ce modèle peut donner des indices utiles pour aider à découvrir ce mécanisme. Effectuer des prédictions. Le modèle paramétré pourra être utilisé pour prédire, par exemple, le poids probable d’un individu de la même population après un certain laps de temps. Comparer différents modèles. En présence de plusieurs populations, nous pourrons ajuster un modèle linéaire pour chacune d’elles et comparer ensuite les pentes des droites pour déterminer quelle population a le meilleur ou le moins bon taux de croissance. Explorer les relations entre variables. Sans aucunes connaissances sur le contexte qui a permit d’obtenir nos données, un modèle peut fournir des informations utiles pour orienter les recherches futures. Idéalement, un modèle devrait pouvoir servir à ces différentes applications. En pratique, comme le modèle est forcément une simplification de la réalité, des compromis doivent être concédés pour arriver à cette simplification. En fonction de son usage, les compromis possibles vont différer. Il s’en suit une spécialisation des modèles en modèles mécanistiques qui décrivent particulièrement bien le mécanisme sous-jacent (fréquents en physique, par exemple), les modèles prédictifs conçus pour calcluer des nouvelles valeurs (que l’intelligence artificielle affectionne particulièrement), les modèles comparatifs, et enfin, les modèles exploratroires (utilisés dans la phase initiale de découverte et de description des données). Retenez simplement qu’un même modèle est rarement efficace sur les quatre tableaux simultanément. 1.1.2 Quand modéliser ? A chaque fois que deux ou plusieurs variables (quantitatives dans le cas de la régression) forment un nuage de points qui présente une forme particulière non sphérique, autrement dit, qu’une corrélation significative existe dans les données, un modèle peut être utile. Etant donné deux variables quantitatives, trois niveaux d’association de force croissante peuvent être définies entre ces deux variables : La corrélation quantifie juste l’allongement dans une direction préférentielle du nuage de points à l’aide des coefficients de corrélation linéaire de Pearson ou non linéaire de Spearman. Ce niveau d’association a été traité dans le module 12 du cours 1. Il est purement descriptif et n’implique aucunes autres hypothèses sur les données observées. La relation considère que la corrélation observée entre les deux variables est issue d’un mécanisme sous-jacent qui nous intéresse. Un modèle mathématique de l’association entre les deux variables matérialise de manière éventuellement simplifiée, ce mécanisme. Il permet de réaliser ensuite des calculs utiles. Nous verrons plus loin que des contraintes plus fortes doivent être supposées concernant le distribution des deux variables. La causalité précise encore le mécanisme sous-jacent dans le sens qu’elle exprime le fait que c’est la variation de l’une de ces variables qui est directement ou indirectement la cause de la variation de la seconde variable. Bien que des outils statistiques existent pour inférer une causalité (nous ne les aborderons pas dans ce cours), la causalité est plutôt étudiée via l’expérimentation : le biologiste contrôle et fait varier la variable supposée causale, toutes autres conditions par ailleurs invariables dans l’expérience. Il mesure alors et constate si la seconde variable répond ou non à ces variations1 et en déduit une causalité éventuelle. La distinction entre ces trois degrés d’association de deux variables est cruciale. Il est fréquent d’observer une confusion entre corrélation (ou relation) et causalité chez ceux qui ne comprennent pas bien la différence. Cela peut mener à des interprétations complètement erronées ! Comme ceci est à la fois crucial mais subtil, voici une vidéo issue de la série “les statistiques expliquées à mon chat” qui explique clairement le problème. Une troisième variable confondante peut en effet expliquer une corrélation, rendant alors la relation et/ou la causalité entre les deux variables fallacieuse… 1.1.3 Entraînement et confirmation En statistique, une règle universelle veut qu’une observation ne peut servir qu’une seule fois. Ainsi, toutes les données utilisées pour calculer le modèle ne peuvent pas servir simultanément à la confirmer. Il faut échantillonner d’autres valeurs pour effectuer cette confirmation. Il s’en suit une spécialisation des jeux de données en : jeu d’entraînement qui sert à établir le modèle jeu de confirmation ou de test qui sert à vérifier que le modèle est génaralisable car il est capable de prédire le comportement d’un autre jeu de données indépendant issu de la même population statistique. C’est une pratique cruciale de toujours confirmer son modèle, et donc, de prendre soin de séparer ses données en jeu d’entraînement et de test. Les bonnes façons de faire cela seront abordées au cours 3 dans la partie consacrée à l’apprentissage machine. Ici, nous nous focaliserons uniquement sur l’établissement du modèle dans la phase d’entraînement. Par conséquent, nous utiliserons toutes nos données pour cet entraînement, mais qu’il soit d’amblée bien clair qu’une confirmation du modèle est une seconde phase également indispensable. En biologie, le vivant peut être étudié essentiellement de deux manières complémentaires : par l’observation du monde qui nous entoure sans interférer, ou le moins possible, et par l’expérimentation où le biologiste fixe alors très précisément les conditions dans lesquelles il étudie ses organismes cibles. Les deux approches se prêtent à la modélisation mais seule l’expérimentation permet d’inférer avec certitude la causalité.↩ "],
["regression-lineaire-simple.html", "1.2 Régression linéaire simple", " 1.2 Régression linéaire simple Nous allons découvrir les bases de la régression linéaire de façon intuitive. Nous utilisons le jeu de données trees qui rassemble la mesure du diamètre, de la hauteur et du volume de bois de cerisiers noirs. # importation des données trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) Rapellons-nous que dans le chapitre 12 du livre science des données 1, nous avons étudié l’association de deux variables quantitatives (ou numériques). Nous utilisons donc une matrice de corrélation afin de mettre en évidence la corrélation entre nos trois variables qui composent le jeu de donnée trees. La fonction correlation() nous renvoie un tableau de la matrice de correlation avec l’indice de Pearson (corrélation linéaire) par défaut. C’est précisement ce coefficient qui nous intéresse dans le cadre d’une régression linéaire comme description préalable des données autant que pour nous guider dans le choix de nos variables. (trees_corr &lt;- correlation(trees)) # Matrix of Pearson&#39;s product-moment correlation: # (calculation uses everything) # diameter height volume # diameter 1.000 0.519 0.967 # height 0.519 1.000 0.597 # volume 0.967 0.597 1.000 Nous pouvons également observer cette matrice sous la forme d’un graphique plus convivial. plot(trees_corr, type = &quot;lower&quot;) Cependant, n’oubliez pas qu’il est indispensable de visualiser les nuages de points pour ne pas tomber dans le piège mis en avant par le jeu de données artificiel appelé “quartet d’Anscombe” qui montre très bien comment des données très différentes peuvent avoir même moyenne, même variance et même coefficient de corrélation. Un graphique de type matrice de nuages de points est tout indiqué ici. GGally::ggscatmat(as.data.frame(trees), 1:3) Nous observons une plus forte corrélation linéaire entre le volume et le diamètre. Intéressons nous à cette association. chart(trees, volume ~ diameter) + geom_point() Si vous deviez ajouter une droite permettant de représenter au mieux les données, où est ce que vous la placeriez ? Pour rappel, une droite respecte l’équation mathématique suivante : \\[y = a \\ x + b\\] dont a est la pente (slope en anglais) et b est l’ordonnée à l’origine (intercept en anglais). # Sélection de pentes et d&#39;ordonnées à l&#39;origine models &lt;- tibble( model = paste(&quot;model&quot;, 1:4, sep = &quot;-&quot;), slope = c(5, 5.5, 6, 0), intercept = c(-0.5, -0.95, -1.5, 0.85) ) chart(trees, volume ~ diameter) + geom_point() + geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + labs( color = &quot;Modèle&quot;) Nous avons quatre droites candidates pour représenter au mieux les observations. Quel est la meilleure d’entre elles selon vous ? 1.2.1 Quantifier l’ajustement d’un modèle Nous voulons identifier la meilleure régression, c’est-à-dire la régression le plus proche de nos données. Nous avons besoin d’une règle qui va nous permettre de quantifier la distance de nos observations à notre modèle afin d’obtenir la régression avec la plus faible distance possible de l’ensemble de nos observations. Décomposons le problème étape par étape et intéressons nous au model-1 (droite en rouge sur le graphique précédent). Calculer les valeurs de \\(y_i\\) prédites par le modèle que nous noterons par convention \\(\\hat y_i\\) (prononcez “y chapeau” ou “y hat” en anglais) pour chaque observation \\(i\\). # Calculer la valeur de y pour chaque valeur de x suivant le model souhaité # Création de notre fonction model &lt;- function(slope, intercept, x) { prediction &lt;- intercept + slope * x attributes(prediction) &lt;- NULL prediction } # Application de notre fonction yhat &lt;- model(slope = 5, intercept = -0.5, x = trees$diameter) # Affichage des résultats yhat # [1] 0.555 0.590 0.620 0.835 0.860 0.870 0.895 0.895 0.910 0.920 0.935 # [12] 0.950 0.950 0.985 1.025 1.140 1.140 1.190 1.240 1.255 1.280 1.305 # [23] 1.340 1.530 1.570 1.695 1.720 1.775 1.785 1.785 2.115 Calculer la distance entre les observations \\(y_i\\) et les prédictions par notre modèle \\(\\hat y_i\\), soit \\(y_i - \\hat y_i\\) Les distances que nous souhaitons calculer, sont appelées les résidus du modèle et sont notés \\(\\epsilon_i\\) (epsilon). Nous pouvons premièrement visualiser ces résidus graphiquement (ici en rouge par rapport à model-1) : Nous pouvons ensuite facilement calculer leurs valeurs comme ci-dessous : # Calculer la distance entre y et y barre # Création de notre fonction de calcul des résidus distance &lt;- function(observations, predictions) { residus &lt;- observations - predictions attributes(residus) &lt;- NULL residus } # Utilisation de la fonction resid &lt;- distance(observations = trees$volume, predictions = yhat) # Impression des résultats resid # [1] -0.263 -0.298 -0.331 -0.371 -0.328 -0.312 -0.453 -0.380 -0.270 -0.357 # [11] -0.250 -0.355 -0.344 -0.382 -0.484 -0.511 -0.183 -0.414 -0.512 -0.550 # [21] -0.303 -0.407 -0.312 -0.445 -0.364 -0.126 -0.143 -0.124 -0.327 -0.341 # [31] 0.065 Définir une règle pour obtenir une valeur unique qui résume l’ensemble des distances de nos observations par rapport aux prédictions du modèle. Une première idée serait de sommer l’ensemble de nos résidus comme ci-dessous : sum(resid) # [1] -10.175 Appliquons ces calculs sur nos quatre modèles afin de les comparer… Le modèle pour lequel notree critère serait le plus proche de zéro serait alors considéré comme le meilleur. model-1 model-2 model-3 model-4 -10.175 -1.441 10.393 0.135 Selon notre méthode, il en ressort que le modèle 4 est le plus approprié pour représenter au mieux nos données. Qu’en pensez vous ? Intuitivement, nous nous aperçevons que le modèle 4 est loin d’être le meilleur. Nous pouvons en déduire que la somme des résidus n’est pas un bon critère pour ajuster un modèle linéaire. Le problème lorsque nous sommons des résidus, est la présence de résidus positifs et de résidus négatifs (ici par rapport à model-4). Ainsi avec notre première méthode naïve de somme des résidus, il suffit d’avoir autant de résidus positifs que négatifs pour avoir un résultat proche de zéro. Mais cela n’implique pas que les observations soient prochent de la droite pour autant. Avez-vous une autre idée que de sommer les résidus ? Sommer le carré des résidus aurait des propriétés intéressantes car d’une part les carrés de nombres positifs et négatifs sont tous positifs, et d’autre part, plus une observation est éloignée plus sa distance au carré pèse fortement dans la somme2. Nous obtenons les résultats suivants : model-1 model-2 model-3 model-4 3.842211 0.4931095 3.929195 6.498511 Sommer les valeurs absolues des résidus mène également à des contributions toutes positives, mais sans pénaliser outre mesure les observations les plus éloignées. Nous obtenons les résultats suivants : model-1 model-2 model-3 model-4 10.305 3.186 10.393 11.525 Nous avons trouvé deux solutions intéressantes pour quantifier la distance de notre droite par rapport à nos observations. En effet, dans les deux cas, la valeur minimale est obtenue pour le model-2 (en vert sur le graphique) qui est visuellement le meilleur des quatre. La méthode utilisant les carrés des résidus s’appelle une régression par les moindres carrés. Notre objectif est donc de trouver les meilleures valeurs des paramètres \\(a\\) et \\(b\\) de la droite pour minimiser ce critère. Il en résulte une fonction dite objective qui dépend de \\(x\\) et de nos paramètres \\(a\\) et \\(b\\) à minimiser. Cette approche s’appelle la régression par les moindres carrés et elle est la plus utilisée. L’approche utilisant la somme de la valeur absolue des résidus est également utilisable (et elle est d’ailleurs préférable en présence de valeurs extrêmes potentiellement suspectes). Elle s’apppelle régression par la médiane, un cas particulier de la régression quantile que nous aborderons dans le module 4. 1.2.2 Trouver la meilleure droite Essayez de trouver le meilleur modèle par vous-même dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;01a_lin_mod&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/01a_lin_mod&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. Nous pouvons nous demander si notre modèle 2 qui est la meilleure droite de nos quatre modèles est le meilleur modèle possible dans l’absolu. Pour se faire nous allons devoir définir unez technique d’optimisation qui nous permet de déterminer quelle est la droite qui minimise notre fonction objective. Dans la suite, nous garderonsq le critère des moindres carrés (des résidus). Imaginons que nous n’avons pas quatre mais 5000 modèles linéaires avec des pentes et des ordonnées à l’origine différentes. Quelle est la meilleure droite ? set.seed(34643) models1 &lt;- tibble( intercept = runif(5000, -5, 4), slope = runif(5000, -5, 15)) chart(trees, volume ~ diameter) + geom_point() + geom_abline(aes(intercept = intercept, slope = slope), data = models1, alpha = 1/6) Nous voyons sur le graphique qu’un grand nombre de droites différentes sont testées, mais nous ne distinguons pas grand chose de plus. Cependant, sur ces 5000 modèles, nous pouvons maintenant calculer la somme des carrés des résidus et ensuite déterminer quel est le meilleur d’entre eux. # Fonction de calcul de la somme des carrés des résidus measure_distance &lt;- function(slope, intercept, x, y) { ybar &lt;- x * slope + intercept resid &lt;- y - ybar sum(resid^2) } # Test de la fonction #measure_distance(slope = 0, intercept = 0.85, x = trees$diameter, y = trees$volume) # Fonction adaptée pour être employé avec purrr:map() pour distribuer le calcul trees_dist &lt;- function(intercept, slope) { measure_distance(slope = slope, intercept = intercept, x = trees$diameter, y = trees$volume) } models1 &lt;- models1 %&gt;% mutate(dist = purrr::map2_dbl(intercept, slope, trees_dist)) Si nous réalisons un graphique de valeurs de pentes, d’ordonnées à l’origine et de la valeur de la fonction objective (distance) en couleur, nous obtenons le graphique ci-dessous. plot &lt;- chart(models1, slope ~ intercept %col=% dist) + geom_point() + geom_point(data = filter(models1, rank(dist) &lt;= 10), shape = 1, color = &#39;red&#39;, size = 3) + labs( y = &quot;Pente&quot;, x = &quot;Ordonnée à l&#39;origine&quot;, color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) plotly::ggplotly(plot) Les 10 valeurs les plus faibles sont mises en évidence sur le graphique par des cercles rouges. Le modèle optimal que nous recherchons se trouve dans cette région. best_models &lt;- models1 %&gt;.% filter(., rank(dist) &lt;= 10) Nous pouvons afficher les 10 meilleurs modèles sur notre graphique : chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + labs(color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) En résumé, nous avons besoin d’une fonction qui calcule la distance d’un modèle par rapport à nos observations et d’un algorithme pour la minimiser. Il n’est cependant pas nécessaire de chercher pendant des heures la meilleure fonction et le meilleur algorithme. Il existe dans R, un outil spécifiquement conçu pour adapter des modèles linéaires sur des observations, la fonction lm(). Dans le cas particulier de la régression par les moindres carrés, la solution s’obtient très facilement par un simple calcul : \\[a = \\frac{cov_{x, y}}{var_x} \\ \\ \\ \\textrm{et} \\ \\ \\ b = \\bar y - a \\ \\bar x\\] où \\(\\bar x\\) et \\(\\bar y\\) sont les moyennes pour les deux variables, \\(cov\\) est la covariance et \\(var\\) est la variance. Vous avez à votre disposition des snippets dédiés aux modèles linéaires (tapez ..., ensuite choisissez models, ensuite models : linear et choisissez le snippet qui vous convient dans la liste. (lm. &lt;- lm(data = trees, volume ~ diameter)) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Coefficients: # (Intercept) diameter # -1.047 5.652 Nous pouvons reporter ces valeurs sur notre graphique afin d’observer ce résultat par rapport à nos modèles aléatoires. nous avons en rouge la droite calculée par la fonction lm(). chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) 1.2.3 La fonction lm() Suite à notre découverte de manière intuitive de la régression linéaire simple, nous pouvons récapituler quelques points clés : Une droite suit l’équation mathématique suivante : \\[y = a \\ x + b\\] dont \\(a\\) est la pente (slope en anglais) et \\(b\\) est l’ordonnée à l’origine (intercept en anglais), tous deux les paramètres du modèle, alors que \\(x\\) et \\(y\\) en sont les variables. La distance entre une valeur observée \\(y_i\\) et une valeur prédite \\(\\hat y_i\\) se nomme le résidu (\\(\\epsilon_i\\)) et se mesure toujours parallèlement à l’axe \\(y\\). Cela revient à considérer que toute l’erreur du modèle se situe sur \\(y\\) et non sur \\(x\\). Cela donne l’équation complète de notre modèle statistique : \\[y_i = a \\ x_i + b + \\epsilon_i\\] avec \\(y_i\\) est la valeur mesurée pour le point i sur l’axe y, \\(a\\) est la pente, \\(x_i\\) est la valeur mesurée pour le point i sur l’axe x, b est l’ordonnée à l’origine et \\(\\epsilon_i\\) les résidus. On peut montrer (nous ne le ferons pas ici pour limiter les développements mathématiques) que le choix des moindres carrés des résidus comme fonction objective revient à considérer que nos résidus suivent une distribution normale centrée autour de zéro et avec un écart type \\(\\sigma\\) constant/ : \\[\\epsilon_i \\approx N(0, \\sigma)\\] Dans le cas de la régression linéaire simple, la meilleure droite s’obtient très facilement par la minimisation de la somme des carrés des résidus. En effet, la pente \\(a = \\frac{cov_{x, y}}{var_x}\\) et l’ordonnée à l’origine \\(b = \\bar y - a \\ \\bar x\\). La fonction lm() permet de faire ce calcul très facilement dans R. # Régression linéaire lm. &lt;- lm(data = trees, volume ~ diameter) # Graphique de nos observations et de la droite obtenue avec la fonction lm() chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) La fonction lm() crée un objet spécifique qui contient de nombreuses informations pour pouvoir ensuite analyser notre modèle linéaire. La fonction class() permet de mettre en avant la classe de notre objet. class(lm.) # [1] &quot;lm&quot; 1.2.4 Résumé à l’aide de summary() Avec la fonction summary() nous obtenons un résumé condensé des informations les plus utiles pour interpréter notre régression linéaire. summary(lm.) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.231211 -0.087021 0.003533 0.100594 0.271725 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.04748 0.09553 -10.96 7.85e-12 *** # diameter 5.65154 0.27649 20.44 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1206 on 29 degrees of freedom # Multiple R-squared: 0.9351, Adjusted R-squared: 0.9329 # F-statistic: 417.8 on 1 and 29 DF, p-value: &lt; 2.2e-16 Call : Il s’agit de la formule employée dans la fonction lm(). C’est à dire le volume en fonction du diamètre du jeu de données trees. Residuals : La tableau nous fournit un résumé via les 5 nombres de l’ensemble des résidus (que vous pouvez récupérer à partir de lm.$residuals). fivenum(lm.$residuals) # 20 7 12 9 31 # -0.231210947 -0.087020695 0.003532709 0.100594122 0.271724973 Coefficients : Il s’agit des résultats associés à la pente et à l’ordonnée à l’origine dont les valeurs estimées des paramètres (Estimate). Les mêmes valeurs peuvent être obtenues à partir de lm.$coefficients : lm.$coefficients # (Intercept) diameter # -1.047478 5.651535 On retrouve également les écart-types calculés sur ces valeurs (Std.Error) qui donnent une indication de la précision de leur estimation, les valeurs des distibutions de Student sur les valeurs estimées (t value) et enfin les valeurs p (PR(&gt;|t|)) liées à un test de Student pour déterminer si le paramètre p correspondant est significativement différent de zéro (avec \\(H_0: p = 0\\) et \\(H_a: p \\neq 0\\)). A partir des données de ce résumé, nous pouvons maintenant paramétrer l’équation de notre modèle : \\[y = ax + b\\] devient3 : \\[volume \\ de \\ bois = 5.65 \\ diamètre \\ à \\ 1.4 \\ m - 1.05 \\] Residual standard error : Il s’agit de l’écart-type résiduel, considérant que les debrés de liberté du modèle est le nombre d’observations \\(n\\) soustrait du nombre de paramètres à estimer (ici 2). \\[\\sqrt{\\frac{\\sum(y_i - ŷ_i)^2}{n-2}}\\] Multiple R-squared : Il s’agit de la valeur du coefficient \\(R^2\\) qui exprime la fraction de variance exprimé par le modèle. Souvenons nous que la variance totale respecte la propiété d’additivité. La variance conditionnelle \\(s^2_{y\\left|x\\right|}\\) peut être décomposée comme une somme de carrés (\\(SC\\)) divisés par des degrés de liberté associés, avec : \\[SC(total) = SC(reg) + SC(résidus)\\] \\[SC(total) = \\sum_{i=0}^n(y_i - \\bar y_i)^2\\] \\[SC(reg) = \\sum_{i=0}^n(ŷ_i - \\bar y_i)^2\\] \\[SC(résidus) = \\sum_{i=0}^n(y_i - ŷ_i)^2\\] A partir de la décomposition de ces sommes de carrés, le coefficient \\(R^2\\) se définit comme : \\[R^2 = \\frac{SC(reg)}{SC(total)}\\] La valeur du \\(R^2\\) est comprise entre 0 (lorsque le modèle est très mauvais et n’explique rien) et 1 (lorsque le modèle est parfait et “capture” toute la variance des données ; dans ce cas, tous les résidus valent zéro). Donc, plus le coefficient \\(R^2\\) se rapproche de un, plus le modèle explique bien les données. Adjusted R-squared: La valeur du coefficient \\(R^2\\) ajustée. Le calcul de cette valeur sera abordé dans la suite de ce livre. F-statistic : Tout comme pour l’ANOVA, le test de la significativité de la régression car \\(MS(reg)/MS(résidus)\\) suit une distribution F à respectivement 1 et \\(n-2\\) degré de liberté, avec \\(MS\\) les carrés moyens, c’est-à-dire les sommes des carrés \\(SC\\) divisés par leurs degrés de liberté respectifs. p-value : Il s’agit de la valeur p associé à la statistique de F, donc à l’ANOVA associée à la régression linéaire. Utiliser le carré des résidus a aussi d’autres propriétés statistiques intéressantes qui rapprochent ce calcul de la variance (qui vaut la somme de la distance au carré à la moyenne pour une seule variables numérique).↩ Lors de la paramétrisation du modèle, pensez à arrondir la valeur des paramètres à un nombre de chiffres significatifs raisonnables. Inutile de garder 5, ou même 3 chiffres derrière la virgule si vous n’avez que quelques dizaines d’obserrvations pour ajuster votre modèle.↩ "],
["outils-de-diagnostic.html", "1.3 Outils de diagnostic", " 1.3 Outils de diagnostic Une fois la meilleure droite de régression obtenue, le travail est loin d’être terminé. Il se peut que le nuage de point ne soit pas tout-à-fait linéaire, que sa dispesion ne soit pas homogène, que les résidus n’aient pas une distribution normale, qu’il existe des valeurs extrêmes aberrantes, ou qui tirent la droite vers alles de manière excessive. Nous allons maintenant devoir diagnostiquer ces possibles problèmes. L’analyse des résidus permet de le faire. Ensuite, si deux ou plusieurs modèles sont utilisable, il nous faut décider lequel conserver. Enfin, nous pouvons aussi calculer et visualiser l’enveloppe de confiance du modèle et extraire une série de données de ce modèle. 1.3.1 Analyse des résidus Le tableau numérique obtenu à l’aide de summary() peut faire penser que l’étude d’une régression linéaire se limite à quelques valeurs numériques et divers tests d’hypothèses associés. C’est un premier pas, mais c’est oublier que la technique est essentiellement visuelle. Le graphique du nuage de points avec la droite superposée est un premier outil diagnostic visuel indispensable, mais il n’est pas le seul ! Plusieurs graphiques spécifiques existent pour mettre en évidence diverses propriétés des résidus qui peuvent révéler des problèmes. Leur inspection est indispensable et s’appelle l’analyse des résidus. Les différents graphiques sont faciles à obtenir à partir des snippets. Le premier de ces graphique permet de vérifier la distribution homogène des résidus. Dans une bonne régression, nous aurons une distribution équilibrée de part et d’autre du zéro sur l’axe Y. Que pensez vous de notre graphique d’anayse des résidus ? Nous avons une valeur plus éloignée du zéro qui est mise en avant par la courbe en bleue qui montre l’influence générale des résidus. #plot(lm., which = 1) lm. %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) Le second graphique permet de vérifier la normalité des résidus par rapport à une distribution normale. #plot(lm., which = 2) lm. %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) Le troisième graphique va standardiser les résidus, et surtout, en prendre la racine carréE. Cela a pour effet de superposer les résidus négatifs sur les résidus positifs. Nous y diagnostiquons beaucoup plus facilement des problèmes de distribution de ces résidus. A nouveau, nous pouvons observer qu’une valeur influence fortement la courbe bleue. #plot(lm., which = 3) lm. %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) Le quatrième graphique met en évidence l’influence des individus sur la régression linéaire. Effectivement, la régression linéaire est sensible aux valeurs extrêmes. Nous pouvons observer que nous avons une valeur qui influence fortement notre régression. On utilise pour ce faire la distance de Cook que nous ne détailleront pas dans le cadre de ce cours. #plot(lm., which = 4) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Le cinquième grapique utilise l’effet de levier (Leverage) qui met également en avant l’influence des individus sur notre régression. Il répond à la question suivante : “est-ce qu’un ou plusieurs points sont tellement influents qu’ils tirent la régression vers eux de manière abusive ?” Nous avons à nouveau une valeur qui influence notre modèle. #plot(lm., which = 5) lm. %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) Le sixième graphique met en relation la distance de Cooks et l’effet de levier. Notre unique point d’une valeur supérieur à 0.5 m de diamètre influence fortement notre modèle. #plot(lm., which = 6) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii]/(1-h[ii]))) A l’issue de l’analyse des résidus, nous abservons donc différents problèmes qui suggèrent que le modèle choisi n’est peut être pas le plus adapté. Nous comprendrons pourquoi plus loin. A vous de jouer Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01b_reg_lin&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. 1.3.2 Comparaison de régressions Vous pouvez à présent comparer ces résultats avec un tableau et les six graphiques d’analyse des résidus sans la valeur supérieur à 0.5m de diamètre. Attention, On ne peut supprimer une valeur sans raison valable. La suppression de pointsd aberrants doit en principe être faite avant de débuter l’analyse. La raison de la suppression de ce point est lié au fait qu’il soit seul et unique point supérieur à 0.5m de diamètre. Nous le faisons ici à titre de comparaison. trees_red &lt;- filter(trees, diameter &lt; 0.5) lm1 &lt;- lm(data = trees_red, volume ~ diameter) chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) + geom_abline( aes(intercept = lm1$coefficients[1], slope = lm1$coefficients[2]), color = &quot;blue&quot;, size = 1.5) Tentez d’analyser le tableau de notre régression. summary(lm1) # # Call: # lm(formula = volume ~ diameter, data = trees_red) # # Residuals: # Min 1Q Median 3Q Max # -0.215129 -0.068502 -0.001149 0.070522 0.181398 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -0.94445 0.09309 -10.15 6.98e-11 *** # diameter 5.31219 0.27540 19.29 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1082 on 28 degrees of freedom # Multiple R-squared: 0.93, Adjusted R-squared: 0.9275 # F-statistic: 372.1 on 1 and 28 DF, p-value: &lt; 2.2e-16 Tentez d’analyser les graphiques d’analyse des résidus ci-dessous. #plot(lm1, which = 1) lm1 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm1, which = 2) lm1 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm1, which = 3) lm1 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm1, which = 4) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) #plot(lm1, which = 5) lm1 %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) -&gt; a #plot(lm1, which = 6) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii]/(1-h[ii]))) -&gt; b Pièges et astuces : extrapolation Notre régression linéaire a été réalisé sur des cerisiers noirs dont le diamètre est compris entre 0.211 et 0.523 mètre. Pensez vous qu’il soit acceptable de prédire des volumes de bois pour des arbres dont le diamètre est inférieur ou supérieur à nos valeurs minimales et maximales mesurées (extrapolation) ? Utilisons notre régression linéaire afin de prédire 10 volumes de bois à partir d’arbre dont le diamètre varie entre 0.1 et 0.8m. new &lt;- data.frame(diameter = seq(0.1, 0.7, length.out = 8)) Ajoutons une variable pred qui contient les prédictions en volume de bois. Observez-vous un problème particulier sur base du tableau ci-dessous ? new %&gt;.% modelr::add_predictions(., lm.) -&gt; new new # diameter pred # 1 0.1000000 -0.482324424 # 2 0.1857143 0.002092891 # 3 0.2714286 0.486510206 # 4 0.3571429 0.970927522 # 5 0.4428571 1.455344837 # 6 0.5285714 1.939762152 # 7 0.6142857 2.424179468 # 8 0.7000000 2.908596783 Il est peut-être plus simple de voir le problème sur un nuage de points. Pour un diamètre de 0.1857143 m de diamètre, le volume de bois est de 0 mis en avant par l’intersection des lignes pointillées bleues. chart(trees, volume~diameter) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) + geom_vline(xintercept = new$diameter[2], linetype = &quot;twodash&quot;, color = &quot;blue&quot;) + geom_hline(yintercept = new$pred[2], linetype = &quot;twodash&quot;, color = &quot;blue&quot;) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]))+ geom_point(data = new, f_aes(pred~diameter), color = &quot;red&quot;) Le volume de bois prédit est négatif ! Notre modèle est-il alors complètement faux ? Rappelons-nous qu’un modèle est nécessairement une vision simplifiée de la réalité. En particulier, notre modèle a été entraîné avec des données comprises dans un intervalle. Il est alors valable pour effectuer des interpolations à l’intérieur de cet intervalle, mais ne peut pas être utilisé pour effectuer des extrapolations en dehors, comme nous venos de réaliser. trees %&gt;.% modelr::add_predictions(., lm.) -&gt; trees chart(trees, volume~diameter) + geom_point() + geom_line(f_aes(pred ~ diameter)) Pièges et astuces : significativité fortuite Gardez toujours à l’esprit qu’il est possible que votre jeu de données donne une régression significative, mais purement fortuite. Les données supplémentaires de test devraient alros démasquer le problème. D’où l’importance de vérifier/valider votre modèle. Le principe de parcimonie veut que l’on ne teste pas toutes les combinaisons possibles deux à deux des variables d’un gros jeu de données, mais que l’on restreigne les explorations à des relations qui ont un sens biologique afin de minimiser le risque d’obtenir une telle régression de manière fortuite. 1.3.3 Enveloppe de confiance De même que l’on peut définir un intervalle de confiance dans lequel la moyenne d’un échantillon se situe avec une probabilité donnée, il est aussi possible de calculer et de tracer une enveloppe de confiance qui indique la région dans laquelle le “vrai” modèle se trouve avec une probabilité donnée (généralement, on choisi cette probabilité à 95%). Voici ce que cela donne : lm. %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x))(.) Cette enveloppe de confiance est en réalité basée sur l’écart type conditionnel (écart type de \\(y\\) sachant quelle est la valeur de \\(x\\)) qui se calcule comme suit : \\[s_{y|x}\\ =\\ \\sqrt{ \\frac{\\sum_{i = 0}^n\\left(y_i - \\hat y_i\\right)^2}{n-2}}\\] A partir de là, il est possible de définir également un intervalle de confiance conditionnel à \\(x\\) : \\[CI_{1-\\alpha}\\ =\\ \\hat y_i\\ \\ \\pm \\ t_{\\frac{\\alpha}{2}}^{n-2} \\frac{s_{y|x}\\ }{\\sqrt{n}}\\] C’est cet intervalle de confiance conditionnel qui est matérialisé par l’enveloppe de confiance autour de la droite de régression représentée sur le graphique. 1.3.4 Extraire les données d’un modèle La fonction tidy() du package broom extrait facilement et rapidement sous la forme d’un tableau différentes valeurs associées à votre régression linéaire. (DF &lt;- broom::tidy(lm.)) # # A tibble: 2 x 5 # term estimate std.error statistic p.value # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 (Intercept) -1.05 0.0955 -11.0 7.85e-12 # 2 diameter 5.65 0.276 20.4 9.09e-19 Pour extraire facilement et rapidement sous la forme d’un tableau de données les paramètres de votre modèle avec la fonction glance(). (DF &lt;- broom::glance(lm.)) # # A tibble: 1 x 11 # r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.935 0.933 0.121 418. 9.09e-19 2 22.6 -39.2 -34.9 # # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Vous avez des snippets à votre disposition pour ces deux fonctions : ... -&gt; models ..m -&gt; .models tools .mt -&gt; .mtmoddf ou encore ... -&gt; models ..m -&gt; .models tools .mt -&gt; .mtpardf A vous de jouer Vous avez à votre disposition la première assignation GitHub Classroom : https://classroom.github.com/a/bvqsukEO "],
["regression-lineaire-multiple.html", "1.4 Régression linéaire multiple", " 1.4 Régression linéaire multiple TODO "],
["regression-lineaire-polynomiale.html", "1.5 Régression linéaire polynomiale", " 1.5 Régression linéaire polynomiale TODO "],
["glm.html", "Module 2 Modèle linéaire généralisé", " Module 2 Modèle linéaire généralisé Objectifs TODO Prérequis TODO "],
["les-variables-facteurs.html", "2.1 Les variables facteurs", " 2.1 Les variables facteurs Le modèle linéaire permet d’analyser une relation linéaire entre deux variables. Jusqu’à présente, nous avons utilisé deux variables quantitative (si vous avez des doutes concernant les types de variables, relisez la section suivante : Type de variables ) . Dans le premier ouvrage, vous avez découvert l’analyse de variance dans le chapitre 10, il est indispensable de relire avec attention cette section au minimum. Avez vous remarqué une ressemblance particulière entre la regression linéaire que nous avons réalisé précédement et l’analyse de variance ? Les plus observateurs auront mis en avant la que la fonction est la même pour réaliser une régression linéaire et une analyse de variance. La fonction lm() est capable de traiter aussi bien des variables réponses qualitatives que quantitatives. 2.1.1 Matrice de contraste "],
["modele-lineaire.html", "2.2 Modèle linéaire", " 2.2 Modèle linéaire Le modèle linaire regroupe l’ensemble des régressions présentées précédement. Nous savons dorénavant la raison d’avoir une seule fonction unique pour l’ensemble des procédures ci dessus , la fonction lm() "],
["comparaison-des-modeles.html", "2.3 Comparaison des modèles", " 2.3 Comparaison des modèles Critère d’Akaike "],
["non-lineaire.html", "Module 3 Modèle non linéaire", " Module 3 Modèle non linéaire Objectifs TODO Prérequis TODO "],
["robuste-survie.html", "Module 4 Régression robuste/quantile &amp; analyse de survie", " Module 4 Régression robuste/quantile &amp; analyse de survie Objectifs TODO Prérequis TODO "],
["hierarchique.html", "Module 5 Classification hiérarchique", " Module 5 Classification hiérarchique Objectifs TODO Prérequis TODO "],
["k-moyenne-som.html", "Module 6 K-moyenne &amp; SOM", " Module 6 K-moyenne &amp; SOM Objectifs TODO Prérequis TODO "],
["k-moyennes.html", "6.1 K-moyennes", " 6.1 K-moyennes "],
["cartes-auto-adaptatives.html", "6.2 Cartes auto adaptatives", " 6.2 Cartes auto adaptatives La méthode des cartes auto-adaptatives se nomme self-organizing map (SOM) en anglais. "],
["acp-afc.html", "Module 7 ACP &amp; AFC", " Module 7 ACP &amp; AFC Objectifs TODO Prérequis TODO "],
["analyse-en-composantes-principales.html", "7.1 Analyse en composantes principales", " 7.1 Analyse en composantes principales "],
["analyse-factorielle-des-correspondances.html", "7.2 Analyse factorielle des correspondances", " 7.2 Analyse factorielle des correspondances "],
["afm.html", "Module 8 AFM", " Module 8 AFM Objectifs TODO Prérequis TODO "],
["analyse-factorielle-multiple-afm.html", "8.1 Analyse factorielle multiple (AFM)", " 8.1 Analyse factorielle multiple (AFM) L’analyse factorielle multiple (AFM) se nomme principal component analysis (PCA) en anglais. "],
["svbox.html", "A Installation de la SciViews Box", " A Installation de la SciViews Box Pour ce cours SDD 2, nous utiliserons la même SciViews Box que pour le cours 1… mais actualisée (version de l’année). Vous allez donc devoir installer la nouvelle version. La procédure n’a changé que sur des points de détails. Référez-vous à l’appendice A1 du cours SDD 1. Vous pouvez conserver l’ancienne SciViews Box en parallèle avec cette nouvelle version, mais vérifiez si vous avez assez d’espace sur le disque dur pour contenir les deux simultanément. Comptez par sécurité 20Go par version. Si vous manquez de place, vous pouvez éliminer l’ancienne version avant d’installer la nouvelle (vos projets ne seront pas effacés). "],
["migration-des-projets.html", "A.1 Migration des projets", " A.1 Migration des projets Concernant les projets réalisés dans une version précédente de la SciViews Box, ceux-ci restent disponibles, même si vous éliminez l’ancienne. Plusieurs cas de figure se présentent : Vous conserver deux ou plusieurs version de la SciViews Box en parallèle. Dans ce cas, nous conseillons fortement de garder chaque projet accessible à partir de la version dans laquelle il a été créé. Seulement les projets que vous décidez de migrer explicitement (voir ci-dessous) seront à déplacer dans le dossier shared de la nouvelle SciViews Box. Vous aurez à faire cette manipulation, par exemple, si vous devez recommencer un cours l’année suivante afin d’être en phase (même version de la svbox) par rapport à vos nouveaux collègues. Vous ne conservez que la dernière version de la SciViews Box, mais ne devez pas accéder fréquemment vos anciens projets, et dans ce cas, vous pouvez réinstaller temporairement l’ancienne version de svbox. Dans ce cas, ne migrez pas vos anciens projets. Éliminez simplement l’ancienne svbox, tout en laisant vos projets intacts dans son répertoire shared. Lors de la réinstallation de l’ancienne svbox, vous retrouverez alors tous vos anciens projets intactes. Vous ne conservez pas d’ancienne version de la svbox et vous ne souhaitez pas devoir la réinstaller. Il est possible de migrer vos anciens projets en les déplaçant de l’ancien répertoire shared vers le nouveau. Soyez toutefois conscients que vos documents R Markdown et scripts R ne fonctionneront pas forcément dans la nouvelle svbox et qu’une adaptation sera peut-être nécessaire ! "],
["configuration-git-et-github.html", "A.2 Configuration Git et Github", " A.2 Configuration Git et Github A chaque nouvelle installation de la SciViews Box, vous devez la reconfigurer via la boite de dialogue SciViews Box Configuration. En particulier, il est très important d’indiquer correctement votre identifiant et email Git (zone encadrée en rouge dans la copie d’écran ci-dessous). Assurez-vous (si ce n’est déjà fait) que vous possédez un compte Github valide. Vous pouvez cliquer sur le bouton Go to Github par facilté dans la même boite de dialogue. Choisissez de manière judicieuse votre login. Vous pourriez être amenés à l’utiliser bien plus longtemps que vous ne le pensez, y compris plus tard dans votre carrière. Donc, lisez les conseils ci-dessous (inspirés et adaptés de Happy Git and Github for the UseR - Register a Github Account : Incluez votre nom réel. Les gens aiment savoir à qui ils ont affaire. Rendez aussi votre nom/login facile à deviner et à retenir. Philippe Grosjean a comme login phgrosjean, par exemple. Vous pouvez réutiliser votre login d’autres contextes, par exemple Twitter ou Slack (ou Facebook). Choisissez un login que vous pourrez échanger de manière confortable avec votre futur boss. Un login plus court est préférable. Soyez unique dans votre login, mais à l’aide d’aussi peu de caractères que possible. Github propose parfois des logins en auto-complétion. Examinez ce qu’il propose. Rendez votre login invariable dans le temps. Par exemple, n’utilisez pas un login lié à votre université (numéro de matricule, ou nom de l’université inclue dans le login). Si tout va bien votre login vous suivra dans votre carrière, … donc, potentiellement loin de l’université où vous avez fait vos études. N’utilisez pas de logins qui sont aussi des mots ayant une signification particulière en programmation, par exemple, n’utilisez pas NA, même si c’est vos initiales ! Une fois votre compte Github créé, et votre login/email pour votre identification Git correctement enregistrés dans la SciViews Box, vous devez pouvoir travailler, faire des “pushs”, des “pulls” et des “commits”4. Cependant, RStudio vous demandera constamment vos logins et mots de passe… à la longue, c’est lassant ! La procédure ci-dessous vous enregistre une fois pour toutes sur votre compte Github dans RStudio. A.2.1 Compte Github dans RStudio RStudio offre la possibilité d’enregistrer une clé publique/privée dans votre SciViews Box afin de vous enregistrer sur Github de manière permanente. L’avantage, c’est que vous ne devrez plus constamment entrer votre login et mot de passe à chaque opération sur Github ! Nous vous le conseillons donc vivement. Entrez dans Rstudio Server, et allez dans le menu Tools -&gt; Global Options.... Ensuite, cliquez dans la rubrique Git/SVN dans la boite de dialogue. Ensuite, cliquez sur le bouton Create RSA key.... La phrase de passe n’est pas nécessaire (il est même préférable de la laisser vide si vous voulez utiliser Github sans rien devoir taper à chaque fois). Cliquez sur le bouton Create. Vous obtenez alors une fenêtre similaire à celle ci-dessous (bien sûr avec des données différentes). Ceci confirme que votre clé cryptographique a été créée localement. Fermez cette fenêtre pour revenir à la boite de dialogue de configuration de RStudio Server. Dans la boite de dialogue de configuration de RStudio Server, section Git/SVN cliquez sur le lien View public key qui apparait une fois la clé créée : La clé apparait dans une fenêtre, déjà présélectionnée. Copiez-là dans le presse-papier (Ctrl-C ou clic bouton droit et sélection de Copy dans le menu contextuel), puis fermez cette fenêtre. Dans votre navigateur web favori, naviguez vers https://github.com, loggez-vous, et accédez aux paramètres de votre compte Github (menu déroulant en haut à droite, entrée Settings) : Dans les paramètres de votre compte, cliquez sur la rubrique SSH and GPG keys, ensuite sur le bouton vert New SSH key Collez-y votre clé à partir du presse-papier dans la zone Key. Vous pouvez lui donner un nom évocateur dans le champ Title. Ensuite, cliquez sur Add SSH key. Déloggez, puis reloggez-vous dans RStudio Server pour que les changements soient pris en compte. La prochaine action sur Github depuis RStudio pourrait encore déclencher la demande de votre login et mot de passe, mais ensuite, les opérations devraient se faire directement. Si vous éprouvez toujours des difficultés à faire collaborer R et RStudio avec Git et Github, voyez https://happygitwithr.com (en anglais) qui explique les différentes procédures bien plus en détails. Vérifiez toujours lors de votre premier commit que Github vous reconnait bien. Pour cela, naviguez vers le dépôt où vous avez commité avec votre explorateur web, et vérifiez l’identité prise en compte lors de votre commit.↩ "],
["references.html", "Références", " Références "]
]
