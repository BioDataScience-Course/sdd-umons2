[
["index.html", "Science des données biologiques 2 Préambule", " Science des données biologiques 2 Philippe Grosjean &amp; Guyliann Engels 2020-02-14 Préambule Cet ouvrage interactif est le second d’une série de trois ouvrages traitant de la science des données biologiques. L’écriture de cette suite de livres a débuté au cours de l’année académique 2018-2019. Pour l’année académique 2019-2020, cet ouvrage interactif sera le support du cours suivant : Science des données II : Analyse et modélisation, UMONS dont le responsable est Grosjean Philippe Cet ouvrage est conçu pour être utilisé de manière interactive en ligne. En effet, nous y ajoutons des vidéos, des démonstrations interactives, et des exercices sous forme de questionnaires interactifs également. Ces différents éléments ne sont, bien évidemment, utilisables qu’en ligne. Le matériel dans cet ouvrage est distribué sous licence CC BY-NC-SA 4.0. "],
["vue-generale-des-cours.html", "Vue générale des cours", " Vue générale des cours Le cours de Science des données II: analyse et modélisation est dispensé aux biologistes de troisième Bachelier en Faculté des Sciences de l’Université de Mons à partir de l’année académique 2019-2020. La matière est divisée en huit modules de 6h chacun en présentiel. Il nécessitera environ un tiers de ce temps (voir plus, en fonction de votre rythme et de votre technique d’apprentissage) en travail à domicile. Cette matière fait suite au premier cours dont le contenu est considéré comme assimilé (voir https://biodatascience-course.sciviews.org/sdd-umons/). La première moitié du cours est consacrée à la modélisation, un domaine particulièrement important de la science des données qui étend les concepts déjà vu au cours 1 d’analyse de variance et de corrélation entre deux variables. Ces quatre modules formeront aussi un socle sur lequel nous pourrons élaborer les techniques d’apprentissage machine (classification supervisée), et puis ensuite l’apprentissage profond à la base de l’intelligence artificielle qui seront abordées plus tard dans le cours 3. Cette partie est dense, mais ultra importante ! La seconde moitié s’intéressera à l’exploration des données, encore appelée analyse des données qui vise à découvrir des caractéristiques intéressantes dans des très gros jeux de données. Ces techniques sont d’autant plus utiles que les données volumineuses deviennent de plus en plus courantes en biologie. "],
["materiel-pedagogique.html", "Matériel pédagogique", " Matériel pédagogique Le matériel pédagogique, rassemblé dans ce syllabus interactif est aussi varié que possible. Vous pourrez ainsi piocher dans l’offre en fonction de vos envies et de votre profil d’apprenant pour optimiser votre travail. Vous trouverez: le présent ouvrage en ligne, des tutoriaux interactifs (réalisés avec un logiciel appelé learnr). Vous pourrez exécuter ces tutoriaux directement sur votre ordinateur, et vous aurez alors accès à des pages Web réactives contenant des explications, des exercices et des quizzs en ligne, des slides de présentations, des dépôts Github Classroom dans la section BioDataScience-Course pour réaliser et documenter vos travaux personnels. des renvois vers des documents externes en ligne, types vidéos youtube ou vimeo, des ouvrages en ligne en anglais ou en français, des blogs, des tutoriaux, des parties gratuites de cours Datacamp ou équivalents, des questions sur des sites comme “Stackoverflow” ou issues des “mailing lists” R, … Tout ce matériel est accessible à partir du site Web du cours, du présent syllabus interactif (et de Moodle pour les étudiants de l’UMONS). Ces derniers ont aussi accès au dossier SDD sur StudentTemp en Intranet à l’UMONS. Les aspects pratiques seront à réaliser en utilisant la ‘SciViews Box’, une machine virtuelle préconfigurée. Nous installerons ensemble la nouvelle version de cette SciViews Box au premier cours. Il est donc très important que vous soyez présent à ce cours, et vous pouvez venir aussi si vous le souhaitez avec votre propre ordinateur portable comme pour le cours 1. Enfin, vous pourrez poser vos questions par mail à l’adresse sdd@sciviews.org. System information sessioninfo::session_info() # ─ Session info ────────────────────────────────────────────────────────── # setting value # version R version 3.5.3 (2019-03-11) # os Ubuntu 18.04.2 LTS # system x86_64, linux-gnu # ui X11 # language (EN) # collate en_US.UTF-8 # ctype en_US.UTF-8 # tz Europe/Brussels # date 2020-02-14 # # ─ Packages ────────────────────────────────────────────────────────────── # package * version date lib source # assertthat 0.2.1 2019-03-21 [2] CRAN (R 3.5.3) # bookdown 0.9 2018-12-21 [2] CRAN (R 3.5.3) # cli 1.1.0 2019-03-19 [2] CRAN (R 3.5.3) # colorspace 1.4-1 2019-03-18 [2] CRAN (R 3.5.3) # crayon 1.3.4 2017-09-16 [2] CRAN (R 3.5.3) # digest 0.6.18 2018-10-10 [2] CRAN (R 3.5.3) # dplyr 0.8.0.1 2019-02-15 [2] CRAN (R 3.5.3) # evaluate 0.13 2019-02-12 [2] CRAN (R 3.5.3) # farver 1.1.0 2018-11-20 [2] CRAN (R 3.5.3) # gganimate 1.0.3 2019-04-02 [2] CRAN (R 3.5.3) # ggplot2 3.1.1 2019-04-07 [2] CRAN (R 3.5.3) # glue 1.3.1 2019-03-12 [2] CRAN (R 3.5.3) # gtable 0.3.0 2019-03-25 [2] CRAN (R 3.5.3) # hms 0.4.2 2018-03-10 [2] CRAN (R 3.5.3) # htmltools 0.3.6 2017-04-28 [2] CRAN (R 3.5.3) # inline 0.3.15 2018-05-18 [2] CRAN (R 3.5.3) # knitr 1.23 2019-12-04 [1] Github (yihui/knitr@b5bf077) # lazyeval 0.2.2 2019-03-15 [2] CRAN (R 3.5.3) # magick 2.0 2018-10-05 [2] CRAN (R 3.5.3) # magrittr 1.5 2014-11-22 [2] CRAN (R 3.5.3) # munsell 0.5.0 2018-06-12 [2] CRAN (R 3.5.3) # pillar 1.3.1 2018-12-15 [2] CRAN (R 3.5.3) # pkgconfig 2.0.2 2018-08-16 [2] CRAN (R 3.5.3) # plyr 1.8.4 2016-06-08 [2] CRAN (R 3.5.3) # prettyunits 1.0.2 2015-07-13 [2] CRAN (R 3.5.3) # progress 1.2.0 2018-06-14 [2] CRAN (R 3.5.3) # purrr 0.3.2 2019-03-15 [2] CRAN (R 3.5.3) # R6 2.4.0 2019-02-14 [2] CRAN (R 3.5.3) # Rcpp 1.0.1 2019-03-17 [2] CRAN (R 3.5.3) # rlang 0.3.4 2019-04-07 [2] CRAN (R 3.5.3) # rmarkdown 1.12 2019-03-14 [2] CRAN (R 3.5.3) # rstudioapi 0.10 2019-03-19 [2] CRAN (R 3.5.3) # scales 1.0.0 2018-08-09 [2] CRAN (R 3.5.3) # sessioninfo 1.1.1 2018-11-05 [2] CRAN (R 3.5.3) # stringi 1.4.3 2019-03-12 [2] CRAN (R 3.5.3) # stringr 1.4.0 2019-02-10 [2] CRAN (R 3.5.3) # tibble 2.1.1 2019-03-16 [2] CRAN (R 3.5.3) # tidyselect 0.2.5 2018-10-11 [2] CRAN (R 3.5.3) # tweenr 1.0.1 2018-12-14 [2] CRAN (R 3.5.3) # withr 2.1.2 2018-03-15 [2] CRAN (R 3.5.3) # xfun 0.6 2019-04-02 [2] CRAN (R 3.5.3) # yaml 2.2.0 2018-07-25 [2] CRAN (R 3.5.3) # # [1] /home/sv/R/x86_64-pc-linux-gnu-library/3.5 # [2] /usr/local/lib/R/site-library # [3] /usr/lib/R/site-library # [4] /usr/lib/R/library "],
["lm.html", "Module 1 Régression linéaire I", " Module 1 Régression linéaire I Objectifs Retrouver ses marques avec R, RStudio et la SciViews Box et découvrir les fonctions supplémentaires de la nouvelle version. Découvrir la régression linaire de manière intuitive. Découvrir les outils de diagnostic de la régression linéaire, en particulier l’analyse des résidus. Prérequis Avant de nous lancer tête baissée dans de la matière nouvelle, nous allons installer la dernière version de la SciViews Box. Une nouvelle version est disponible chaque année début septembre. Reportez-vous à l’appendice A pour son installation et pour la migration éventuelle de vos projets depuis la version précédente. Une fois la box installée, consacrez un petit quart d’heure à repérer les icônes nouvelles dans le Dock et dans le menu Applications. Vous retrouverez R et RStudio, mais dans des version plus récentes qui apportent également leur lot de nouveautés. Lancez RStudio et repérez ici aussi les nouveaux onglets et les nouvelles entrées de menu. Aidez-vous de l’aide en ligne ou de recherches sur le net pour vous familiariser avec ces nouvelles fonctionnalités. Une fois la nouvelle SciViews Box fonctionnelle sur votre ordinateur, vous allez réaliser une séance d’exercice couvrant les points essentiels des notions abordées dans le livre science des données biologiques partie 1, histoire de rafraîchir vos connaissances. Les tutoriaux learnr auxquels vous êtes maintenant habitués seront là pour vous aider à auto-évaluer votre progression. Pour le cours 2, ces tutoriaux sont dans le package BioDataScience2 que vous venez normalement d’installer si vous avez bien suivi toutes les instructions de configuration de votre SciViews Box (sinon, vérifiez votre configuration). Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01a_rappel&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["modele.html", "1.1 Modèle", " 1.1 Modèle Qu’est-ce qu’un “modèle” en science des données et en statistique ? Il s’agit d’une représentation simplifiée sous forme mathématique du mécanisme responsable de la distribution des observations. Rassurez-vous, dans ce module, le côté mathématique du problème sera volontairement peu développé pour laisser une large place à une compréhension intuitive du modèle. Les seules notions clés à connaître ici concernent l’équation qui définit une droite quelconque dans le plan \\(xy\\) : \\[y = a \\ x + b\\] Cette équation comporte : deux variables \\(x\\) et \\(y\\) qui sont matérialisées par les axes des abscisses et des ordonnées dans le plan \\(xy\\). Ces variables prennent des valeurs bien définies pour les observations réalisées sur chaque individu du jeu de données. deux paramètres \\(a\\) et \\(b\\), respectivement la pente de la droite (\\(a\\)) et son ordonnée à l’origine (\\(b\\)). Ecrit de la sorte, \\(a\\) et \\(b\\) peuvent prendre n’importe quelle valeur et l’équation définit de manière généraliste toutes les droites possibles qui existent dans le plan \\(xy\\). Paramétrer ou paramétriser le modèle consiste à définir une et une seule droite en fixant les valeurs de \\(a\\) et de \\(b\\). Par exemple, si je décide de fixer \\(a = 0.35\\) et \\(b = -1.23\\), mon équation définit maintenant une droite bien précise dans le plan \\(xy\\) : \\[y = 0,35 \\ x - 1.23\\] La distinction entre variable et paramètre dans les équations précédentes semble difficile pour certaines personnes. C’est pourtant crucial de pouvoir le faire pour bien comprendre la suite. Alors, c’est le bon moment de relire attentivement ce qui est écrit ci-dessus et de le mémoriser avant d’aller plus avant ! 1.1.1 Pourquoi modéliser ? Le but de la modélisation consiste à découvrir l’équation mathématique de la droite (ou plus généralement, de la fonction) qui décrit au mieux la forme du nuage de points matérialisant les observations dans le plan \\(xy\\) (ou plus généralement dans un hyper-espace représenté par les différentes variables mesurées). Cette équation mathématique peut ensuite être utilisée de différentes façons, toutes plus utiles les unes que les autres : Aide à la compréhension du mécanisme sous-jacent qui a généré les données. Par exemple, si une droite représente bien la croissance pondérale d’un organisme dans le plan représenté par le logarithme du poids (P) en ordonnée et le temps (t) en abscisse, nous pourrons déduire que la croissance de cet organisme est probablement un mécanisme de type exponentiel (puisqu’une transformation inverse, c’est-à-dire logarithmique, linéarise alors le nuage de points). Attention ! Le modèle n’est pos le mécanisme sous-jacent de génération des données, mais utilisé habilement, ce modèle peut donner des indices utiles pour aider à découvrir ce mécanisme. Effectuer des prédictions. Le modèle paramétré pourra être utilisé pour prédire, par exemple, le poids probable d’un individu de la même population après un certain laps de temps. Comparer différents modèles. En présence de plusieurs populations, nous pourrons ajuster un modèle linéaire pour chacune d’elles et comparer ensuite les pentes des droites pour déterminer quelle population a le meilleur ou le moins bon taux de croissance. Explorer les relations entre variables. Sans aucunes connaissances sur le contexte qui a permit d’obtenir nos données, un modèle peut fournir des informations utiles pour orienter les recherches futures. Idéalement, un modèle devrait pouvoir servir à ces différentes applications. En pratique, comme le modèle est forcément une simplification de la réalité, des compromis doivent être concédés pour arriver à cette simplification. En fonction de son usage, les compromis possibles vont différer. Il s’en suit une spécialisation des modèles en modèles mécanistiques qui décrivent particulièrement bien le mécanisme sous-jacent (fréquents en physique, par exemple), les modèles prédictifs conçus pour calculer des nouvelles valeurs (que l’intelligence artificielle affectionne particulièrement), les modèles comparatifs, et enfin, les modèles exploratroires (utilisés dans la phase initiale de découverte et de description des données). Retenez simplement qu’un même modèle est rarement efficace sur les quatre tableaux simultanément. 1.1.2 Quand modéliser ? A chaque fois que deux ou plusieurs variables (quantitatives dans le cas de la régression) forment un nuage de points qui présente une forme particulière non sphérique, autrement dit, qu’une corrélation significative existe dans les données, un modèle peut être utile. Etant donné deux variables quantitatives, trois niveaux d’association de force croissante peuvent être définies entre ces deux variables : La corrélation quantifie juste l’allongement dans une direction préférentielle du nuage de points à l’aide des coefficients de corrélation linéaire de Pearson ou non linéaire de Spearman. Ce niveau d’association a été traité dans le module 12 du cours 1. Il est purement descriptif et n’implique aucunes autres hypothèses sur les données observées. La relation considère que la corrélation observée entre les deux variables est issue d’un mécanisme sous-jacent qui nous intéresse. Un modèle mathématique de l’association entre les deux variables matérialise de manière éventuellement simplifiée, ce mécanisme. Il permet de réaliser ensuite des calculs utiles. Nous verrons plus loin que des contraintes plus fortes doivent être supposées concernant le distribution des deux variables. La causalité précise encore le mécanisme sous-jacent dans le sens qu’elle exprime le fait que c’est la variation de l’une de ces variables qui est directement ou indirectement la cause de la variation de la seconde variable. Bien que des outils statistiques existent pour inférer une causalité (nous ne les aborderons pas dans ce cours), la causalité est plutôt étudiée via l’expérimentation : le biologiste contrôle et fait varier la variable supposée causale, toutes autres conditions par ailleurs invariables dans l’expérience. Il mesure alors et constate si la seconde variable répond ou non à ces variations1 et en déduit une causalité éventuelle. La distinction entre ces trois degrés d’association de deux variables est cruciale. Il est fréquent d’observer une confusion entre corrélation (ou relation) et causalité chez ceux qui ne comprennent pas bien la différence. Cela peut mener à des interprétations complètement erronées ! Comme ceci est à la fois crucial mais subtil, voici une vidéo issue de la série “les statistiques expliquées à mon chat” qui explique clairement le problème. Une troisième variable confondante peut en effet expliquer une corrélation, rendant alors la relation et/ou la causalité entre les deux variables fallacieuse… 1.1.3 Entraînement et confirmation En statistique, une règle universelle veut qu’une observation ne peut servir qu’une seule fois. Ainsi, toutes les données utilisées pour calculer le modèle ne peuvent pas servir simultanément à la confirmer. Il faut échantillonner d’autres valeurs pour effectuer cette confirmation. Il s’en suit une spécialisation des jeux de données en : jeu d’entraînement qui sert à établir le modèle jeu de confirmation ou de test qui sert à vérifier que le modèle est génaralisable car il est capable de prédire le comportement d’un autre jeu de données indépendant issu de la même population statistique. C’est une pratique cruciale de toujours confirmer son modèle, et donc, de prendre soin de séparer ses données en jeu d’entraînement et de test. Les bonnes façons de faire cela seront abordées au cours 3 dans la partie consacrée à l’apprentissage machine. Ici, nous nous focaliserons uniquement sur l’établissement du modèle dans la phase d’entraînement. Par conséquent, nous utiliserons toutes nos données pour cet entraînement, mais qu’il soit d’emblée bien clair qu’une confirmation du modèle est une seconde phase également indispensable. En biologie, le vivant peut être étudié essentiellement de deux manières complémentaires : par l’observation du monde qui nous entoure sans interférer, ou le moins possible, et par l’expérimentation où le biologiste fixe alors très précisément les conditions dans lesquelles il étudie ses organismes cibles. Les deux approches se prêtent à la modélisation mais seule l’expérimentation permet d’inférer avec certitude la causalité.↩ "],
["regression-lineaire-simple.html", "1.2 Régression linéaire simple", " 1.2 Régression linéaire simple Nous allons découvrir les bases de la régression linéaire de façon intuitive. Nous utilisons le jeu de données trees qui rassemble la mesure du diamètre, de la hauteur et du volume de bois de cerisiers noirs. # importation des données trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) Rapellons-nous que dans le chapitre 12 du livre science des données 1, nous avons étudié l’association de deux variables quantitatives (ou numériques). Nous utilisons donc une matrice de corrélation afin de mettre en évidence la corrélation entre nos trois variables qui composent le jeu de donnée trees. La fonction correlation() nous renvoie un tableau de la matrice de correlation avec l’indice de Pearson (corrélation linéaire) par défaut. C’est précisement ce coefficient qui nous intéresse dans le cadre d’une régression linéaire comme description préalable des données autant que pour nous guider dans le choix de nos variables. (trees_corr &lt;- correlation(trees)) # Matrix of Pearson&#39;s product-moment correlation: # (calculation uses everything) # diameter height volume # diameter 1.000 0.519 0.967 # height 0.519 1.000 0.597 # volume 0.967 0.597 1.000 Nous pouvons également observer cette matrice sous la forme d’un graphique plus convivial. plot(trees_corr, type = &quot;lower&quot;) Cependant, n’oubliez pas qu’il est indispensable de visualiser les nuages de points pour ne pas tomber dans le piège mis en avant par le jeu de données artificiel appelé “quartet d’Anscombe” qui montre très bien comment des données très différentes peuvent avoir même moyenne, même variance et même coefficient de corrélation. Un graphique de type matrice de nuages de points est tout indiqué ici. GGally::ggscatmat(as.data.frame(trees), 1:3) Nous observons une plus forte corrélation linéaire entre le volume et le diamètre. Intéressons nous à cette association. chart(trees, volume ~ diameter) + geom_point() Si vous deviez ajouter une droite permettant de représenter au mieux les données, où est ce que vous la placeriez ? Pour rappel, une droite respecte l’équation mathématique suivante : \\[y = a \\ x + b\\] dont a est la pente (slope en anglais) et b est l’ordonnée à l’origine (intercept en anglais). # Sélection de pentes et d&#39;ordonnées à l&#39;origine models &lt;- tibble( model = paste(&quot;model&quot;, 1:4, sep = &quot;-&quot;), slope = c(5, 5.5, 6, 0), intercept = c(-0.5, -0.95, -1.5, 0.85) ) chart(trees, volume ~ diameter) + geom_point() + geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + labs( color = &quot;Modèle&quot;) Nous avons quatre droites candidates pour représenter au mieux les observations. Quel est la meilleure d’entre elles selon vous ? 1.2.1 Quantifier l’ajustement d’un modèle Nous voulons identifier la meilleure régression, c’est-à-dire la régression le plus proche de nos données. Nous avons besoin d’une règle qui va nous permettre de quantifier la distance de nos observations à notre modèle afin d’obtenir la régression avec la plus faible distance possible de l’ensemble de nos observations. Décomposons le problème étape par étape et intéressons nous au model-1 (droite en rouge sur le graphique précédent). Calculer les valeurs de \\(y_i\\) prédites par le modèle que nous noterons par convention \\(\\hat y_i\\) (prononcez “y chapeau” ou “y hat” en anglais) pour chaque observation \\(i\\). # Calculer la valeur de y pour chaque valeur de x suivant le model souhaité # Création de notre fonction model &lt;- function(slope, intercept, x) { prediction &lt;- intercept + slope * x attributes(prediction) &lt;- NULL prediction } # Application de notre fonction yhat &lt;- model(slope = 5, intercept = -0.5, x = trees$diameter) # Affichage des résultats yhat # [1] 0.555 0.590 0.620 0.835 0.860 0.870 0.895 0.895 0.910 0.920 0.935 # [12] 0.950 0.950 0.985 1.025 1.140 1.140 1.190 1.240 1.255 1.280 1.305 # [23] 1.340 1.530 1.570 1.695 1.720 1.775 1.785 1.785 2.115 Calculer la distance entre les observations \\(y_i\\) et les prédictions par notre modèle \\(\\hat y_i\\), soit \\(y_i - \\hat y_i\\) Les distances que nous souhaitons calculer, sont appelées les résidus du modèle et sont notés \\(\\epsilon_i\\) (epsilon). Nous pouvons premièrement visualiser ces résidus graphiquement (ici en rouge par rapport à model-1) : Nous pouvons ensuite facilement calculer leurs valeurs comme ci-dessous : # Calculer la distance entre y et y barre # Création de notre fonction de calcul des résidus distance &lt;- function(observations, predictions) { residus &lt;- observations - predictions attributes(residus) &lt;- NULL residus } # Utilisation de la fonction resid &lt;- distance(observations = trees$volume, predictions = yhat) # Impression des résultats resid # [1] -0.263 -0.298 -0.331 -0.371 -0.328 -0.312 -0.453 -0.380 -0.270 -0.357 # [11] -0.250 -0.355 -0.344 -0.382 -0.484 -0.511 -0.183 -0.414 -0.512 -0.550 # [21] -0.303 -0.407 -0.312 -0.445 -0.364 -0.126 -0.143 -0.124 -0.327 -0.341 # [31] 0.065 Définir une règle pour obtenir une valeur unique qui résume l’ensemble des distances de nos observations par rapport aux prédictions du modèle. Une première idée serait de sommer l’ensemble de nos résidus comme ci-dessous : sum(resid) # [1] -10.175 Appliquons ces calculs sur nos quatre modèles afin de les comparer… Le modèle pour lequel notree critère serait le plus proche de zéro serait alors considéré comme le meilleur. model-1 model-2 model-3 model-4 -10.175 -1.441 10.393 0.135 Selon notre méthode, il en ressort que le modèle 4 est le plus approprié pour représenter au mieux nos données. Qu’en pensez vous ? Intuitivement, nous nous aperçevons que le modèle 4 est loin d’être le meilleur. Nous pouvons en déduire que la somme des résidus n’est pas un bon critère pour ajuster un modèle linéaire. Le problème lorsque nous sommons des résidus, est la présence de résidus positifs et de résidus négatifs (ici par rapport à model-4). Ainsi avec notre première méthode naïve de somme des résidus, il suffit d’avoir autant de résidus positifs que négatifs pour avoir un résultat proche de zéro. Mais cela n’implique pas que les observations soient prochent de la droite pour autant. Avez-vous une autre idée que de sommer les résidus ? Sommer le carré des résidus aurait des propriétés intéressantes car d’une part les carrés de nombres positifs et négatifs sont tous positifs, et d’autre part, plus une observation est éloignée plus sa distance au carré pèse fortement dans la somme2. Nous obtenons les résultats suivants : model-1 model-2 model-3 model-4 3.842211 0.4931095 3.929195 6.498511 Sommer les valeurs absolues des résidus mène également à des contributions toutes positives, mais sans pénaliser outre mesure les observations les plus éloignées. Nous obtenons les résultats suivants : model-1 model-2 model-3 model-4 10.305 3.186 10.393 11.525 Nous avons trouvé deux solutions intéressantes pour quantifier la distance de notre droite par rapport à nos observations. En effet, dans les deux cas, la valeur minimale est obtenue pour le model-2 (en vert sur le graphique) qui est visuellement le meilleur des quatre. La méthode utilisant les carrés des résidus s’appelle une régression par les moindres carrés. Notre objectif est donc de trouver les meilleures valeurs des paramètres \\(a\\) et \\(b\\) de la droite pour minimiser ce critère. Il en résulte une fonction dite objective qui dépend de \\(x\\) et de nos paramètres \\(a\\) et \\(b\\) à minimiser. Cette approche s’appelle la régression par les moindres carrés et elle est la plus utilisée. L’approche utilisant la somme de la valeur absolue des résidus est également utilisable (et elle est d’ailleurs préférable en présence de valeurs extrêmes potentiellement suspectes). Elle s’apppelle régression par la médiane, un cas particulier de la régression quantile, une approche intéressante dans le cas de non normalité des résidus et/ou de présence de valeurs extrêmes suspectes. 1.2.2 Trouver la meilleure droite Essayez de trouver le meilleur modèle par vous-même dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;01a_lin_mod&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/01a_lin_mod&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. Nous pouvons nous demander si notre modèle 2 qui est la meilleure droite de nos quatre modèles est le meilleur modèle possible dans l’absolu. Pour se faire nous allons devoir définir unez technique d’optimisation qui nous permet de déterminer quelle est la droite qui minimise notre fonction objective. Dans la suite, nous garderonsq le critère des moindres carrés (des résidus). Imaginons que nous n’avons pas quatre mais 5000 modèles linéaires avec des pentes et des ordonnées à l’origine différentes. Quelle est la meilleure droite ? set.seed(34643) models1 &lt;- tibble( intercept = runif(5000, -5, 4), slope = runif(5000, -5, 15)) chart(trees, volume ~ diameter) + geom_point() + geom_abline(aes(intercept = intercept, slope = slope), data = models1, alpha = 1/6) Nous voyons sur le graphique qu’un grand nombre de droites différentes sont testées, mais nous ne distinguons pas grand chose de plus. Cependant, sur ces 5000 modèles, nous pouvons maintenant calculer la somme des carrés des résidus et ensuite déterminer quel est le meilleur d’entre eux. # Fonction de calcul de la somme des carrés des résidus measure_distance &lt;- function(slope, intercept, x, y) { ybar &lt;- x * slope + intercept resid &lt;- y - ybar sum(resid^2) } # Test de la fonction #measure_distance(slope = 0, intercept = 0.85, x = trees$diameter, y = trees$volume) # Fonction adaptée pour être employé avec purrr:map() pour distribuer le calcul trees_dist &lt;- function(intercept, slope) { measure_distance(slope = slope, intercept = intercept, x = trees$diameter, y = trees$volume) } models1 &lt;- models1 %&gt;% mutate(dist = purrr::map2_dbl(intercept, slope, trees_dist)) Si nous réalisons un graphique de valeurs de pentes, d’ordonnées à l’origine et de la valeur de la fonction objective (distance) en couleur, nous obtenons le graphique ci-dessous. plot &lt;- chart(models1, slope ~ intercept %col=% dist) + geom_point() + geom_point(data = filter(models1, rank(dist) &lt;= 10), shape = 1, color = &#39;red&#39;, size = 3) + labs( y = &quot;Pente&quot;, x = &quot;Ordonnée à l&#39;origine&quot;, color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) plotly::ggplotly(plot) Les 10 valeurs les plus faibles sont mises en évidence sur le graphique par des cercles rouges. Le modèle optimal que nous recherchons se trouve dans cette région. best_models &lt;- models1 %&gt;.% filter(., rank(dist) &lt;= 10) Nous pouvons afficher les 10 meilleurs modèles sur notre graphique : chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + labs(color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) En résumé, nous avons besoin d’une fonction qui calcule la distance d’un modèle par rapport à nos observations et d’un algorithme pour la minimiser. Il n’est cependant pas nécessaire de chercher pendant des heures la meilleure fonction et le meilleur algorithme. Il existe dans R, un outil spécifiquement conçu pour adapter des modèles linéaires sur des observations, la fonction lm(). Dans le cas particulier de la régression par les moindres carrés, la solution s’obtient très facilement par un simple calcul : \\[a = \\frac{cov_{x, y}}{var_x} \\ \\ \\ \\textrm{et} \\ \\ \\ b = \\bar y - a \\ \\bar x\\] où \\(\\bar x\\) et \\(\\bar y\\) sont les moyennes pour les deux variables, \\(cov\\) est la covariance et \\(var\\) est la variance. Vous avez à votre disposition des snippets dédiés aux modèles linéaires (tapez ..., ensuite choisissez models, ensuite models : linear et choisissez le snippet qui vous convient dans la liste. (lm. &lt;- lm(data = trees, volume ~ diameter)) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Coefficients: # (Intercept) diameter # -1.047 5.652 Nous pouvons reporter ces valeurs sur notre graphique afin d’observer ce résultat par rapport à nos modèles aléatoires. nous avons en rouge la droite calculée par la fonction lm(). chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) 1.2.3 La fonction lm() Suite à notre découverte de manière intuitive de la régression linéaire simple, nous pouvons récapituler quelques points clés : Une droite suit l’équation mathématique suivante : \\[y = a \\ x + b\\] dont \\(a\\) est la pente (slope en anglais) et \\(b\\) est l’ordonnée à l’origine (intercept en anglais), tous deux les paramètres du modèle, alors que \\(x\\) et \\(y\\) en sont les variables. La distance entre une valeur observée \\(y_i\\) et une valeur prédite \\(\\hat y_i\\) se nomme le résidu (\\(\\epsilon_i\\)) et se mesure toujours parallèlement à l’axe \\(y\\). Cela revient à considérer que toute l’erreur du modèle se situe sur \\(y\\) et non sur \\(x\\). Cela donne l’équation complète de notre modèle statistique : \\[y_i = a \\ x_i + b + \\epsilon_i\\] avec \\(y_i\\) est la valeur mesurée pour le point i sur l’axe y, \\(a\\) est la pente, \\(x_i\\) est la valeur mesurée pour le point i sur l’axe x, b est l’ordonnée à l’origine et \\(\\epsilon_i\\) les résidus. On peut montrer (nous ne le ferons pas ici pour limiter les développements mathématiques) que le choix des moindres carrés des résidus comme fonction objective revient à considérer que nos résidus suivent une distribution normale centrée autour de zéro et avec un écart type \\(\\sigma\\) constant/ : \\[\\epsilon_i \\approx N(0, \\sigma)\\] Dans le cas de la régression linéaire simple, la meilleure droite s’obtient très facilement par la minimisation de la somme des carrés des résidus. En effet, la pente \\(a = \\frac{cov_{x, y}}{var_x}\\) et l’ordonnée à l’origine \\(b = \\bar y - a \\ \\bar x\\). La fonction lm() permet de faire ce calcul très facilement dans R. # Régression linéaire lm. &lt;- lm(data = trees, volume ~ diameter) # Graphique de nos observations et de la droite obtenue avec la fonction lm() chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) La fonction lm() crée un objet spécifique qui contient de nombreuses informations pour pouvoir ensuite analyser notre modèle linéaire. La fonction class() permet de mettre en avant la classe de notre objet. class(lm.) # [1] &quot;lm&quot; 1.2.4 Résumé avec summary() Avec la fonction summary() nous obtenons un résumé condensé des informations les plus utiles pour interpréter notre régression linéaire. summary(lm.) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.231211 -0.087021 0.003533 0.100594 0.271725 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.04748 0.09553 -10.96 7.85e-12 *** # diameter 5.65154 0.27649 20.44 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1206 on 29 degrees of freedom # Multiple R-squared: 0.9351, Adjusted R-squared: 0.9329 # F-statistic: 417.8 on 1 and 29 DF, p-value: &lt; 2.2e-16 Call : Il s’agit de la formule employée dans la fonction lm(). C’est à dire le volume en fonction du diamètre du jeu de données trees. Residuals : La tableau nous fournit un résumé via les 5 nombres de l’ensemble des résidus (que vous pouvez récupérer à partir de lm.$residuals). fivenum(lm.$residuals) # 20 7 12 9 31 # -0.231210947 -0.087020695 0.003532709 0.100594122 0.271724973 Coefficients : Il s’agit des résultats associés à la pente et à l’ordonnée à l’origine dont les valeurs estimées des paramètres (Estimate). Les mêmes valeurs peuvent être obtenues à partir de lm.$coefficients : lm.$coefficients # (Intercept) diameter # -1.047478 5.651535 On retrouve également les écart-types calculés sur ces valeurs (Std.Error) qui donnent une indication de la précision de leur estimation, les valeurs des distibutions de Student sur les valeurs estimées (t value) et enfin les valeurs p (PR(&gt;|t|)) liées à un test de Student pour déterminer si le paramètre p correspondant est significativement différent de zéro (avec \\(H_0: p = 0\\) et \\(H_a: p \\neq 0\\)). Pour l’instant, nous nous contenterons d’interpréter et d’utiliser les informations issues de summary() dans sa partie supérieure. Le contenu des trois dernières lignes sera détaillé dans le module suivant. A partir des données de ce résumé, nous pouvons maintenant paramétrer l’équation de notre modèle : \\[y = ax + b\\] devient3 : \\[volume \\ de \\ bois = 5.65 \\ diamètre \\ à \\ 1.4 \\ m - 1.05 \\] Utiliser le carré des résidus a aussi d’autres propriétés statistiques intéressantes qui rapprochent ce calcul de la variance (qui vaut la somme de la distance au carré à la moyenne pour une seule variables numérique).↩ Lors de la paramétrisation du modèle, pensez à arrondir la valeur des paramètres à un nombre de chiffres significatifs raisonnables. Inutile de garder 5, ou même 3 chiffres derrière la virgule si vous n’avez que quelques dizaines d’obserrvations pour ajuster votre modèle.↩ "],
["outils-de-diagnostic.html", "1.3 Outils de diagnostic", " 1.3 Outils de diagnostic Une fois la meilleure droite de régression obtenue, le travail est loin d’être terminé. Il se peut que le nuage de point ne soit pas tout-à-fait linéaire, que sa dispersion ne soit pas homogène, que les résidus n’aient pas une distribution normale, qu’il existe des valeurs extrêmes aberrantes, ou qui tirent la droite vers elle de manière excessive. Nous allons maintenant devoir diagnostiquer ces possibles problèmes. L’analyse des résidus permet de le faire. Ensuite, si deux ou plusieurs modèles sont utilisable, il nous faut décider lequel conserver. Enfin, nous pouvons aussi calculer et visualiser l’enveloppe de confiance du modèle et extraire une série de données de ce modèle. 1.3.1 Analyse des résidus Le tableau numérique obtenu à l’aide de summary() peut faire penser que l’étude d’une régression linéaire se limite à quelques valeurs numériques et divers tests d’hypothèses associés. C’est un premier pas, mais c’est oublier que la technique est essentiellement visuelle. Le graphique du nuage de points avec la droite superposée est un premier outil diagnostic visuel indispensable, mais il n’est pas le seul ! Plusieurs graphiques spécifiques existent pour mettre en évidence diverses propriétés des résidus qui peuvent révéler des problèmes. Leur inspection est indispensable et s’appelle l’analyse des résidus. Les différents graphiques sont faciles à obtenir à partir des snippets. Le premier de ces graphique permet de vérifier la distribution homogène des résidus. Dans une bonne régression, nous aurons une distribution équilibrée de part et d’autre du zéro sur l’axe Y. Que pensez-vous de notre graphique d’anayse des résidus ? Nous avons une valeur plus éloignée du zéro qui est mise en avant par la courbe en bleu qui montre l’influence générale des résidus. #plot(lm., which = 1) lm. %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) Le second graphique permet de vérifier la normalité des résidus (comparaison par graphique quantile-quantile à une distribution normale). #plot(lm., which = 2) lm. %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) Le troisième graphique va standardiser les résidus, et surtout, en prendre la racine carrée. Cela a pour effet de superposer les résidus négatifs sur les résidus positifs. Nous y diagnostiquons beaucoup plus facilement des problèmes de distribution de ces résidus. A nouveau, nous pouvons observer qu’une valeur influence fortement la régression. #plot(lm., which = 3) lm. %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) Le quatrième graphique met en évidence l’influence des individus sur la régression linéaire. Effectivement, la régression linéaire est sensible aux valeurs extrêmes. Nous pouvons observer que nous avons une valeur qui influence fortement notre régression. On utilise pour ce faire la distance de Cook que nous ne détaillerons pas dans le cadre de ce cours. #plot(lm., which = 4) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Le cinquième graphique utilise l’effet de levier (Leverage) qui met également en avant l’influence des individus sur notre régression. Il répond à la question suivante : “est-ce qu’un ou plusieurs points sont tellement influents qu’ils tirent la régression vers eux de manière abusive ?” Nous avons à nouveau une valeur qui influence fortement notre modèle. #plot(lm., which = 5) lm. %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) Le sixième graphique met en relation la distance de Cooks et l’effet de levier. Notre unique point d’une valeur supérieur à 0.5 m de diamètre influence fortement notre modèle. #plot(lm., which = 6) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii] / (1 - h[ii]))) A l’issue de l’analyse des résidus, nous abservons donc différents problèmes qui suggèrent que le modèle choisi n’est peut être pas le plus adapté. Nous comprendrons pourquoi plus loin. A vous de jouer Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01b_reg_lin&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Pièges et astuces : extrapolation Notre régression linéaire a été réalisée sur des cerisiers noirs dont le diamètre est compris entre 0.211 et 0.523 mètre. Pensez vous qu’il soit acceptable de prédire des volumes de bois pour des arbres dont le diamètre est inférieur ou supérieur à nos valeurs minimales et maximales mesurées (extrapolation) ? Utilisons notre régression linéaire afin de prédire 10 volumes de bois à partir d’arbre dont le diamètre varie entre 0.1 et 0.8m. new &lt;- data.frame(diameter = seq(0.1, 0.7, length.out = 8)) Ajoutons une variable pred qui contient les prédictions en volume de bois. Observez-vous un problème particulier sur base du tableau ci-dessous ? new %&gt;.% modelr::add_predictions(., lm.) -&gt; new new # diameter pred # 1 0.1000000 -0.482324424 # 2 0.1857143 0.002092891 # 3 0.2714286 0.486510206 # 4 0.3571429 0.970927522 # 5 0.4428571 1.455344837 # 6 0.5285714 1.939762152 # 7 0.6142857 2.424179468 # 8 0.7000000 2.908596783 Il est peut-être plus simple de voir le problème sur un nuage de points. Pour un diamètre de 0.1857143 m de diamètre, le volume de bois est de 0 mis en avant par l’intersection des lignes pointillées bleues. chart(trees, volume~diameter) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) + geom_vline(xintercept = new$diameter[2], linetype = &quot;twodash&quot;, color = &quot;blue&quot;) + geom_hline(yintercept = new$pred[2], linetype = &quot;twodash&quot;, color = &quot;blue&quot;) + geom_point() + geom_abline(aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2])) + geom_point(data = new, f_aes(pred~diameter), color = &quot;red&quot;) Le volume de bois prédit est négatif ! Notre modèle est-il alors complètement faux ? Rappelons-nous qu’un modèle est nécessairement une vision simplifiée de la réalité. En particulier, notre modèle a été entraîné avec des données comprises dans un intervalle. Il est alors valable pour effectuer des interpolations à l’intérieur de cet intervalle, mais ne peut pas être utilisé pour effectuer des extrapolations en dehors, comme nous venons de le faire. trees %&gt;.% modelr::add_predictions(., lm.) -&gt; trees chart(trees, volume~diameter) + geom_point() + geom_line(f_aes(pred ~ diameter)) Pièges et astuces : significativité fortuite Gardez toujours à l’esprit qu’il est possible que votre jeu de données donne une régression significative, mais purement fortuite. Les données supplémentaires de test devraient alors démasquer le problème. D’où l’importance de vérifier/valider votre modèle. Le principe de parcimonie veut que l’on ne teste pas toutes les combinaisons possibles deux à deux des variables d’un gros jeu de données, mais que l’on restreigne les explorations à des relations qui ont un sens biologique afin de minimiser le risque d’obtenir une telle régression de manière fortuite. 1.3.2 Enveloppe de confiance De même que l’on peut définir un intervalle de confiance dans lequel la moyenne d’un échantillon se situe avec une probabilité donnée, il est aussi possible de calculer et de tracer une enveloppe de confiance qui indique la région dans laquelle le “vrai” modèle se trouve avec une probabilité donnée (généralement, on choisi cette probabilité à 95%). Voici ce que cela donne : lm. %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x))(.) Cette enveloppe de confiance est en réalité basée sur l’écart type conditionnel (écart type de \\(y\\) sachant quelle est la valeur de \\(x\\)) qui se calcule comme suit : \\[s_{y|x}\\ =\\ \\sqrt{ \\frac{\\sum_{i = 0}^n\\left(y_i - \\hat y_i\\right)^2}{n-2}}\\] A partir de là, il est possible de définir également un intervalle de confiance conditionnel à \\(x\\) : \\[CI_{1-\\alpha}\\ =\\ \\hat y_i\\ \\ \\pm \\ t_{\\frac{\\alpha}{2}}^{n-2} \\frac{s_{y|x}\\ }{\\sqrt{n}}\\] C’est cet intervalle de confiance conditionnel qui est matérialisé par l’enveloppe de confiance autour de la droite de régression représentée sur le graphique. 1.3.3 Extraire les données d’un modèle La fonction tidy() du package broom extrait facilement et rapidement sous la forme d’un tableau différentes valeurs associées à votre régression linéaire. (DF &lt;- broom::tidy(lm.)) # # A tibble: 2 x 5 # term estimate std.error statistic p.value # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 (Intercept) -1.05 0.0955 -11.0 7.85e-12 # 2 diameter 5.65 0.276 20.4 9.09e-19 Pour extraire facilement et rapidement sous la forme d’un tableau de données les paramètres de votre modèle vouys pouvez aussi utiliser la fonction glance(). (DF &lt;- broom::glance(lm.)) # # A tibble: 1 x 11 # r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.935 0.933 0.121 418. 9.09e-19 2 22.6 -39.2 -34.9 # # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Vous avez des snippets à votre disposition pour ces deux fonctions : ... -&gt; models ..m -&gt; .models tools .mt -&gt; .mtmoddf ou encore ... -&gt; models ..m -&gt; .models tools .mt -&gt; .mtpardf A vous de jouer Vous avez à votre disposition la première assignation GitHub Classroom : https://classroom.github.com/a/bvqsukEO "],
["lm2.html", "Module 2 Régression linéaire II", " Module 2 Régression linéaire II Objectifs Savoir utiliser les outils de diagnostic de la régression linéaire correctement, en particulier l’analyse des résidus. Appréhender les différentes formes de régressions linéaires par les moindres carrés. Choisir sa régression linéaire de manière judicieuse. Prérequis Le module précédent est une entrée en matière indispensable qui est complétée par le contenu du présent module. "],
["outils-de-diagnostic-suite.html", "2.1 Outils de diagnostic (suite)", " 2.1 Outils de diagnostic (suite) La régression linéaire est une matière complexe et de nombreux outils existent pour vous aider à déterminer si le modèle que vous ajustez tient la route ou non. Il est très important de le vérifier avant d’utiliser un modèle. Ajuster un modèle quelconque dans des données est à la portée de tout le monde, mais choisir un modèle pertinent et pouvoir expliquer pourquoi est nettement plus difficile ! 2.1.1 Résumé avec summary()(suite) Reprenons la sortie renvoyée par summary() appliqué à un objet lm. trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) lm. &lt;- lm(data = trees, volume ~ diameter) summary(lm.) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.231211 -0.087021 0.003533 0.100594 0.271725 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.04748 0.09553 -10.96 7.85e-12 *** # diameter 5.65154 0.27649 20.44 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1206 on 29 degrees of freedom # Multiple R-squared: 0.9351, Adjusted R-squared: 0.9329 # F-statistic: 417.8 on 1 and 29 DF, p-value: &lt; 2.2e-16 Nous n’avons pas encore étudié la signification des trois dernières lignes de ce résumé. Voici de quoi il s’agit. Residual standard error : Il s’agit de l’écart-type résiduel, considérant que les degrés de liberté du modèle est le nombre d’observations \\(n\\) (ici 31) soustrait du nombre de paramètres à estimer (ici 2, la pente et l’ordonnée à l’origine de la droite). C’est donc une mesure globale de l’importance (c’est-à-dire de l’étendue) des résidus de manière générale. \\[\\sqrt{\\frac{\\sum(y_i - ŷ_i)^2}{n-2}}\\] Multiple R-squared : Il s’agit de la valeur du coefficient de détermination du modèle noté R^2 de manière générale ou r2 dans le cas d’une régression linéaire simple. Il exprime la fraction de variance exprimée par le modèle. Autrement dit, le R2 quantifie la capacité du modèle à prédire la valeur de \\(y\\) connaissant la valeur \\(x\\) pour le même individu. C’est dons une indication du pouvoir prédictif de notre modèle autant que de sa qualité d’ajustement (goodness-of-fit en anglais). Souvenons-nous que la variance totale respecte la propiété d’additivité. La variance est composée au numérateur d’une somme de carrés, et au dénominateur de degrés de liberté. La somme des carrés totaux (de la variance) peut elle-même être décomposée en une fraction expliquée par notre modèle, et la fraction qui ne l’est pas (les résidus) : \\[SC(total) = SC(rég) + SC(résidus)\\] avec : \\[SC(total) = \\sum_{i=0}^n(y_i - \\bar y_i)^2\\] \\[SC(rég) = \\sum_{i=0}^n(ŷ_i - \\bar y_i)^2\\] \\[SC(résidus) = \\sum_{i=0}^n(y_i - ŷ_i)^2\\] A partir de la décomposition de ces sommes de carrés, le coefficient R2 (ou r2) se définit comme : \\[R^2 = \\frac{SC(rég)}{SC(total)} = 1 - \\frac{SC(résidus)}{SC(total)}\\] La valeur du R2 est comprise entre 0 (lorsque le modèle est très mauvais et n’explique rien) et 1 (lorsque le modèle est parfait et “capture” toute la variance des données ; dans ce cas, tous les résidus valent zéro). Donc, plus le coefficient R2 se rapproche de un, plus le modèle explique bien les données et aura un bon pouvoir de prédiction. Dans R, le R2 multiple se réfère simplement au R2 (ou au r2 pour les régressions linéaires simples) calculé de cette façon. L’adjectif multiple indique simplement que le calcul est valable pour une régression multiple telle que nous verrons plus loin. Par contre, le terme au dénominateur considère en fait la somme des carrés totale par rapport à un modèle de référence lorsque la variable dépendante \\(y\\) ne dépend pas de la ou des variables indépendantes \\(x_i\\). Les équations indiquées plus haut sont valables lorsque l’ordonnée à l’origine n’est pas figée (\\(y = a \\ x + b\\)). Dans ce cas, la valeur de référence pour \\(y\\) est bien sa moyenne, \\(\\bar y\\). D’un autre côté, si l’ordonnée à l’origine est fixée à zéro dans le modèle simplifié \\(y = a \\ x\\) (avec \\(b = 0\\) obtenu en indiquant la formule y ~ x + 0 ou y ~ x - 1), alors le zéro sur l’axe \\(y\\) est considéré comme une valeur appartenant d’office au modèle et devient valeur de référence. Ainsi, dans les équations ci-dessus il faut remplacer \\(\\bar y\\) par 0 partout. Le R2 est alors calculé différemment, et sa valeur peut brusquement augmenter si le nuage de points est très éloigné du zéro sur l’axe y. Ne comparez donc jamais les R2 obtenus avec et sans forçage à zéro de l’ordonnée à l’origine ! Adjusted R-squared : La valeur du coefficient R2 ajustée, noté \\(\\bar{R^2}\\) n’est pas utile dans le cadre de la régression linéaire simple, mais est indispensable avec la régression multiple. En effet, à chaque fois que vous rendez votre modèle plus complexe en ajoutant une ou plusieurs variables indépendantes, le modèle s’ajustera de mieux en mieux dans les données, même par pur hasard. C’est un phénomène que l’on appelle l’inflation du R2. A la limite, si nous ajoutons une nouvelle variable fortement corrélée avec les précédentes4, l’apport en terme d’information nouvelle sera négligeable, mais le R2 augmentera malgré tout un tout petit peu. Alors dans quel cas l’ajout d’une nouvelle variable est-il pertinent ou non ? Le R2 ajusté apporte l’information désirée ici. Sa valeur n’augmentera pour l’ajout d’un nouveau prédicteur que si l’ajustement est meilleur que ce que l’on obtiendrait par le pur hasard. Le R2 ajusté se calcule comme suit (il n’est pas nécessaire de retenir cette formule, mais juste de constater que l’ajustement fait intervenir p, le nombre de paramètres du modèle et n, la taille de l’échantillon) : \\[ \\bar{R^2} = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1} \\] F-statistic : Tout comme pour l’ANOVA, le test de la significativité de la régression car \\(MS(rég)/MS(résidus)\\) suit une distribution F à respectivement 1 et \\(n-2\\) degré de liberté, avec \\(MS\\) les carrés moyens, c’est-à-dire les sommes des carrés \\(SC\\) divisés par leurs degrés de liberté respectifs. p-value : Il s’agit de la valeur p associé à la statistique de F, donc à l’ANOVA associée à la régression linéaire. Pour cette ANOVA particulière, l’hypothèse nulle est que la droite n’apporte pas plus d’explication des valeurs de y à partir des valeurs de x que la valeur moyenne de y (ou zéro, dans le cas paerticulier d’un modèle dont l’ordonnée à l’origine est forcé à zéro). L’hypothèse alternative est donc que le modèle est significatif au seuil \\(\\alpha\\) considéré. Donc, notre objectif est de rejetter H0 pour cet test ANOVA pour que le modèle ait un sens (valeur p plus petite quez le seuil \\(\\alpha\\) choisi). Le tableau complet de l’ANOVA associée au modèle peut aussi être obtenu à l’aide de la fonction anova() : anova(lm.) # Analysis of Variance Table # # Response: volume # Df Sum Sq Mean Sq F value Pr(&gt;F) # diameter 1 6.0762 6.0762 417.8 &lt; 2.2e-16 *** # Residuals 29 0.4218 0.0145 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On y retrouve les mêmes informations, fortement résumées en une ligne à la fin de la sortie de summary(), mais ici sous une forme plus classique de tableau de l’analyse de la variance. 2.1.2 Comparaison de régressions Vous pouvez à présent comparer ces résultats avec un tableau et les six graphiques d’analyse des résidus sans la valeur supérieure à 0.5m de diamètre. Attention, On ne peut supprimer une valeur sans raison valable. La suppression de points aberrants doit en principe être faite avant de débuter l’analyse. La raison de la suppression de ce point est liée au fait qu’il soit seul et unique point supérieur à 0.5m de diamètre. Nous le faisons ici à titre de comparaison. trees_red &lt;- filter(trees, diameter &lt; 0.5) lm1 &lt;- lm(data = trees_red, volume ~ diameter) chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) + geom_abline( aes(intercept = lm1$coefficients[1], slope = lm1$coefficients[2]), color = &quot;blue&quot;, size = 1.5) La droite en bleu correspond à la régression sans utiliser l’arbre de diamètre supérieur à 0,5m. Tentez d’analyser le tableau de notre régression en bleu (astuce : comparez avec ce que la régeression précédente donnait). summary(lm1) # # Call: # lm(formula = volume ~ diameter, data = trees_red) # # Residuals: # Min 1Q Median 3Q Max # -0.215129 -0.068502 -0.001149 0.070522 0.181398 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -0.94445 0.09309 -10.15 6.98e-11 *** # diameter 5.31219 0.27540 19.29 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1082 on 28 degrees of freedom # Multiple R-squared: 0.93, Adjusted R-squared: 0.9275 # F-statistic: 372.1 on 1 and 28 DF, p-value: &lt; 2.2e-16 Tentez d’analyser également les graphiques d’analyse des résidus ci-dessous. #plot(lm1, which = 1) lm1 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm1, which = 2) lm1 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm1, which = 3) lm1 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm1, which = 4) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) #plot(lm1, which = 5) lm1 %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) #plot(lm1, which = 6) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii] / (1 - h[ii]))) Au travers de cet exemple, nous constatons que la comparaison de modèles, dans le but de choisir le meilleur est un travail utile. Cela apparaitra d’autant plus utile que la situation va passablement se complexifier (dans le bon sens) avec l’introduction de la régression multiple et polynomiale ci-dessous. Heureusement, nous terminerons ce module avec la découverte d’une métrique qui va nous permettre d’effectuer le choix du meilleur modèle de manière fiable : le critère d’Akaike. A vous de jouer ! Réalisez une nouvelle assignation individuelle : Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/jkh3ruyX La corrélation entre les prédicteurs dans un modèle linéaire multiple est un gros problème et doit être évité le plus possible. Cela s’appelle la colinéarité ou encore multicollinéairité. Ainsi, il est toujours préférable de choisir un ensemble de variables indépendantes peu corrélées entre elles dans un même modèle, mais ce n’est pas toujours possible.↩ "],
["regression-lineaire-multiple.html", "2.2 Régression linéaire multiple", " 2.2 Régression linéaire multiple Dans le cas de la régression linéaire simple, nous considèrions le modèle stqatistique suivant (avec \\(\\epsilon\\) représentant les résidus, terme statistique dans l’équation) : \\[y = a \\ x + b + \\epsilon \\] Dans le cas de la régression, nous introduirons plusieurs variables indépendantes notés \\(x_1\\), \\(x_2\\), …, \\(x_n\\) : \\[y = a_1 \\ x_1 + a_2 \\ x_2 + ... + a_n \\ x_n + b + \\epsilon \\] La bonne nouvelle, c’est que tous les calculs, les métriques et les tests d’hypothèses relatifs à la régression linéaire simple se généraliser simplement et naturellement, tout comme nous sommes passés dans le cours SDD 1 de l’ANOVA à 1 facteur à un modèle plus complexe à 2 ou plusoieurs facteurs. Voyons tout de suite ce que cela donne si nous voulions utiliser à la fois le diamètree et la hauteur des cerisiers noirs pour prédire leur volume de bois : summary(lm2 &lt;- lm(data = trees, volume ~ diameter + height)) # # Call: # lm(formula = volume ~ diameter + height, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.180423 -0.074919 -0.006874 0.062244 0.241801 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.63563 0.24462 -6.686 2.95e-07 *** # diameter 5.25643 0.29594 17.762 &lt; 2e-16 *** # height 0.03112 0.01209 2.574 0.0156 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1104 on 28 degrees of freedom # Multiple R-squared: 0.9475, Adjusted R-squared: 0.9438 # F-statistic: 252.7 on 2 and 28 DF, p-value: &lt; 2.2e-16 D’un point de vue pratique, nous voyons que la formule qui spécifie le modèle peut très bien comporter plusieurs variables séparées par des +. Nous avons ici trois paramètres dans notre modèle : l’ordonnée à l’origine qui vaut -1,63, la pente relative au diamètre de 5,25, et la pente relative à la hauteur de 0,031. Le modèle lm2 sera donc paramétré comme suit : volume de bois = 5,25 . diamètre + 0,031 . hauteur - 1,63. Notons que la pente relative à la hauteur (0,031) n’est pas significativement différente de zéro au seuil \\(\\alpha\\) de 5% (mais l’est seulement pour \\(\\alpha\\) = 1%). En effet, la valeur t du test de Student associé (H0 : le paramètre vaut zéro, H1 : le paramètre est différent de zéro) vaut 2,574. Cela correspond à une valeur p du test de 0,0156, une valeur moyennement significative donc, matérialisée par une seule astérisque à la droite du tableau. Cela dénote un plus faible pouvoir de prédiction du volume de bois via la hauteur que via le diamètre de l’arbre. Nous l’avions déjà observé sur le graphique matrice de nuages de points réalisé initialement, ainsi que via les coefficients de correlation respectifs. La représentation de cette régression nécessite un graphique à trois dimensions (diamètre, hauteur et volume) et le modèle représente en fait le meilleur plan dans cet espace à 3 dimensions. Pour un modèle comportant plus de deux variables indépendantes, il n’est plus possible de représenter graphiquement la régression. library(rgl) knitr::knit_hooks$set(webgl = hook_webgl) car::scatter3d(data = trees, volume ~ diameter + height, fit = &quot;linear&quot;, residuals = TRUE, bg = &quot;white&quot;, axis.scales = TRUE, grid = TRUE, ellipsoid = FALSE) # Loading required namespace: mgcv rgl::rglwidget(width = 800, height = 800) Utilisez la souris pour zoomer (molette) et pour retourner le graphique (cliquez et déplacer la souris en maintenant le bouton enfoncé) pour comprendre ce graphique 3D. La régression est matérialisée par un plan en bleu. Les observations sont les boules jaunes et les résidus sont des traits cyans lorsqu’ils sont positifs et magenta lorsqu’ils sont négatifs. Les graphes d’analyse des résidus sont toujours disponibles (nous ne représentons ici que les quatre premiers) : #plot(lm2, which = 1) lm2 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm2, which = 2) lm2 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm2, which = 3) lm2 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm2, which = 4) lm2 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Est-ce que ce modèle est préférable à celui n’utilisant que le diamètre ? Le R2 ajusté est passé de 0,933 avec le modèle simple lm. utilisant uniquement le diamètre à 0,944 dans le présent modèle lm2 utilisant le diamètre et la hauteur. Cela semble une amélioration, mais le test de significativité de la pente pour la hauteur ne nous indique pas un résultat très significatif. De plus, cela a un coût en pratique de devoir mesurer deux variables au lieu d’une seule pour estimer le volume de bois. Cela en vaut-il la peine ? Nous sommes encore une fois confrontés à la question de comparer deux modèles, cette fois-ci ayant une complexité croissante. Dans le cas particulier de modèles imbriqués (un modèle contient l’autre, mais rajoute un ou plusieurs termes), une ANOVA est possible en décomposant la variance selon les composantes reprises respectivement par chacun des deux modèles. La fonction anova() est programmée pour faire ce calcul en lui indiquant chacun des deux objets contenant les modèles à comparer : anova(lm., lm2) # Analysis of Variance Table # # Model 1: volume ~ diameter # Model 2: volume ~ diameter + height # Res.Df RSS Df Sum of Sq F Pr(&gt;F) # 1 29 0.42176 # 2 28 0.34104 1 0.080721 6.6274 0.01562 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notez que dans le cas de l’ajout d’un seul terme, la valeur p de cette ANOVA est identique à la valeur p de test de significativité du paramètre (ici, cette valeur p est de 0,0156 dans les deux cas). Donc, le choix peut se faire directement à partir de summary() pour ce terme unique. La conclusion est similaire : l’ANOVA donne un résultat seulement moyennent significatif entre les 2 modèles. Dans un cas plus complexe, la fonction anova() de comparaison pourra être utile. Enfin, tous les modèles ne sont pas nécessairement imbriqués. Dans ce cas, il nous faudra un autre moyen de les départager, … mais avant d’aborder cela, étudions une variante intéressante de la régression multiple : la régression polynomiale. A vous de jouer ! Réalisez une nouvelle assignation individuelle : Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;02a_reg_multi&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["regression-lineaire-polynomiale.html", "2.3 Régression linéaire polynomiale", " 2.3 Régression linéaire polynomiale Pour rappel, un polynome est une expression mathématique du type (notez la ressemblance avec l’équation de la régression multiple) : \\[ a_0 + a_1 . x + a_2 . x^2 + ... + a_n . x^n \\] Un polynome d’ordre 2 (terme jusqu’au \\(x^2\\)) correspond à une parabole dans le plan xy. Que se passe-t-il si nous calculons une variable diametre2 qui est le carré du diamètre et que nous prétendons faire une régression multiple en utilisant à la fois diamètre et diamètre2/ ? trees %&gt;.% mutate(., diameter2 = diameter^2) -&gt; trees summary(lm(data = trees, volume ~ diameter + diameter2)) # # Call: # lm(formula = volume ~ diameter + diameter2, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # diameter2 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 Il semble que R ait pu réaliser cette analyse. Cette fois-ci, nous n’avons cependant pas une droite ou un plan ajusté, mais par ce subterfuge, nous avons pu ajuster une courbe dans les données ! Nous pourrions augmenter le degré du polynome (ajouter un terme en diameter^3, voire encore des puissance supérieures). Dans ce cas, nous obtiendrons une courbe de plus en plus flexible, toujours dans le plan xy. Ceci illustre parfaitement d’ailleurs l’ambiguité de la complexité du modèle qui s’ajuste de mieux en mieux dans les données, mais qui ce faisant, perd également progressivement son pouvoir explicatif. En effet, on sait qu’il existe toujours une droite qui passe entre deux points dans le plan. De même, il existe toujours une parabole qui passe par 3 points quelconques dans le plan. Et par extension, il existe une courbe correspondant à un polynome d’ordre n - 1 qui passe par n’importe quel ensemble de n points dans le plan. Un modèle construit à l’aide d’un tel polynome aura toujours un R2 égal à un, … mais en même temps ce modèle ne sera d’aucune utilité car il ne contient plus aucune information pertinente. C’est ce qu’on appelle le surajustement (overfitting en anglais). La figure ci-dessous (issue d’un article écrit par Anup Bhande ici) illuste bien ce phénomène. Devoir calculer les différentes puissance des variables au préalable devient rapidement fastidieux. Heureusement, R autorise de glisser ce cacul directement dans la formule, mais à condition de lui indiquer qu’il ne s’agit pas du nom d’une variable diameter^2, mais d’un calcul effectué sur diameter en utilisant la fonction, d’identité I(). Ainsi, sans rien calculer au préalable, nous pouvons utiliser la formule volume ~ diameter + I(diameter^2). Un snippet est d’ailleurs disponible pour ajuster un polynome d’ordre 2 ou d’ordre 3, et il est accompagné du code nécessaire pour représenter également graphiquement cette régression polynomiale. Le code ci-dessous qui construit le modèle lm3 l’utilise. summary(lm3 &lt;- lm(data = trees, volume ~ diameter + I(diameter^2))) # # Call: # lm(formula = volume ~ diameter + I(diameter^2), data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # I(diameter^2) 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 lm3 %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2)))(.) Remarquez sur le graphique comment, à présent, la courbe s’ajuste bien mieux dans le nuage de point et comme l’arbre le plus grand avec un diamètre supérieur à 0,5m est à présent presque parfaitement ajusté par le modèle. Faites donc très attention que des points influents ou extrêmes peuvent apparaitre également comme tel à cause d’un mauvais choix de modèle ! L’analyse des résidus nous montre aussi un comportement plus sain. #plot(lm3, which = 1) lm3 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm3, which = 2) lm3 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm3, which = 3) lm3 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm3, which = 4) lm3 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Revenons un instant sur le résumé de ce modèle. summary(lm3) # # Call: # lm(formula = volume ~ diameter + I(diameter^2), data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # I(diameter^2) 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 La pente relative au diameter nécessite quelques éléments d’explication. En effet, que signifie une pente pour une courbe dont la dérivée première (“pente locale”) change constamment ? En fait, il faut comprendre ce paramètre comme étant la pente de la courbe au point x = 0. Si le modèle est très nettement significatif (ANOVA, valeur p &lt;&lt;&lt; 0,001), et si le R2 ajusté grimpe maintenant à 0,959, seul le paramètre relatif au diamètre2 est significatif cette fois-ci. Ce résultat suggère que ce modèle pourrait êtrte simplifié en considérant que l’ordonnée à l’origine et la pente pour le terme diameter valent zéro. Cela peut être tenté, mais à condition de refaire l’analyse. On ne peut jamais laisser tomber un paramètre dans une analyse et considérer que les autres sont utilisable tels quels. tous les paramètres caculés sont interconnectés. Voyons ce que cela donne (la formule devient volume ~ I(diameter^2) - 1 ou volume ~ I(diameter^2) + 0, ce qui est identique) : summary(lm4 &lt;- lm(data = trees, volume ~ I(diameter^2) - 1)) # # Call: # lm(formula = volume ~ I(diameter^2) - 1, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.19474 -0.07234 -0.04120 0.04522 0.18240 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # I(diameter^2) 7.3031 0.1397 52.26 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1027 on 30 degrees of freedom # Multiple R-squared: 0.9891, Adjusted R-squared: 0.9888 # F-statistic: 2731 on 1 and 30 DF, p-value: &lt; 2.2e-16 Notez bien que quand on réajuste un modèle simplifié, les paramètres restants doivent être recalculés. En effet, le paramètre relatif au diamètre2 vallait 11,2 dans le modèle lm3 plus haut. Un fois les autres termes éliminés, ce paramètre devient 7,30 dans ce modèle lm4 simplifié. Le modèle lm4 revient aussi (autre point de vue) à transformer d’abord le diamètre en diamètre2 et à effectuer ensuite une régression linéaire simple entre deux variables, volume et diametre2 : summary(lm4bis &lt;- lm(data = trees, volume ~ diameter2 - 1)) # # Call: # lm(formula = volume ~ diameter2 - 1, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.19474 -0.07234 -0.04120 0.04522 0.18240 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # diameter2 7.3031 0.1397 52.26 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1027 on 30 degrees of freedom # Multiple R-squared: 0.9891, Adjusted R-squared: 0.9888 # F-statistic: 2731 on 1 and 30 DF, p-value: &lt; 2.2e-16 Notez qu’on obtient bien évidemment exactement les mêmes résultats si nous transformons d’abord les données ou si nous intégrons le calcul à l’intérieur de la formule qui décrit le modèle. Faites bien attention de ne pas comparer le R2 acvec ordonnée à l’origine fixée à zéro ici dans notre modèle lm4 avec les R2 des modèles lm. ou lm3 qui ont ce paramètre estimé. Rappelez-vous que le R2 est calculé différemment dans les deux cas ! Donc, nous voilà une fois de plus face à un nouveau modèle pour lequel il nous est difficile de décider s’il est meilleur que les précédents. Avant de comparer, élaborons un tout dernier modèle, le plus complexe, qui reprend à la fois notre régression polynomiale d’ordre 2 sur le diamètre et la hauteur. Autrement dit, une régression à la fois multiple et polynomiale. summary(lm5 &lt;- lm(data = trees, volume ~ diameter + I(diameter^2) + height)) # # Call: # lm(formula = volume ~ diameter + I(diameter^2) + height, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.12169 -0.04806 -0.00237 0.05156 0.12559 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -0.267097 0.284895 -0.938 0.356798 # diameter -3.281421 1.463401 -2.242 0.033354 * # I(diameter^2) 11.891724 2.019252 5.889 2.83e-06 *** # height 0.034788 0.008169 4.259 0.000223 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.07436 on 27 degrees of freedom # Multiple R-squared: 0.977, Adjusted R-squared: 0.9745 # F-statistic: 382.8 on 3 and 27 DF, p-value: &lt; 2.2e-16 Ah ha, ceci est bizarre ! Le R2 ajusté nous indique que le modèle serait très bon puisqu’il grimpe à 0,975. Le terme en diamètre2 reste très significatif, … mais la pente relative à la hauteur est maintenant elle aussi très significative alors que dans le modèle multiple lm2 ce n’était pas le cas. De plus, la pente à l’origine en face du diamètre semble devenir un peu plus significative. Bienvenue dans les instabilités liées aux intercorrelations entre paramètres dans les modèles linéaires complexes. "],
["rmse-critere-dakaike.html", "2.4 RMSE &amp; critère d’Akaike", " 2.4 RMSE &amp; critère d’Akaike Le R2 (ajusté) n’est pas la seule mesure d’ajustement d’un modèle. Il existe d’autres indicateurs. Par exemple, l’erreur quadratique moyenne, (root mean square error, ou RMSE en anglais) est la racine carrée de la moyenne des résidus au carré. Elle représente en quelque sorte la distance “typique” des résidus. Comme cette distance est exprimée dans les mêmes unités que l’axe y, cette mesure est particulièrement parlante. Nous pouvons l’obtenir par exemple comme ceci : modelr::rmse(lm., trees) # [1] 0.1166409 Cela signifie que l’on peut s’attendre à ce que, en moyenne, les valeurs prédites de volume de bois s’écartent (dans un sens ou dans l’autre) de 0,117 m3 de la valeur effectivement observée. Evidemment, plus un modèle est bon, plus le RMSE est faible, contrairement au R2 qui lui doit être élevé. Si le R2 comme le RMSE sont utiles pour quantifier la qualité d’ajustement d’une régression, ces mesures sont peu adaptées pour la comparaison de modèles entre eux. En effet, nous avons vu que plus le modèle est complexe, mieux il s’ajuste dans les données. Le R2 ajusté tente de remédier partiellement à ce problème, mais cette métrique reste peu fiable pour comparer des modèles très différents. Le critère d’Akaike, du nom du statisticien japonais qui l’a conçu, est une métrique plus adaptée à de telles comparaisons. Elle se base au départ sur encore une autre mesure de la qualité d’ajustement d’un modèle : la log-vraisemblance. Les explications relatives à cette mesure sont obligatoirement complexes d’un point de vue mathématique et nous vous proposons ici d’en retenir la définition sur un plan purement conceptuel. Un estimateur de maximum de vraisemblance est une mesure qui permet d’inférer le meilleur ajustement possible d’une loi de probabilité par rapport à des données. Dans le cas de la régression par les moindres carrés, la distribution de probabilité à ajuster est celle des résidus (pour rappel, il s’agit d’une distribution Normale de moyenne nulle et d’écart type constant \\(\\sigma\\)). La log-vraisemblance, pour des raisons purement techniques est souvent préféré au maximum de vraissemblance. Il s’agit simplement du logarithme de sa valeur. Donc, plus la log-vraisemblance est grande, mieux les données sont compatibles avec le modèle probabiliste considéré. Pour un même jeu de données, ces valeurs sont comparables entre elles… même pour des modèles très différents. Mais cela ne règle pas la question de la complexité du modèle. C’est ici qu’Akaike entre en piste. Il propose le critère suivant : \\[ \\textrm{AIC} = -2 . \\textrm{log-vraisemblance} + 2 . \\textrm{nbrpar} \\] où nbrpar est le nombre de paramètres à estimer dans le modèle. Donc ici, nous prenons comme point de départ moins deux fois la log-vraisemblance, une valeur a priori à minimiser, mais nous lui ajoutons le second terme de pénalisation en fonction de la complexité du modèle valant 2 fois le nombre de paramètres du modèle. Notons d’ailleurs que le terme multiplicateur 2 ici est modifiable. Si nous voulons un modèle le moins complexe possible, nous pourrions très bien multiplier par 3 ou 4 pour pénaliser encore plus. Et si nous voulons être moins restrictifs, nous pouvons aussi diminuer ce facteur multiplicatif. Dans la pratique, le facteur 2 est quand même très majoritairement adapté par les praticiens, mais la possibilité de changer l’impact de complexité du modèle est inclue dans le calcul de facto. Dès lors que ce critère peut être calculé (et R le fait pour pratiquement tous les modèles qu’il propose), une comparaison est possible avec pour objectif de sélectionner le, ou un des modèles qui a l’AIC la plus faible. N’oubliez toutefois pas de comparer visuellement les différents modèles ajustés et d’interpréter les graphiques d’analyse des résidus respectifs en plus des valeurs d’AIC. C’est l’ensemble de ces outils qui vous orientent vers le meilleur modèle, pas l’AIC seul ! Calculons maintenant les critères d’Akaike pour nos 6 modèles lm. à lm5… AIC(lm.) # Linéaire diamètre # [1] -39.24246 AIC(lm2) # Multiple diamètre et hauteur # [1] -43.82811 AIC(lm3) # Polynomial diamètre # [1] -53.50964 AIC(lm4) # Diamètre^2 # [1] -50.15027 AIC(lm5) # Multiple et polynomial # [1] -67.4391 D’après ce critère, le modèle linéaire est le moins bon, et le dernier modèle le plus complexe serait le meilleur. Notez toutefois que la différence est relativement minime (en regard du gain total) entre le modèle polynomial complet lm3 et la version simplifié au seul terme diamètre2 en lm4, ce qui permet de penser que cette simplification est justifiée. Dans l’hypothèse où nous déciderions de conserver le modèle lm5, en voici l’analyse des résidus qui est bonne dans l’ensemble : #plot(lm5, which = 1) lm5 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm5, which = 2) lm5 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm5, which = 3) lm5 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm5, which = 4) lm5 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Naturellement, même si c’est le cas ici, ce n’est pas toujours le modèle le plus complexe qui “gagne” toujours. Même ici, nous pourrions nous demander si le modèle polynomial utilisant uniquement le diamètre ne serait pas plus intéressant en pratique car son ajustement est tout de même relativement bon (même si son critère d’Akaike est nettement moins en sa faveur), mais d’un point de vue pratique, il nous dispense de devoir mesurer la hauteur des arbres pour prédire le volume de bois. Ce n’est peut-être pas négligeable comme gain, pour une erreur de prédiction légèrement supérieure si on compare les valeurs de RMSE. modelr::rmse(lm5, trees) # Multiple et polynomial # [1] 0.06939391 modelr::rmse(lm3, trees) # Polynomial diamètre # [1] 0.08972287 L’erreur moyenne d’estimation du volume de bois passe de 0,07 m3 pour le modèle le plus complexe lm5 utilisant à la fois le diamètre et la hauteur à 0,09 m3. C’est à l’exploitant qu’il appartient de déterminer si le gain de précision vaut la peine de devoir effectuer deux mesures au lieu d’une seule. Mais au moins, nous sommes capables, en qualité de scientifiques des données, de lui proposer les alternatives possible et d’en quantifier les effets respectifs. Différentes méthodes d’ajustement par xkcd. A vous de jouer ! Après cette longue lecture avec énormément de nouvelles matières, nous vous proposons les exercices suivants : Répondez aux questions d’un learnr afin de vérifier vos acquis. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;02b_reg_poly&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Réalisez un carnet de laboratoire sur la biométrie des oursins avec l’assignation ci-dessous. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/5hI-HSOv Réalisez un rapport scientifique sur la croissance des escargots géants d’Afrique. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/_wJZDbNp "],
["mod-lineaire.html", "Module 3 Modèle linéaire", " Module 3 Modèle linéaire Objectifs Comprendre le modèle linéaire (ANOVA et régression linéaire tout en un) Appréhender la logique des matrices de contraste Découvrir l’ANCOVA Comprendre le mécanisme du modèle linéaire généralisé Prérequis L’ANOVA (modules 10 &amp; 11 du cours SDD 1), ainsi que la régression linéaires (modules 1 et 2 du présent cours) doivent être maitrisés avant d’aborder cette matière. "],
["variables-numeriques-ou-facteurs.html", "3.1 Variables numériques ou facteurs", " 3.1 Variables numériques ou facteurs L’ANOVA analyse une variable dépendante numérique en fonction d’une ou plusieurs variables indépendantes qualitatives. Ces variables sont dites “facteurs” non ordonnés (objets de classe factor), ou “facteurs” ordonnés (objets de classe ordered) dans R. La régression linéaire analyse une variable dépendante numérique en fonction d’une ou plusieurs variables indépendantes numérique (quantitatives) également. Ce sont des objets de classe numeric (ou éventuellement integer, mais assimilé à numeric concrètement) dans R. Donc, la principale différence entre ANOVA et régression linéaire telles que nous les avnos abordés jusqu’ici réside dans la nature de la ou des variables indépendantes, c’est-à-dire, leur type. Pour rappel, il existe deux grandes catégories de variables : quantitatives et qualitatives, et deux sous-catégories pour chacune d’elle. Cela donne quatyre types principaux de variables, formant plus de 90% des cas rencontrés : variables quantitatives continues représentables par des nombres réels (numeric dans R), variables quantitatives discrètes pour des dénombrements d’événements finis par exemple, et représentables par des nombres entiers (integer dans R), variables qualitatives ordonnées pour des variables prenant un petit nombre de valeurs, mais pouvant être ordonnées de la plus petite à la plus grande (ordered dans R), variables qualitatives non ordonnées prenant également un petit nombre de valeurs possibles, mais sans ordre particulier (factor dans R). Par la suite, un encodage correct des variables sera indispensable afin de distinguer correctement ces différentes situations. En effet, R considèrera automatiquement comment mener l’analyse en fonction de la classe des variables fournies. Donc, si la classe est incorrecte, l’analyse le sera aussi ! Si vous avez des doutes concernant les types de variables, relisez la section type de variables avant de continuer ici. "],
["anova-et-regression-lineaire.html", "3.2 ANOVA et régression linéaire", " 3.2 ANOVA et régression linéaire Avez-vous remarqué une ressemblance particulière entre la régression linéaire que nous avons réalisé précédement et l’analyse de variance (ANOVA) ? Les plus observateurs auront mis en avant que la fonction de base dans R est la même dans les deux cas : lm(). Cette fonction est donc capable de traiter aussi bien des variables réponses qualitatives que quantitatives, et effectue alors une ANOVA dans un cas ou une régression linéaire dans l’autre. Par ailleurs, nous avons vu que l’ANOVA et la régression linéaire se représentent par des modèles semblables : \\(y = \\mu + \\tau_i + \\epsilon\\) pour l’ANOVA et \\(y = \\beta_1 + \\beta_2 x + \\epsilon\\) pour la régression linéaire, avec \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\) dans les deux cas. Donc, nous retrouvons bien au niveau du modèle mathématique sous-jacent la différence principale entre les deux qui réside dans le type de variable indépendante (ou explicative) : Variable qualitative pour l’ANOVA, Variable quantitative pour la régression linéaire. Le calcul est, en réalité, identique en interne. Il est donc possible de généraliser ces deux approches en une seule appelée modèle linéaire, mais à condition d’utiliser une astuce pour modifier nos modèles afin qu’ils soient intercompatibles. 3.2.1 Modèle linéaire commun Le nœud du problème revient donc à transformer nos modèles mathématiques pour qu’ils puissent être fusionnés en un seul. Comment homogénéiser ces deux modèles ? \\(y = \\mu + \\tau_i + \\epsilon\\) pour l’ANOVA et \\(y = \\beta_1 + \\beta_2 x + \\epsilon\\) pour la régression linéaire. Avant de poursuivre, réfléchisser un peu par vous-même. Quelles sont les différences qu’il faut contourner ? Est-il possible d’effectuer une ou plusieurs transformations des variables pour qu’elles se comportent de manière similaire dans les deux cas ? 3.2.2 Réencodage des variables de l’ANOVA Considérons dans un premier temps, un cas très simple : une ANOVA à un facteur avec une variable indépendante qualitative (factor) à deux niveaux5. Nous pouvons écrire : \\[ y = \\mu + \\tau_1 I_1 + \\tau_2 I_2 + \\epsilon \\] avec \\(I_i\\), une variable dite indicatrice créée de toute pièce qui prend la valeur 1 lorsque le niveau correspond à i, et 0 dans tous les autres cas. Vous pouvez vérifier par vous-même que l’équation ci-dessus fonctionnera exactement de la même manière que le modèle utilisé jusqu’ici pour l’ANOVA. En effet, poiur un individu de la population 1, \\(I_1\\) vaut 1 et \\(\\tau_1\\) est utilisé, alors que comme \\(I_2\\) vaut 0, \\(\\tau_2\\) est annulé dans l’équation car \\(\\tau_2 I_2\\) vaut également 0. Et c’est exactement l’inverse qui se produit pour un individu de la population 2, de sorte que c’est \\(\\tau_2\\) qui est utilisé cette fois-ci. Notez que notre nouvelle formulation, à l’aide de variables indicatrices ressemble fortement à la régression linéaire. La seule différence par rapport à cette dernière est que nos variables \\(I_i\\) ne peuvent prendre que des valeurs 0 ou 1 (en tous cas, pour l’instant), alors que les \\(x_i\\) dans la régression linéaire multiple sont des variables quantitatives qui peuvent prendre une infinité de valeurs différentes (nombres réels). Nouys pouvons encore réécrire notre équation comme suit pour qu’elle se rapproche encore plus de celle de la régression linéaire simple. Passons par l’introduction de deux termes identiques \\(\\tau_1 I_2\\) additionné et soustrait, ce qui revient au même qu’en leur absence : \\[ y = \\mu + \\tau_1 I_1 + \\tau_1 I_2 - \\tau_1 I_2 + \\tau_2 I_2 + \\epsilon \\] En considérant \\(\\beta_2 = \\tau_2 - \\tau_1\\), cela donne : \\[ y = \\mu + \\tau_1 I_1 + \\tau_1 I_2 + \\beta_2 I_2 + \\epsilon \\] En considérant \\(\\beta_1 = \\mu + \\tau_1 = \\mu + \\tau_1 I_1 + \\tau_1 I_2\\) (car quelle que soit la population à laquelle notre individu appartient, il n’y a jamais qu’une seule des deux valeurs \\(I_1\\) ou \\(I_2\\) non nulle et dans tous les cas le résultat est donc égal à \\(\\tau_1\\)), on obtient : \\[ y = \\beta_1 + \\beta_2 I_2 + \\epsilon \\] Cette dernière formulation est strictement équivalente au modèle de la régression linéaire simple dans laquelle la variable \\(x\\) a simplement été remplacée par notre variable indicatrice \\(I_2\\). Ceci se généralise pour une variable indépendante à \\(k\\) niveaux, avec \\(k - 1\\) variables indicatrices au final. En prenant soin de réencoder le modèle de l’ANOVA relatif aux variables indépendantes qualitatives, nous pouvons à présent mélanger les termes des deux modèles en un seul : notre fameux modèle linéaire. Nous aurons donc, quelque chose du genre (avec les \\(x_i\\) correspondant aux variables quantitatives et les \\(I_j\\) des variables indicatrices pour les différents niveaux des variables qualitatives) : \\[ y = \\beta_1 + \\beta_2 x_1 + \\beta_3 x_2 + ... + \\beta_n I_1 + \\beta_{n+1} I_2 ... + \\epsilon \\] Concrètement, un cas aussi simple se traite habituellement à l’aide d’un test t de Student, mais pour notre démonstration, nous allons considérer ici utiliser une ANOVA à un facteur plutôt.↩ "],
["matrice-de-contraste.html", "3.3 Matrice de contraste", " 3.3 Matrice de contraste La version que nous avons étudié jusqu’ici pour nos variables indicatrices, à savoir, une seule prend la valeur 1 lorsque toutes les autres prend une valeur zéro, n’est qu’un cas particulier de ce qu’on appelle les contrastes appliqués à ces variables indicatrices. En réalité, nous pouvons leurs donner bien d’autres valeurs (on parle de poids), et cela permettra de considérer dses contrastes différents, eux-mêmes représentatifs de situations différentes. Afin de mieux comprendre les contrastes appliqués à nos modèles linéaires, les statisticiens ont inventé les matrices de contrastes. Ce sont des tableaux à deux entrées indiquant pour chaque niveau de la variable indépendante qualitative quelles sont les valeurs utilisées pour les différentes variables indicatrices présentées en colonne. Dans le cas de notre version simplifiée du modèle mathématique où nous avons fait disparaitre \\(I_1\\) en l’assimilant à la moyenne \\(\\mu\\) pour obteniur \\(\\beta_1\\). Dans le cas où notre variable qualitative a quatre niveaux, nous avons donc le modèle suivant : \\[ y = \\beta_1 + \\beta_2 I_2 + \\beta_3 I_3 + \\beta_4 I_4 + \\epsilon \\] Cela revient à considérer le premier niveau comme niveau de référence et à établir tous les contrastes par rapport à ce niveau de référence. C’est une situation que l’on rencontre fréquemment lorsque nos testons l’effet de différents médicaments ou de différents traitement par rapport à un contrôle (pas de traitement, ou placébo). La matrice de contrastes correspondante, dans un cas où on aurait trois traitements en plus du contrôle (donc, notre variable factor à quatre niveaux) s’obteint facilement dans R à l’aide de la fonction contr.treatment() : contr.treatment(4) # 2 3 4 # 1 0 0 0 # 2 1 0 0 # 3 0 1 0 # 4 0 0 1 Les lignes de cette matrice sobnt numérotées de 1 à 4. Elles correspondent aux quatres niveaux de notre variable factor, avec le niveau 1 qui doit nécessairement correspondre à la situation de référtence, donc au contrôle. Les colonnes de cette matrice correspondent aux trois variables indicatrices \\(I_1\\), \\(I_2\\) et \\(I_3\\) de l’équation au dessus. Nous voyons que pour une individu contrôle, de niveau 1, les trois \\(I_i\\) prennent la valeur 0. Nous sommes bien dans la situation de référence. En d’autres terme, le modèle de base est ajusté sur la moyenne des individus contrôle. Notre modèle se réduit à : \\(y = \\beta_1 + \\epsilon\\). Donc, seule la moyenne des individus contrôles, \\(\\beta_1\\) est considérée, en plus des résidus \\(\\epsilon\\) bien sûr. Pour le niveau deux, nous observons que \\(I_2\\) vaut 1 et les deux autres \\(I_i\\) valent 0. Donc, cela revient à considérer un décalage constant \\(\\beta_2\\) appliqué par rapport au modèle de référence matérialisé par \\(\\beta_1\\). En effezt, notre équation se réduit dans ce cas à : \\(y = \\beta_1 + \\beta_2 + \\epsilon\\). Le même raisonnement peut être fait pour les niveaux 3 et 4, avec des décalages constants par rapport à la situation cxontrôle de respectivement \\(\\beta_3\\) et \\(\\beta_4\\). En d’autres termes, les contrastes qui sont construits ici font tous référence au contrôle, et chaque médicament est explicitement comparté au contrôle (mais les médicaments ne sont pas comparés entre eux). Nous voyons donc que les variables indicatrices etr la matrice de contrastes permet de spécifier quelles sont les contrastes pertinents et éliminent ceux qui ne le sont pas (nous n’utilisons donc pas systématiquement toutes les comparaisons deux à deux des différents niveaux6). 3.3.1 Contraste orthogonaux Les contrastes doivent être de préférence orthogonaux par rapport à l’ordonnée à l’origine, ce qui signifie que la somme de leurs pondérations doit être nulle pour tous les contrastes définis (donc, en colonnes). Bien que n’étant pas obligatoire, cela confère des propriétés intéressantes au modèle (l’explication et la démonstration sortent du cadre de ce cours). Or, les contrastes de type traitement ne sont pas orthogonaux puisque toutes les sommes par colonnes vaut un. 3.3.2 Autres matrices de contrastes courantes Somme à zéro. Ces constraste, toujours pour une variable à quatre niveaux, se définissen t comme suit en utilisant la fonction contr.sum() dans R : contr.sum(4) # [,1] [,2] [,3] # 1 1 0 0 # 2 0 1 0 # 3 0 0 1 # 4 -1 -1 -1 Ici nous avons bien des contrastes orthogonaux puisque toutes les sommes par colonnes valeur zéro. Dans le cas présent, aucun niveau n’est considéré comme référence, mais les n - 1 niveaux sont systématiquement contrastés avec le dernier et nîème^ niveau. Ainsi, un contraste entre deux niveaux particuliers peut s’élaborer en indiquant une pondération de 1 pour le premier niveau à comparer, une pondération de -1 pour le second à comparer et une pondération de 0 pour tous les autres. Matrice de contrastes de Helmert : chaque niveau est comparé à la moyenne des niveaux précédents. La matrice de constrastes correspondant pour une variable à quatre niveaux s’obtient à l’aide de la fonction R contr.helmert() : contr.helmert(4) # [,1] [,2] [,3] # 1 -1 -1 -1 # 2 1 -1 -1 # 3 0 2 -1 # 4 0 0 3 Cette matrice est également orthogonale avec toutes les sommes par colonnes qui valent zéro. Ici, nous découvrons qu’il est possible de créer un contrastye entre un niveau et la moyenne de plusieurs autres niveaux en mettant le poids du premier à m (le nombre de populations à comparer de l’autre côté du contraste), et les poids des autres populations tous à -1. Ainsi, la colonne 4 compare le niveau quatre avec pondération 3 aux trois autres niveaux qui reçoivent tous une pondération -1. Matrice de contrastes polynomiaux : adapté aux facteurs ordonnés (ordered dans R) pourvlesquels on s’attend à une certaine évolution du modèle du niveau le plus petit au plus grand. Donc ici aussi une comparaison deux à deux de tous les niveaux n’est pas souhaitable, mais une progression d’un effet qui se propage de manière graduelle du plus petit niveau au plus grand. A priori cela parait difficile à métérialiser dans une matrice de contraste… et pourtant, c’est parfaitement possible ! Il s’agit de constrastes polynomiaux où nous ajustons de polynomes de degré croissant comme pondération des différents contrastes étudiés. La fonction contr.poly() permet d’obtenir ce type de contraste dans R. Pour une variable ordonnée à quatre niveaux, cela donne : contr.poly(4) # .L .Q .C # [1,] -0.6708204 0.5 -0.2236068 # [2,] -0.2236068 -0.5 0.6708204 # [3,] 0.2236068 -0.5 -0.6708204 # [4,] 0.6708204 0.5 0.2236068 Ici, les pondérations sont plus difficiles à expliquer rien qu’en observant la matrice de contrastes. De plus, les colonnes portent ici des noms particuliers .L pour un contraste linéaire (polynome d’ordre 1), .Q pour un contraste quadratique (polynome d’ordre 2), et .C pour un contraste conique (ou polynome d’ordre 3). Les pondérations appliquées se comprennent mieux lorsqu’on augmente le nombre de niveaux etr que l’on représente graphiquement la valeur des pondérations choisées. Par exemple, pour une variable facteur ordonnée à dix niveaux, nous représentrons graphiquement les 3 premeirs contrastes (linéaire, quadratique et conique) comme suit : plot(contr.poly(10)[, 1], type = &quot;b&quot;) plot(contr.poly(10)[, 2], type = &quot;b&quot;) plot(contr.poly(10)[, 3], type = &quot;b&quot;) Sur le graphique, l’axe X nommé index correspiond en réalité à la succession des 10 niveaux de la variable présentés dans l’ordre du plus petit au plus grand. Nous voyons maintenant clairement comment les contrastes sont construits ici. Pour le conbtraste linéaire, on contraste les petits niveaux avec les grands, et ce, de manière proportionnelle par rapport à la progression d’un niveau à l’autre (polynome d’ordre un = droite). Pour le contraste quadratique, on place “dans le même sac” les petits et greand niveaux qui sont contrastés avec les niveaux moyens (nous avons une parabole ou polynome d’ordre 2). Pour le troisième graphique, la situation se complexifie en encore un peu plus avec un polynome d’ordre 3, et ainsi de suite pour des polynomes d’ordres croissants jusqu’à remplir complètement la matrice de contrastes. R utilise par défaut des contrastes de traitement pour les facteurs non ordonnés et des contrastes polynomiaux pour des facteurs ordonnés. Ces valeurs par défaut sont stockées dans l’option contrasts que l’on peut lire à l’aide de getOption(). Bien sûr, il est possible de changer ces contrastes, tant au niveau global qu’au niveau de la construction d’un modèle en particulier. getOption(&quot;contrasts&quot;) # unordered ordered # &quot;contr.treatment&quot; &quot;contr.poly&quot; Attention : le fait d’utiliser une matrtice de contraste qui restreint ceux utilisés dans le modèle est indépendant des tests post hoc de comparaisons multiples, qui restent utilisables par après. Les comparaisons deux à deux des médicaments restent donc accessibles, mais ils ne sont tout simplement pas mis en évidence dans le modèle de base.↩ "],
["ancova.html", "3.4 ANCOVA", " 3.4 ANCOVA Avant l’apparition du modèle linéaire, une version particulière d’un mélange de régression linéaire et d’une ANOVA avec une variable indépendante quantitative et une autre variable indépendante qualitative s’appelait une ANCOVA (ANalyse de la COVariance). Un tel modèle d’ANCOVA peut naturellement également se résoudre à l’aide de la fonction lm() qui, en outre, peut faire bien plus. Nous allons maintenant ajuster un tel modèle à titre de première application concrète de tout ce que nous venons de voir sur le modèle linéaire et sur les matrices de contrastes associées. 3.4.1 Bébés à la naissance Nous étudions la masse de nouveaux nés en fonction du poids de la mère et du fait qu’elle fume ou non. Cette analyse s’inspire de Verzani (2005). Nous avons donc ici une variable dépendante wt, la masse des bébés qui est quantitative, et deux variables indépendantes ou prédictives wt1, la masse de la mère, et smoke le fait que la mère fume ou non. Or la première de ces variables explicatives est quantitative (wt1) et l’autre (smoke) est une variable facteur à quatre niveaux (0 = la mère n’a jamais fumé, 1 = elle fume y compris pendant la grossesse, 2 = elle fumait mais a arrêté à la grossesses, et 3 = la mère a fumé, mais a arrêté, et ce, bien avant la grossesse. Un dernier niveau 9 = inconnu encode de manière non orthodoxe les valeurs manquantes dans notre tableau de données (valeurs que nous éliminerons). De même les masses des nouveaux nés et des mères sont des des unités impériales (américaines) respectivement en “onces” et en “livres”. Enfin, nous devons prendre soin de bien encoder la variable smoke comme une variable factor (ici nous ne considèrerons pas qu’il s’agit d’un facteur ordonné et nous voulons faire un contraste de type traitement avec comparaison à des mères qui n’ont jamais fumé). Un reminement soigneux des données est donc nécessaire avant de pouvoir appliquer notre modèle ! SciViews::R babies &lt;- read(&quot;babies&quot;, package = &quot;UsingR&quot;) knitr::kable(head(babies)) id pluralty outcome date gestation sex wt parity race age ed ht wt1 drace dage ded dht dwt marital inc smoke time number 15 5 1 1411 284 1 120 1 8 27 5 62 100 8 31 5 65 110 1 1 0 0 0 20 5 1 1499 282 1 113 2 0 33 5 64 135 0 38 5 70 148 1 4 0 0 0 58 5 1 1576 279 1 128 1 0 28 2 64 115 5 32 1 99 999 1 2 1 1 1 61 5 1 1504 999 1 123 2 0 36 5 69 190 3 43 4 68 197 1 8 3 5 5 72 5 1 1425 282 1 108 1 0 23 5 67 125 0 24 5 99 999 1 1 1 1 5 100 5 1 1673 286 1 136 4 0 25 2 62 93 3 28 2 64 130 1 4 2 2 2 Ce tableau est “brut de décoffrage”. Voyez help(&quot;babies&quot;, package = &quot;UsingR&quot;) pour de plus amples informations. Nous allons maintenant remanier tout cela correctement. # wt = masse du bébé à la naissance en onces et 999 = valeur manquante # wt1 = masse de la mère à la naissance en livres et 999 = valeur manquante # smoke = 0 (non), = 1 (oui), = 2 (jusqu&#39;à grossesse), # = 3 (plus depuis un certain temps) and = 9 (inconnu) babies %&gt;.% select(., wt, wt1, smoke) %&gt;.% # Garder seulement wt, wt1 &amp; smoke filter(., wt1 &lt; 999, wt &lt; 999, smoke &lt; 9) %&gt;.% # Eliminer les valeurs manquantes mutate(., wt = wt * 0.02835) %&gt;.% # Transformer le poids en kg mutate(., wt1 = wt1 * 0.4536) %&gt;.% # Idem mutate(., smoke = as.factor(smoke)) -&gt; # S&#39;assurer d&#39;avoir une variable factor Babies # Enregistrer le résultat dans Babies knitr::kable(head(Babies)) wt wt1 smoke 3.40200 45.3600 0 3.20355 61.2360 0 3.62880 52.1640 1 3.48705 86.1840 3 3.06180 56.7000 1 3.85560 42.1848 2 Description des données : skimr::skim(Babies) # Skim summary statistics # n obs: 1190 # n variables: 3 # # ── Variable type:factor ─────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts # smoke 0 1190 1190 4 0: 531, 1: 465, 3: 102, 2: 92 # ordered # FALSE # # ── Variable type:numeric ────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # wt 0 1190 1190 3.39 0.52 1.56 3.06 3.4 3.71 4.99 # wt1 0 1190 1190 58.3 9.49 39.46 51.82 56.7 62.6 113.4 # hist # ▁▁▂▆▇▅▁▁ # ▂▇▆▂▁▁▁▁ chart(data = Babies, wt ~ wt1 %col=% smoke) + geom_point() + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt ~ smoke) + geom_boxplot() + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt1 ~ smoke) + geom_boxplot() + ylab(&quot;Masse de la mère [kg]&quot;) Visuellement, nous ne voyons pas d’effet marquant. Peut-être la condition 1 de smoke (mère qui fume pendant la grossesse) mène-t-il à des bébés moins gros, mais est-ce significatif ? Pour cela, ajustons notre modèle ANCOVA avec matrice traitement (choix par défaut pour une la variable factor smoke). Comme nous savons déjà utiliser lm(), c’est très simple. Cela fonctionne exactement comme avant7. # ANCOVA Babies_lm &lt;- lm(data = Babies, wt ~ smoke * wt1) summary(Babies_lm) # # Call: # lm(formula = wt ~ smoke * wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.9568 -0.3105 0.0133 0.3136 1.4989 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.000663 0.128333 23.382 &lt; 2e-16 *** # smoke1 -0.303614 0.196930 -1.542 0.123405 # smoke2 0.901888 0.371393 2.428 0.015314 * # smoke3 -0.035502 0.371379 -0.096 0.923858 # wt1 0.008117 0.002149 3.777 0.000167 *** # smoke1:wt1 0.001153 0.003346 0.345 0.730444 # smoke2:wt1 -0.015340 0.006390 -2.401 0.016523 * # smoke3:wt1 0.001177 0.006147 0.191 0.848258 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4992 on 1182 degrees of freedom # Multiple R-squared: 0.08248, Adjusted R-squared: 0.07705 # F-statistic: 15.18 on 7 and 1182 DF, p-value: &lt; 2.2e-16 anova(Babies_lm) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.9636 1.158e-15 *** # wt1 1 6.162 6.1621 24.7325 7.559e-07 *** # smoke:wt1 3 1.653 0.5511 2.2117 0.08507 . # Residuals 1182 294.497 0.2492 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 L’analyse de variance montre que la masse de la mère a un effet significatif au seuil alpha de 5%, de même si la mère fume. Par contre, il n’y a pas d’interactions entre les deux. Le fait de pouvoir meurer des interactions entre variables qualitatives et quantitatives est ici bien évidemment un plus du modèle linéaire par rapport à ce qu’on pouvait faire avant ! Le résumé de l’analyse nous montre que la régression de la masse des bébés en fonction de la masse de la mère (ligne wt1 dans le tableau des coefficients), bien qu’étant significative, n’explique que 8% de la variance totale (le \\(R^2\\)). Les termes smoke1, smoke2 et smoke3 sont les contrastes appliqués par rapport au contrôle (smoke == 0). On voit ici qu’aucun de ces contrastes n’est significatif au seuil alpha de 5%. Cela signifie que le seul effet significatif est celui lié à une ordonnée à l’origine non nulle (Intercept) matérialisant la condition smoke == 0. Cela signifie que des mères de masse nulle n’ayant jamais fumé engendreraient des bébés pesant environ 3kg. Dans le contexte présent, cette constatation n’a bien sûr aucun sens, et l’interprétation de l’ordonnée à l’origine ne doit pas être faite. Donc, le modèle linéaire, en offrant plus de contrôle dans notre ajustement et une définition de contrastes “utiles” matérialisés par les lignes smoke1, smoke2 et smoke3 du tableau nous permet de faire des tests plus utiles dans le contexte de notre analyse. N’oublions pas non plus la possibilité de déterminer si des interactions entre smoke et wt1 existent pour ces différents contrastes, interactions testées respectivements aux lignes smoke1:wt1, smoke2:wt1, et smoke3:wt1du tableau des coefficients. Dans le cas présent, aucune de ces interactions n’est siginificative au seuil alpha de 5%. Pour comprendre à quoi tout cela fait référence, il faut considérer le modèle de base comme une droite de régression ajustée entre wt et wt1 pour la population de référence smoke == 0. Ainsi, si nous faisons : summary(lm(data = Babies, wt ~ wt1, subset = smoke == 0)) # # Call: # lm(formula = wt ~ wt1, data = Babies, subset = smoke == 0) # # Residuals: # Min 1Q Median 3Q Max # -1.95685 -0.25825 0.01476 0.25464 1.49890 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.000663 0.123572 24.283 &lt; 2e-16 *** # wt1 0.008117 0.002069 3.922 9.92e-05 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4806 on 529 degrees of freedom # Multiple R-squared: 0.02826, Adjusted R-squared: 0.02642 # F-statistic: 15.38 on 1 and 529 DF, p-value: 9.924e-05 Nous voyons en effet que les pentes et ordonnées à l’origine sont ici parfaitement identiques au modèle ANCOVA complet (mais pas les tests associés). Maintenant plus difficile : à quoi correspond une régression entre wt et wt1 pour smoke == 1 ? summary(lm(data = Babies, wt ~ wt1, subset = smoke == 1)) # # Call: # lm(formula = wt ~ wt1, data = Babies, subset = smoke == 1) # # Residuals: # Min 1Q Median 3Q Max # -1.70870 -0.35089 0.01034 0.33576 1.39420 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.697048 0.153270 17.597 &lt; 2e-16 *** # wt1 0.009270 0.002632 3.522 0.000471 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.5122 on 463 degrees of freedom # Multiple R-squared: 0.02609, Adjusted R-squared: 0.02399 # F-statistic: 12.4 on 1 and 463 DF, p-value: 0.0004711 Nous avons une ordonnées à l’origine qui vaut 2,70 ici. Notons que cela correspond aussi à (Intercept) + smoke1 = 3,00 - 0,30 = 2,70. Donc, l’ordonnées à l’origine pour smoke == 1 est bien la valeur de référence additionnée de la valeur fournie à la ligne smoke1 dans l’ANCOVA. Cela se vérifie aussi pour les deux autres droites pour smoke2 et smoke3. Maintenant, la pente pour notre droite ajustée sur la population smoke == 1 uniquement vaut 0,00927. Dans l’ANCOVA, nous avions une pente wt1 de 0,00812 et une interaction smoke1:wt1 claculée comme 0,00115. Notez alors que la pente de la droite seule 0,00927 = 0,00812 + 0,00115. Donc, tout comme smoke1 correspond au décalage de l’ordonnée à l’origine du modèle de référence, les interactions smoke1:wt1 correspondent au décalage de la pente par rapport au modèle de référence. Cela se vérifie également pour smoke2:wt1 et smoke3:wt1. Donc, notre modèle complet ne fait rien d’autre que d’ajuster les quatre droites correspondant aux relations linéaires entre wt et wt1, mais en décompose les effets, niveau par niveau de la variable qualitative smoke en fonction de la matrice de contraste que l’on a choisie. En bonnus, nous avons la possibilité de tester si chacune des composantes (tableau coefficient de summary()) ou si globalement chacune des variables (tableau obtenu avec anova()) a un effet significatif ou non dans le modèle. Le graphique correspondant est le même que si nous avions ajusté les 4 régressions linéaires indépendamment l’une de l’autre (mais les tests et les enveloppes de confiance diffèrent). chart(data = Babies, wt ~ wt1 %col=% smoke) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt ~ wt1 | smoke) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Comme toujours, lorsqu’un effet n’est pas siugnificatif, nous pouvons décider de simplifier le modèle. Mais attention ! Toujours considérer que les composantes sont interdépendantes. Donc, éliminer une composante du modèle peut avoir des effets parfois surprenants sur les autres. Voyons ce que cela donne si nous éliminons les interactions. Dans ce cas, nous ajustons des droites toutes parallèles avec uniquement un décalage de leur ordonnée à l’origine matérialisé par smoke1, smoke2 et smoke3 par rapport au modèle de référence ajusté pour la population smoke == 0 (notez l’utilisation, du signe + dans la formuile, là où nous utilisions le signe * dans la modèle précédent). # ANCOVA Babies_lm2 &lt;- lm(data = Babies, wt ~ smoke + wt1) summary(Babies_lm2) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.030052 0.092861 32.630 &lt; 2e-16 *** # smoke1 -0.237938 0.031816 -7.478 1.46e-13 *** # smoke2 0.022666 0.056508 0.401 0.688 # smoke3 0.035486 0.054068 0.656 0.512 # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 anova(Babies_lm2) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.887 1.285e-15 *** # wt1 1 6.162 6.1621 24.657 7.853e-07 *** # Residuals 1185 296.150 0.2499 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Hé, ça c’est intéressant ! Maintenant que nous avons éliminé les interactions qui apparaissent non pertinentes ici, nous avons toujours une régression significative entre wt et wt1 (mais avec un \\(R^2\\) très faible de 7,7%, attention), mais maintenant, nous faisons apparaitre un effet signicfication du contraste avec smoke1 au seuil alpha de 5%. Et du coup, les effets des deux variables deviennent plus clairs dans notre tableau de l’ANOVA. Le graphique correspondant est l’ajustement de droites parallèles les unes aux autres pour les 4 sous-populations en fonction de smoke. Ce graphique est difficile à réaliser. Il faut ruser, et les détails du code vont au delà de ce cours (il n’est pas nécessaire de les comprendre à ce stade). cols &lt;- iterators::iter(scales::hue_pal()(4)) # Get colors for lines chart(data = Babies, wt ~ wt1) + geom_point(aes(col = smoke)) + lapply(c(0, -0.238, 0.0227, 0.0355), function(offset) geom_smooth(method = lm, formula = y + offset ~ x, col = iterators::nextElem(cols))) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Voyons ce que donne l’analyse post hoc des comparaisons multiples (nous utilisons ici simplement le snippet disponible à partir de ... -&gt; hypothesis tests -&gt; hypothesis tests: means -&gt; hmanovamult : anova - multiple comparaisons [multcomp]) que nous avons déjà employé et qui reste valable ici. summary(anovaComp. &lt;- confint(multcomp::glht(Babies_lm2, linfct = multcomp::mcp(smoke = &quot;Tukey&quot;)))) # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lm(formula = wt ~ smoke + wt1, data = Babies) # # Linear Hypotheses: # Estimate Std. Error t value Pr(&gt;|t|) # 1 - 0 == 0 -0.23794 0.03182 -7.478 &lt; 1e-05 *** # 2 - 0 == 0 0.02267 0.05651 0.401 0.977 # 3 - 0 == 0 0.03549 0.05407 0.656 0.908 # 2 - 1 == 0 0.26060 0.05704 4.568 2.79e-05 *** # 3 - 1 == 0 0.27342 0.05478 4.991 &lt; 1e-05 *** # 3 - 2 == 0 0.01282 0.07199 0.178 0.998 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) .oma &lt;- par(oma = c(0, 5.1, 0, 0)); plot(anovaComp.); par(.oma); rm(.oma) Ici, comme nous testons tous les contrastes, nous pouvons dire que la population des mères qui ont fumé pendant la grossesse smoke == 1 donne des bébés significativement moins gros au seuil alpha de 5%, et ce, en comparaison de tous les autres niveaux (mère n’ayant jamais fumé, ou ayant fumé mais arrêté avant la grossesse, que ce soit longtemps avant ou juste avant). Il semble évident maintenant qu’il n’est pas utile de préciser si la mère a fumé ou non avant sa grossesse. L’élément déterminant est uniquement le fait de fumer pendant la grossesse ou non. Nous pouvons le montrer également en utilisant des contrastes de Helmert, à condition de recoder smoke avec des niveaux de “gravité” croissants (“0” = n’a jamais fumé, “1” = a arrêté il y a longtemps, “2” = a arrêté juste avant la grossesse et finalement, “1” = a continué à fumé à la grossesse). Il faut donc intervertir les cas “1” et “3”. Nous pouvons utiliser recode() pour cela, mais attention, nous avons ici une variable factor, donc, ce ne sont pas des nombres mais des chaines de caractères (à placer entre guillements). Une fois le recodage réalisé, il faut aussi retrier les niveaux en appelant factor(..., levels = c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;)) sinon l’ancien ordre est conservé. Babies %&gt;.% mutate(., smoke = recode(smoke, &quot;0&quot; = &quot;0&quot;, &quot;1&quot; = &quot;3&quot;, &quot;2&quot; = &quot;2&quot;, &quot;3&quot; = &quot;1&quot;)) %&gt;.% mutate(., smoke = factor(smoke, levels = c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) -&gt; Babies2 Si cela semble trop compliqué, vous pouvez aussi utiliser l’addins de réencodage dans R (QUESTIONR -&gt; Levels Recoding or Levels Ordering). A présent que l’encodage de smoke est corrigé dans Babies2, nous pouvons modéliser à nouveau, mais cette fois-ci avec des contrastes de Helmert (notez la façon particulière de spécifier des contrastes différents de la valeur pas défaut pour une variable factor) : Babies_lm3 &lt;- lm(data = Babies2, wt ~ smoke + wt1, contrasts = list(smoke = &quot;contr.helmert&quot;)) summary(Babies_lm3) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies2, contrasts = list(smoke = &quot;contr.helmert&quot;)) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.985106 0.091695 32.555 &lt; 2e-16 *** # smoke1 0.017743 0.027034 0.656 0.512 # smoke2 0.001641 0.019599 0.084 0.933 # smoke3 -0.064330 0.008540 -7.533 9.82e-14 *** # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 anova(Babies_lm3) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.887 1.285e-15 *** # wt1 1 6.162 6.1621 24.657 7.853e-07 *** # Residuals 1185 296.150 0.2499 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Ici les valeurs estimées pour smoke1-3 sont à interpréter en fonction des contrastes utilisés, soit : contr.helmert(4) # [,1] [,2] [,3] # 1 -1 -1 -1 # 2 1 -1 -1 # 3 0 2 -1 # 4 0 0 3 smoke1 est le décalage de l’ordonnée à l’origine entre la modèle moyen établi avec les données smoke == 0 et smoke == 1 et celui pour smoke == 1 (non significatif au seuil alpha de 5%), smoke2 est le décalage de l’ordonnée à l’origine pour smoke == 2 par rapport au modèle ajusté sur smoke == 0, smoke == 1 et smoke == 2 avec des pondérations respectives de -1, -1, et 2 (non significatif au seuil alpha de 5%), smoke3 est le décalage de l’ordonnée à l’origine par rapport au modèle ajusté sur l’ensemble des autres observations, donc, avec smoke valant 0, 1, ou 2, et des pondérations respectives comme dans la dernière colonne de la matrice de contraste.. Donc, ce dernier contraste est celui qui nous intéresse car il compare les cas où la mère n’a pas fumé pendant la grossesse avec le cas smoke == 3 où la mère a fumé pendant la grossesse, et il est significatif au seuil alpha de 5%. L’interprétation des vlauers estimées est plus complexe ici. Comparer ce résultat avec le modèle ajusté avec les contrastes traitement par défaut avec smoke réencodé : summary(lm(data = Babies2, wt ~ smoke + wt1)) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies2) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.030052 0.092861 32.630 &lt; 2e-16 *** # smoke1 0.035486 0.054068 0.656 0.512 # smoke2 0.022666 0.056508 0.401 0.688 # smoke3 -0.237938 0.031816 -7.478 1.46e-13 *** # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 Les conclusions sont les mêmes, mais la valeurs estimées pour smoke1, smoke2 et smoke3 diffèrent. Par exemple, dans ce dernier cas, smoke1 est double de la valeur avec les contrastes Helmert, ce qui est logique puisque la référence est ici la droite ajustée pour la sous-population smoke == 0 là où dans le modèle avec les contrastes de Helmert, le décalage est mesuré par rapport au modèle moyen (donc à “mi-chemin” entre les deux droites pour smoke == 0 et smoke == 1). Naturellement, nous pouvons aussi considérer la variable smoke réencodée dans Babies2 comme une variable facteur ordonné (ordered). Dans ce cas, c’est les contrastes polynomiaux qui sont utilisés : Babies2 %&gt;.% mutate(., smoke = as.ordered(smoke)) -&gt; Babies3 summary(lm(data = Babies3, wt ~ smoke + wt1)) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies3) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.985106 0.091695 32.555 &lt; 2e-16 *** # smoke.L -0.162480 0.026780 -6.067 1.75e-09 *** # smoke.Q -0.148045 0.039294 -3.768 0.000173 *** # smoke.C -0.044605 0.048790 -0.914 0.360787 # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 Notez comment R est capable d’utiliser automatiquement les contrasts adéquats (polynomiaux) lorsque la variable facteur smoke est encodée en ordered. Nous voyons ici que des contrastes tenant compte d’une variation le long des successions croissante de niveaux de “gravité” de la variable smoke sont maintenant calculés. La ligne smoke.L du tableau Coefficients indique une variation linéaire (significative au seuil alpha de 5%), smoke.Q est une variation quadratique (également significative) et enfin smoke.C est une variation cubique. Voyez la présentation des matrices de contrastes plus haut pour bien comprendre ce qui est calculé ici. Au final, l’élément important relatif à la variable smoke est en définitive le fait de fumer pendant la grossesse ou non, pas l’histoire de la mère avant sa grossesse en matière de tabocologie ! En modélisation, nous avons toujours intérêt à choisir le modèle le plus simple. Donc ici, cela vaut le coup de simplifier smoke à une variable à deux niveaux smoke_preg qui indique uniquement si la mère fume ou non pendant la grossesse. Ensuite, nous ajustons à nouveau un modèle plus simple avec cette nouvelle variable. Babies %&gt;.% mutate(., smoke_preg = recode(smoke, &quot;0&quot; = &quot;0&quot;, &quot;1&quot; = &quot;1&quot;, &quot;2&quot; = &quot;0&quot;, &quot;3&quot; = &quot;0&quot;)) %&gt;.% mutate(., smoke_preg = factor(smoke_preg, levels = c(&quot;0&quot;, &quot;1&quot;))) -&gt; Babies Babies_lm4 &lt;- lm(data = Babies, wt ~ smoke_preg + wt1) summary(Babies_lm4) # # Call: # lm(formula = wt ~ smoke_preg + wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.96243 -0.30708 0.01208 0.31051 1.48662 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.037508 0.091896 33.054 &lt; 2e-16 *** # smoke_preg1 -0.245797 0.029747 -8.263 3.76e-16 *** # wt1 0.007624 0.001531 4.981 7.25e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4996 on 1187 degrees of freedom # Multiple R-squared: 0.07692, Adjusted R-squared: 0.07537 # F-statistic: 49.46 on 2 and 1187 DF, p-value: &lt; 2.2e-16 anova(Babies_lm4) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke_preg 1 18.497 18.4971 74.106 &lt; 2.2e-16 *** # wt1 1 6.193 6.1934 24.813 7.253e-07 *** # Residuals 1187 296.281 0.2496 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A présent, tous les termes de notre modèle sont significatifs au seuil alpha de 5%. La ligne smoke_preg1 est le décalage de l’ordonnée à l’origine du poids des bébés issus de mères fumant pendant la grossesse. Il donne donc directement la perte moyenne de poids du à la tabacologie. La représentation graphique de ce dernier modèle est la suivante : cols &lt;- iterators::iter(scales::hue_pal()(2)) # Get colors for lines chart(data = Babies, wt ~ wt1) + geom_point(aes(col = smoke_preg)) + lapply(c(0, -0.246), function(offset) geom_smooth(method = lm, formula = y + offset ~ x, col = iterators::nextElem(cols))) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Enfin, n’oublions pas que notre modèle n’est valide que si les conditions d’application sont rencontrées, en particulier, une distribution normale des résidus et une homoscédasticité (même variance pour les résidus). Nous vérifions cela visuellement toujours avec les graphiques d’analyse des résidus. En voici les plus importants (pensez à utiliser les snippets pour récupérer le template du code) : #plot(Babies_lm4, which = 1) Babies_lm4 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(Babies_lm4, which = 2) Babies_lm4 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(Babies_lm4, which = 3) Babies_lm4 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(Babies_lm4, which = 4) Babies_lm4 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Ici le comportement des résidus est sain. Des petits écarts de la normalité sur le graphique quantile-quantile s’observent peut-être, mais ce n’est pas dramatique et le modèle linéaire est rabuste à ce genre de petis changements d’autant plus qu’ils apparaissent relativement symétriques en haut et et en bas de la distribution. En conclusion de cette analyse, nous pouvons dire que la masse du bébé dépend de la masse de la mère, mais assez faiblement (seulement 7,7% de la variance totale expliquée). Par contre, nous pouvons aussi dire que le fait de fumer pendant la grossesse a un effet significatif sur la réduction de la masse du bébé à la naissance (en moyenne cette réduction est de 0,246kg pour une masse moyenne à la naissance de 3,038kg, soit une réduction de 0,246 / 3,034 * 100 = 8%). Voilà, nous venons d’analyser et d’interpréter notre premier modèle linéaire sous forme d’une ANCOVA. A vous de jouer ! Après cette longue lecture avec énormément de nouvelles matières, nous vous proposons les exercices suivants : Répondez aux questions d’un learnr afin de vérifier vos acquis. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;03a_mod_lin&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Poursuivez l’analyse des données sur la biométrie des oursins en y intégrant vos nouvelles notions sur le modèle linéaire Reprenez votre travail sur la biométrie des oursins et appliquer les nouvelles notions vues Références "],
["modele-lineaire-generalise.html", "3.5 Modèle linéaire généralisé", " 3.5 Modèle linéaire généralisé Le modèle linéaire nous a permis de combiner différent types de variables indépendantes ou explicatives dans un même modèle. Cependant la variable dépendante ou réponse à la gauche de l’équation doit absolument être numérique et une distribution normale est exigée pour la composante statistique du modèle exprimée dans les résidus \\(\\epsilon\\). Donc, si nous voulons modéliser une variable dépendante qui ne répond pas à ces caractéristiques, nous sommes dans l’impasse avec la fonction lm(). Dans certains cas, une transformation des données peut résoudre le problème. Par exemple, prendre le logarithme d’une variable qui a une distribution log-normale. Dans d’autres cas, il semble qu’il n’y ait pas de solution… C’est ici que la modèle linéaire généralisé vient nous sauver la mise. Le modèle linéaire généralisé se représente comme suit : \\[ f(y) = \\beta_1 + \\beta_2 I_2 + \\beta_3 I_3 + ... + \\beta_k I_k + \\beta_l x_1 + \\beta_m x_2 + ... + \\epsilon \\] La différence par rapport au modèle linéaire, c’est que notre variable dépendante \\(y\\) est transformée à l’aide d’une fonction \\(f(y)\\) que l’on appelle fonction de lien. Cette fonction de lien est choisie soigneusement pour transformer une variable qui a une distribution non-normale vers une distribution normale ou quasi-normale. Du coup, il ne faut rien changer à la droite du signe égal par rapport au modèle linéaire, et les outils existants peuvent être réemployés. Toute la difficulté ici tient donc à la définition des fonctions de liens pertinentes par rapport à la distribution de \\(y\\). Le tableau suivant reprend les principales situations prises en compte par la fonction glm() dans R qui calcule le modèle linéaire généralisé. Distribution de Y Fonction de lien Code R Gaussienne (Normale) identité (pas de transfo.) glm(..., family = gaussian(link = &quot;identity&quot;)) Log-Normale log glm(..., family = gaussian(link = &quot;log&quot;)) Binomiale logit glm(..., family = binomial(link = logit)) Binomiale probit (alternative) glm(..., family = binomial(link = probit)) Poisson log glm(..., family = poisson(link = log)) Il en existe bien d’autres. Voyez l’aide de ?family pour plus de détails. Par exemple, pour une variable réponse binaire acceptant seulement deux valeurs possibles et ayant une distribution binomiale, avec une réponse de type logistique (une variation croissante d’une ou plusieurs variables indépendantes fait passer la proportion des individus appartenant au second état selon une courbe logistique en S), une fonction de type logit est à utiliser. \\[ y = 1/(1 + e^{- \\beta x}) \\] La transformation logit calcule alors : \\(\\ln(y / (1 - y)) = \\beta x\\). Les situations correspondant à ce cas de figure concernent par exemple des variables de type (vivant versus mort) par rapport à une situation potentiellement léthale, ou alors, le développement d’une maladie lors d’une épidémie (sain versus malade). 3.5.1 Exemple Continuons à analyser nos données concernant les bébés à la naissance. Un bébé prématuré est un bébé qui nait avant 37 semaines de grossesse. Dans notre jeu de données Babies, nous pouvons déterminer si un enfant est prématuré ou non (variable binaire) à partir de la variable gestation(en jours). Transformons nos données pour obtenir les variables d’intérêt. SciViews::R babies &lt;- read(&quot;babies&quot;, package = &quot;UsingR&quot;) babies %&gt;.% select(., gestation, smoke, wt1, ht, race, age) %&gt;.% # Eliminer les valeurs manquantes filter(., gestation &lt; 999, smoke &lt; 9, wt1 &lt; 999, ht &lt; 999, race &lt; 99, age &lt; 99) %&gt;.% # Transformer wt1 en kg et ht en cm mutate(., wt1 = wt1 * 0.4536) %&gt;.% mutate(., ht = ht / 39.37) %&gt;.% # Transformer smoke en variable facteur mutate(., smoke = as.factor(smoke)) %&gt;.% # Idem pour race mutate(., race = as.factor(race)) %&gt;.% # Déterminer si un bébé est prématuré ou non (en variable facteur) mutate(., premat = as.factor(as.numeric(gestation &lt; 7*37))) %&gt;.% # Calculer le BMI comme meilleur index d&#39;embonpoint des mères que leur masse mutate(., bmi = wt1 / ht^2) -&gt; Babies_prem Comment se répartissent les enfants entre prématurés et nés à terme ? table(Babies_prem$premat) # # 0 1 # 1080 96 Nous avons un nombre relativement faible de prématurés dans l’ensemble. C’était à prévoir. Attention à un plan très mal balancé ici : c’est défavorable à une bonne analyse, mais pas rédhibitoire. Décrivons ces données. Babies_table &lt;- table(Babies_prem$premat, Babies_prem$smoke) knitr::kable(addmargins(Babies_table)) 0 1 2 3 Sum 0 486 420 83 91 1080 1 39 40 9 8 96 Sum 525 460 92 99 1176 Ce tableau de contingence ne nous donne pas encore l’idée de la répartition de prématurés en fonction de statut de fumeuse de la mère, mais le graphique suivant nous le montre. chart(data = Babies_prem, ~smoke %fill=% premat) + geom_bar(position = &quot;fill&quot;) Il ne semble pas y avoir un effet flagrant, même si le niveau smoke == 2 semble contenir une plus forte proportion de prématurés. Qu’en est-il en fonction de l’éthnicité (voir help(babies, packahge = &quot;UsingR&quot;) pour le détail sur les variétés éthniques considérées) de la mère (variable race) ? Babies_table &lt;- table(Babies_prem$premat, Babies_prem$race) knitr::kable(addmargins(Babies_table)) 0 1 2 3 4 5 6 7 8 9 10 Sum 0 491 42 22 58 50 124 31 192 34 24 12 1080 1 24 2 5 1 6 8 3 41 5 1 0 96 Sum 515 44 27 59 56 132 34 233 39 25 12 1176 chart(data = Babies_prem, ~race %fill=% premat) + geom_bar(position = &quot;fill&quot;) Ici, nous voyons déjà un effet semble-t-il plus marqué. Qu’en est-il du BMI ? chart(data = Babies_prem, bmi ~ premat) + geom_boxplot() + ylab(&quot;BMI de la mère&quot;) Sur ce graphique, il ne semble pas y avoir d’influence du BMI sur le fait d’avoir un enfant prématuré ou non. Enfin, l’âge de la mère influence-t-il également ? chart(data = Babies_prem, age ~ premat) + geom_boxplot() + ylab(&quot;Age de la mère (an)&quot;) Il ne sembnle pas y avoir un effet flagrant. Voyons ce que donne le modèle (nous ne considérons pas les interactions possibles ici, mais cela doit être fait dans le cas de plusieurs effets significatifs au moins). Avec 4 variables explicatives, le modèle est déjà très complexe sans interactions. Nous serions trop ambitieux de vouloir ici ajuster un modèle complet ! # Modèle linéaire généralisé avec fonction de lien de type logit Babies_glm &lt;- glm(data = Babies_prem, premat ~ smoke + race + bmi + age, family = binomial(link = logit)) summary(Babies_glm) # # Call: # glm(formula = premat ~ smoke + race + bmi + age, family = binomial(link = logit), # data = Babies_prem) # # Deviance Residuals: # Min 1Q Median 3Q Max # -0.6875 -0.4559 -0.3228 -0.2857 2.8199 # # Coefficients: # Estimate Std. Error z value Pr(&gt;|z|) # (Intercept) -3.349082 0.860945 -3.890 0.00010 *** # smoke1 0.247161 0.244442 1.011 0.31196 # smoke2 0.247474 0.401007 0.617 0.53715 # smoke3 0.245883 0.417970 0.588 0.55634 # race1 -0.037501 0.754621 -0.050 0.96037 # race2 1.510254 0.539895 2.797 0.00515 ** # race3 -1.017780 1.030541 -0.988 0.32334 # race4 0.891191 0.481411 1.851 0.06414 . # race5 0.303990 0.421363 0.721 0.47064 # race6 0.750868 0.644376 1.165 0.24391 # race7 1.495166 0.280313 5.334 9.61e-08 *** # race8 1.155898 0.531592 2.174 0.02967 * # race9 -0.123997 1.043118 -0.119 0.90538 # race10 -13.513140 691.812473 -0.020 0.98442 # bmi -0.001689 0.032621 -0.052 0.95871 # age 0.007806 0.019119 0.408 0.68307 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # (Dispersion parameter for binomial family taken to be 1) # # Null deviance: 665.00 on 1175 degrees of freedom # Residual deviance: 618.84 on 1160 degrees of freedom # AIC: 650.84 # # Number of Fisher Scoring iterations: 15 Nous voyons que le résumé de l’objet glm est très similaire à celui d’un objet lm, notamment avec un tableau des Coefficients identique et qui s’interprète de la même manière. Ici, nous pouvons confirmer que ni le fait de fumer, ni l’âge, ni le BMI de la mère n’a d’incidence sur les bébés prématurés au seuil alpha de 5%. En revanche, certaines éthnies sont significativement plus susceptibles d’accoucher avant terme. Cela suggère soit un facteur génétique, soit un facteur environnmental/culturel lié à ces éthnies. Naturellement, il faudrait ici simplifier le modèle qui se ramène en fin de compte à l’équivalent d’une ANOVA à un facteur, mais en version glm() une fois que les variables non significatives sont éliminées. De même, on pourrait légitimement se demander si la variable premat ne pourrait pas aussi être modélisée avec une autre fonction de lien en considérant une distribution de Poisson par exemple. A vous de voir… A vous de jouer ! Réalisez un rapport scientifique sur la maturation d’ovocytes, en définissant un modèle linéaire généralisé le plus pertinent pour ces données. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/mXAIu4Ir Lisez le README afin de prendre connaissance de l’exercice Réalisez un rapport scientifique sur la biométrie humaine Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/hS069etL Lisez le README afin de prendre connaissance de l’exercice "],
["reg-non-lin.html", "Module 4 Régression non linéaire", " Module 4 Régression non linéaire Objectifs Comprendre comment ajuster une courbe dans un nuage de points à l’aide de la régression non linéaire par les moindres carrés Apprendre à réaliser une régression non linéaire dans R, éventuellement en utilisant des modèles “self-start” Comparer les modèles à l’aide du coefficient d’Akaike Connaitre quelques unes des courbes mathématiques les plus utilisées en biologie Prérequis Les modules 1 &amp; 2 du présent cours concernant la régression linéaire sont une entrée en matière indispensable puisque la régression va être abordée ici comme une extension de ce qui a déjà été vu. "],
["rendement-photosynthetique.html", "4.1 Rendement photosynthétique", " 4.1 Rendement photosynthétique Afin d’avoir un premier aperçu de ce qu’est une régression non linéaire par les moindres carrés et comment on la calcule dans R, nous allons résoudre un exemple concret. La posidonie (Posidonia oceanica (L.) Delile (1813)) est une plante à fleur marine qui forme des herbiers denses en mer Méditerranée. Ses feuilles sont particulièrement adaptées à l’utilisation de la lumière qui règne à quelques mètres en dessous de la surface où elle prospère en herbiers denses. (un mémoire portant sur l’étude du diméthylsulfoniopropionate et du diméthylsulfoxyde chez Posidonia oceanica (L.) Delile (1813) a été réalisé au sein du service d’Écologie numérique des Milieux aquatiques). Herbier de posidonies Pour étudier le rendement de sa photosynthèse, c’est-à-dire la part du rayonnement lumineux reçu qui est effectivement utilisée pour initier la chaîne de transport d’électrons au sein de son photosystème II, nous pouvons utiliser un appareil spécialisé : le diving PAM. Diving PAM Cet appareil est capable de déterminer le taux de transfert des électrons (ETR en µmol électrons/m2/s) par l’analyse de la fluorescence réémise par la plante lorsque ses photosites sont excités par une lumière monochromatique pulsée (Walz 2018). Une façon de déterminer la réponse de la plante en rendement photosynthétique en fonction de l’intensité de la lumière reçue est de mesurer successivement l’ETR pour différentes intensités de lumière. En anglais cela s’appelle la “Rapid Light Curve” ou RLC en abbrégé. Comme toutes les longueurs d’ondes lumineuses ne sont pas utilisables par la chlorophylle, l’intensité lumineuse est exprimé dans une unité particulière, le “PAR” ou “Photosynthetically Active Radiation” en µmol photons/m2/s. Une RLC represente donc la variation de l’ERT en fonction des PAR8. Une RLC typique commence par une relation quasi-linéaire aux faibles intensités, pour s’infléchir et atteindre un plateau de rendement maximum. Au delà, si l’intensité lumineuse augmente encore, des phénomènes de photoinhibition apparaissent et le rendement diminue dans une troisième phase. Voici une RLC mesurée à l’aide du diving PAM sur une feuille de P. oceanica. rlc &lt;- tribble( ~etr, ~par, 0.0, 0, 0.5, 2, 3.2, 11, 5.7, 27, 7.4, 50, 8.4, 84, 8.9, 170, 8.4, 265, 7.8, 399 ) rlc &lt;- labelise(rlc, self = FALSE, label = list(etr = &quot;ETR&quot;, par = &quot;PAR&quot;), units = list(etr = &quot;µmol électrons/m^2/s&quot;, par = &quot;µmol photons/m^2/s&quot;) ) chart(data = rlc, etr ~ par) + geom_point() Les trois phases successives sont bien visibles ici (linéaire pour des PAR de 0 à 25, plateau à des PAR de 150-200 et photoinhibition à partir de 200 PAR). Naturellement, une régression linéaire dans ces données n’a pas de sens. Une régression polynomiale d’ordre trois donnerait ceci (code issu du snippet correspondant) : rlc_lm &lt;- lm(data = rlc, etr ~ par + I(par^2) + I(par^3)) rlc_lm %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3)))(.) La régression polynomiale tente maladroitement de s’ajuster dans les données mais est incapable de retranscrire les trois phases correctement. En particulier, la troisième est incorrecte puisque le modèle semble indiquer une reprise du rendement aux intensités les plus élevées. Une représentation incorrecte est à craindre lorsque le modèle mathématique utilisé ne représente pas les différentes caractéristiques du phénomène étudié. L’utilisation d’un modèle adéquat est possible ici seulement par régression non linéaire. En effet, aucune transformation monotone croissante ou décroissante ne peut linéariser ce type de données puisque la courbe monte d’abord pour s’infléchir ensuite. Quand passer à la régression non linéaire ? Nous pouvons être amenés à utiliser une régression non linéaire pour l’une de ces deux raisons, voire les deux en même temps : Lorsque le nuage de points est curvilinéaire, évidemment, mais après avoir tenté de le linéariser (et de résoudre un problème éventuel d’hétéroscédasticité ou de non-normalité des résidus) par transformation sans succès, En fonction de nos connaissances a priori du phénomène. Tout phénomène issu d’un mécanisme dont nous connaissons le mode de fonctionnement menant à une équation mathématique non linéaire. Cela se rencontre fréquemment en physique, en chimie, et même en biologie (courbes de croissance, effet de modifications environmentales, etc.) Les spécialistes de la photosynthèse ont mis au point différents modèles pour représenter les RLC. (Platt, Gallegos, and Harrison 1980) ont proposé une formulation mathématique des phénomènes mis en jeu ici. Leur équation est la suivante : \\[ETR = ETR_{max} \\cdot (1 - e^{-PAR \\cdot \\alpha/ETR_{max}}) \\cdot e^{-PAR \\cdot \\beta/ETR_{max}}\\] avec \\(PAR\\) la variable indépendante, \\(ETR\\), la variable dépendante, et \\(ETR_{max}\\), \\(\\alpha\\) et \\(\\beta\\), les trois paramètres du modèle. \\(ETR_{max}\\) est le rendement maximum possible, \\(\\alpha\\) est la pente de la partie initiale linéaire avant infléchissement vers le maximum et \\(\\beta\\) est le coefficient de photoinhibition. En matière de régression non linéaire, il est tout aussi important de bien comprendre les propriétés mathématiques de la fonction utilisée que de faire un choix judicieux du modèle. En particulier, il faut s’attacher à bien comprendre la signification (biologique) des paramètres du modèle. Non seulement, cela aide à en définir des valeurs intiales plausibles, mais c’est aussi indispensable pour pouvoir ensuite bien interpréter les résultats obtenus. Nous pouvons facilement créer une fonction dans R qui représente ce modèle : pgh_model &lt;- function(x, etr_max, alpha, beta) etr_max * (1 - exp(-x * alpha/etr_max)) * (exp(-x * beta/etr_max)) Le premier argument de la fonction doit être la variable indépendante (notée de manière générique x, mais n’importe quel nom fait l’affaire ici) et les autres arguments correspondent aux paramètres du modèle, donc etr_max, alpha et beta. Ce modèle peut être ajusté dans R à l’aide de la fonction nls() pour “Nonlinear Least Squares” (regression). Par contre, nous devons fournir une information supplémentaire (l’explication sera détaillée plus loin) : des valeurs approximatives de départ pour les paramètres. Nous voyons sur le graphique que etr_max = 9 est une estimation plausible, mais il est difficile de déterminer alpha et beta rien qu’en regardant le graphique. Nous allons fixer alpha = 1 (rendement max), et partir d’un modèle sans photoinhibition en utilisant beta = 0. Voici comment la régression non linéaire par les moindre carrés avec notre fonction pgh_model peut être calculée : rlc_nls &lt;- nls(data = rlc, etr ~ pgh_model(par, etr_max, alpha, beta), start = list(etr_max = 9, alpha = 1, beta = 0)) summary(rlc_nls) # # Formula: etr ~ pgh_model(par, etr_max, alpha, beta) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # etr_max 9.351064 0.261969 35.695 3.22e-08 *** # alpha 0.327385 0.013416 24.402 3.11e-07 *** # beta 0.003981 0.001084 3.673 0.0104 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1729 on 6 degrees of freedom # # Number of iterations to convergence: 7 # Achieved convergence tolerance: 3.619e-06 La dernière ligne, “achieved convergence” indique que le modèle a pu être calculé. Nous avons un tableau des paramètres qui ressemble très fort à celui de la régression linéaire, y compris les tests t de Student sur chacun des paramètres. Nous obtenons etr_max = 9.4, alpha = 0.33 et beta = 0.0040, mais d’après le test de Student ce dernier paramètre n’est pas significativement différent de zéro9. Pour l’instant, nous allons conserver ce modèle tel quel. Notre modèle paramétré donne donc : \\[ETR = 9.4 \\cdot (1 - e^{-PAR \\cdot 0.33/9.3}) \\cdot e^{-PAR \\cdot 0.0040/9.3}\\] Voyons ce que cela donne sur le graphique. Nous utiliserons pour ce faire une petite astuce qui consiste à transformer l’objet nls obtenu en une fonction utilisable par stat_function() pour le graphique ggplot2 réalisé à l’aide de chart()10. as.function.nls &lt;- function(x, ...) { nls_model &lt;- x name_x &lt;- names(nls_model$dataClasses) stopifnot(length(name_x) == 1) function(x) predict(nls_model, newdata = structure(list(x), names = name_x)) } Maintenant, nous allons pouvoir ajouter la courbe correspondant à notre modèle comme ceci : chart(data = rlc, etr ~ par) + geom_point() + stat_function(fun = as.function(rlc_nls)) Ce modèle représente bien mieux le phénomène étudié, et il s’ajuste d’ailleurs beaucoup mieux également dans les données que notre polynome. Une comparaison sur base du critère d’Akaïke est également en faveur de ce dernier modèle non linéaire (pour rappel, plus la valeur est faible, mieux c’est) : AIC(rlc_lm, rlc_nls) # df AIC # rlc_lm 5 31.834047 # rlc_nls 4 -1.699078 Super ! Nous venons de réaliser ensemble notre première régression non linéaire par les moindres carrés. Étudions un petit peu plus dans le détail cette technique dans la section suivante. En effet, il est utile de connaitre et comprendre les différents pièges qui peuvent se présenter à nous pour les éviter. Références "],
["principe.html", "4.2 Principe", " 4.2 Principe La régression non linéaire consiste à modéliser la variation d’une variable (dite variable réponse ou dépendante) par rapport à la variation d’une ou plusieurs autres variables (dites explicatives ou indépendantes). Le modèle utilisé pour représenter cette relation est une fonction mathématique de forme quelconque. Ceci constitue une généralisation de la régression linéaire où la fonction mathématique était nécessairement une droite (\\(y = a x + b\\) dans le cas de la régression linéaire simple). La fonction est, dans la technique la plus courante, ajustée en minimisant la somme des carrés des résidus (écart entre les observations \\(y_i\\) et les valeurs prédites par la droite, notées \\(\\hat{y_i}\\)). Lorsque le nuage de point ne s’étire pas le long d’une droite, nous pouvons tenter de transformer les données afin de les linéariser. Malheureusement, il existe de nombreux cas où la relation n’est pas linéarisable et la régression non linéaire est alors notre meilleur choix. Il existe, en réalité, une autre raison pour laquelle nous pourrions être amenés à ne pas transformer les données pour les linéariser. Il s’agit du cas où les résidus ont une distribution correcte avec les données non transformées (distribution normale, et variance homogène -homoscédasticité-) lorsqu’on utilise un modèle non linéaire. Dans ce cas précis, une transformation pour linéariser les données devrait permettre d’utiliser une régression linéaire. Mais ce faisant, on rend alors les résidus non normaux et/ou on perd la propriété d’homoscédasticité, ce qui constitue une violation des conditions d’application de la régression par les moindres carrés que nous utilisons ici. Ainsi, dans ce cas-là, il vaut alors mieux ne pas transformer et utiliser plutôt une régression non linéaire à la place d’une régression linéaire pourtant plus simple d’emploi. 4.2.1 Fonction objective Nous appelons “fonction objective” la fonction qui quantifie la qualité de l’ajustement de sorte que plus le nombre renvoyé par cette fonction est petit, meilleur est l’ajustement. Cette fonction objective peut être définie librement, mais dans de nombreux cas, il s’agit du même critère que pour la régression linéaire par les moindres carrés, à savoir (considérant que la fonction \\(f\\) que nous souhaitons ajuster a \\(k\\) paramètres notés \\(p_1\\), \\(p_2\\), …, \\(p_k\\)) : \\[f_{obj}(p_{1},p_{2},...,p_{k})=\\sum_{i=1}^{n}(y_{i}-f(x_{i,}p_{1},p_{2},...,p_{k}))^{2}=\\sum_{i}(y_{i}-\\hat{{y_{i}}})^{2}\\] 4.2.2 Calcul itératif L’ajustement de notre courbe selon le modèle \\(y = f(x, p_1, p_2, ... p_k) + \\epsilon\\) avec les résidus \\(\\epsilon \\approx N(0, \\sigma)\\) peut se faire de manière itérative, en testant différentes valeurs des paramètres de la fonction, et en retenant au final la combinaison qui minimise le plus la fonction objective. Par exemple, si notre courbe à ajuster est : \\(y = a x^b\\), nous devrons estimer conjointement la valeur des deux paramètres \\(a\\) et \\(b\\) de la courbe. Bien qu’il s’agisse effectivement de la technique employée pour ajuster une courbe dans un nuage de point, nls() utilise ici des algorithmes d’optimisation efficaces pour trouver la solution plus rapidement. Une recherche en aveugle serait très peu efficace évidemment. La description détaillée et les développements mathématiques de ces algorithmes d’optimisation sortent du cadre de ce cours. Nous renvoyons le lecteur intéressé à l’annexe C de (Sen and Srivastava 1990). Les algorithmes utilisés dans la fonction nls() sont : Gauss-Newton, (par défaut), un algorithme utilisant la différentiation de la courbe et une expansion en série de Taylor pour approximer cette courbe par une série de termes additifs dont la solution peut être trouvée par régression linéaire multiple. plinear de Golub-Pereyra, bien que peu répandu, est également implémenté. Il est utile en ce sens qu’il sépare les paramètres en deux sous-groupes : ceux qui sont linéaires dans la fonction (coefficients multiplicateurs de termes additifs) et ceux qui ne le sont pas. La recherche itérative ne se fait que sur les seconds. Les premiers étant estimés par régression linéaire. Ainsi, lorsque la fonction ne comporte que peu de paramètres non linéaires, l’algorithme converge beaucoup plus rapidement. Port est également disponible. Il a la particularité, contrairement aux deux précédents, de permettre de définir des limites supérieures et inférieures acceptables pour chaque paramètres. Cela limite la recherche dans un domaine de validité, lorsque celui-ci peut être défini. Reprenons le calcul de notre RLC pour P. oceanica. L’argument trace = TRUE peut être ajouté à l’appel de nls() pour visionner les différentes étapes du calcul itératif. Voici ce que cela donne : rlc_nls &lt;- nls(data = rlc, etr ~ pgh_model(par, etr_max, alpha, beta), start = list(etr_max = 9, alpha = 1, beta = 0), trace = TRUE) # 24.34024 : 9 1 0 # 4.236559 : 8.4785637209 0.5324097242 -0.0004037391 # 1.141967 : 8.772552037 0.289806296 0.001855493 # 0.1807006 : 9.361592818 0.325563316 0.003954183 # 0.1793698 : 9.353620948 0.327230129 0.003990887 # 0.1793659 : 9.351296953 0.327369993 0.003982181 # 0.1793658 : 9.35108479 0.32738331 0.00398142 # 0.1793658 : 9.351064462 0.327384565 0.003981347 A gauche, nous avons la valeur de la fonction objective \\(f_{obj}\\), et à droite des deux points, la valeur actuelle des trois paramètres dans l’ordre etr_max, alpha et beta. La première ligne indique l’étape 0. \\(f_{obj}\\) vaut 24,3 avec les valeurs par défaut. Dès l’étape suivante (itération #1), cette valeur est divisée par 6 et vaut 4,2. A ce stade, etr_max = 8,5, alpha = 0,29 et beta = 0,0019. Les étapes #2 et #3 font encore significativement diminuer \\(f_{obj}\\), et puis nous grapillons des décimales. Voici une animation de ce processus : La convergence est considérée comme obtenue lorsque la différence de valeur de \\(f_{obj}\\) d’une étape à l’autre tombe en dessous d’un seuil de tolérance que nous nous sommes fixés. C’est l’argument control = de nls() qui en reprend les détails, voir ?nls.control. Les options sont : maxiter = le nombre maximum d’itérations permises. Pour éviter de calculer à l’infini, nous arrêtons à cette valeur et décrétons que le calcul n’a pas convergé. La valeur pas défaut est de 50 itérations. tol = le niveau de tolérance pour décider que l’on a convergé. C’est lui notre seuil de tolérance ci-dessus. Par défaut, il vaut 1e-05, soit 0,00001. C’est une valeur relative. Cela siginifie que la variation de la fonction objective ne doit pas être plus grande que cela en valeur relative, soit \\(|\\frac{fobj_i - fobj_{i + 1}}{fobj_i}| &lt; tol\\). … d’autres arguments moins importants ici. Comme nous pouvons le constater, le seuil de tolérance par défaut est très bas, de sorte que nous n’ayons besoin de le changer que très rarement. 4.2.3 Pièges et difficultés L’ajustement d’un modèle de manière itérative en utilisant un algorithme d’optimisation est une tâche délicate. Dans certains cas, la convergence est lente (il faut beaucoup d’étapes pour arriver au résultat). Dans d’autre cas, le processus s’interrompt à cause d’une singularité de la fonction à ajuster, d’une discontinuité, ou d’une fonction qui n’est pas définie sur une partie du domaine, … Nous pouvons aussi rencontrer des divisions par zéro ou des paramètres qui tendent vers l’infini lors des calculs. Dans ce cas, il faut tenter d’autres valeurs de départ, voire changer le paramétrage de la fonction. Valeurs initiales La recherche de la solution optimale nécessite de partir de valeurs de départ plausibles pour les paramètres du modèle (c’est-à-dire, un ensemble de valeurs pour les paramètres telle que la courbe initiale est proche de la solution recherchée). Définir les valeurs initiales n’est pas toujours chose aisée, et de plus, le résultat final peut dépendre fortement de ces valeurs initiales, à savoir que, si l’on choisi des valeurs initiales trop éloignées de la solution recherchée, on peut très bien se trouver enfermé dans une fausse solution (un minimum local de la fonction objective qui est moins bas que le minimum absolu que l’on recherche). Considérons un cas fictif simple où la fonction à ajuster n’aurait qu’un seul paramètre \\(p\\). Dans ce cas, nous pouvons visualiser la valeur de la fonction objective en fonction de la valeur choisie pour le paramètre \\(p\\). Admettons que nous obtenons le graphique suivant pour cette représentation : Nous observons que la fonction objective a un minimum global pour \\(p\\) valant 0,72. Par contre, un minimum local est également présent pour \\(p\\) valant -0,26. Dans ce cas, si nous prenons une valeur de départ pour \\(p\\) supérieure à 0,25, nous atterrirons dans le minimum global. Mais si nous prenons une valeur plus faible, par exemple 0 ou -0.5 comme valeur de départ, nous nous ferons piéger dans le minimum local. Dans ce cas, nls() déclarera qu’elle a convergé et que la valeur de \\(p\\) vaut -0,26. Si ce n’est toujours pas clair, imaginez que la courbe en rouge représente la forme d’un terrain sur lequel vous placer une balle à la position de départ. En fonction de l’endroit où vous la placer, la balle va glisser le long de la pente et finir par s’immobiliser dans une des deux cuvettes… mais celle de droite (minimum global) est plus profonde que celle de gauche (minimum local). Il vaut toujours mieux tester différentes combinaisons de valeurs de départ pour vérifier si la convergence se fait correctement et pour démasquer les cas de minima locaux. Complexité du modèle Lorsque la convergence a du mal à se faire, ceci est éventuellement lié à la complexité de la fonction que l’on cherche à ajuster. Les propriétés mathématique de la courbe choisie peuvent très bien faire que cette courbe ne soit pas définie pour certaines valeurs des paramètres, ou qu’elle ait un comportement spécial dans un certain domaine (par exemple, courbe tendant vers l’infini). Ces cas sont difficilement traités par la plupart des algorithmes de minimisation de la fonction objective, et des erreurs de calcul de type “division par zéro”, ou “résultat non défini” peuvent apparaître. Dans les meilleures implémentations (nls() en est une), des gardes-fous ont été ajoutés dans le code de la fonction pour éviter les erreurs dans pareils cas. Toutefois, il n’est pas possible de traiter tous les cas possibles. Ainsi, il arrive parfois que le modèle choisi ne soit pas ajustable sur les données. Au final, l’ajustement d’un modèle non linéaire par les moindres carrés est une opération beaucoup plus délicate que l’ajustement par les moindres carrés d’un modèle linéaire. Dans le cas linéaire, la solution est trouvée de manière immédiate grâce à un calcul matriciel simple. Dans le cas non linéaire, il n’existe souvent pas de solution miracle et le meilleur ajustement doit être recherché de manière itérative. Nous verrons ci-dessous que R propose, pour certains modèles appelés ‘SelfStart’ une solution élégante pour calculer automatiquement les valeurs initiales et pour converger très rapidement vers la solution recherchée, mais la plupart du temps il faut tâtonner pour ajuster sa courbe. A vous de jouer ! Réalisez un rapport scientifique sur la vitesse d’une réaction chimique, en définissant un modèle non linéaire pertinent pour ces données. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/pa48JVSl Lisez le README afin de prendre connaissance de l’exercice 4.2.4 Modèles ‘selfStart’ dans R Dans certains cas, il existe des petites astuces de calcul pour converger directement vers la solution, ou du moins, pour calculer des valeurs de départ très proches de la solution recherchée de sorte que l’algorithme de recherche pourra converger très rapidement. Par exemple, lorsqu’il est possible de linéariser le modèle en transformant les données. Dans ce cas, une bonne estimation des valeurs de départ des paramètres peut être obtenue en linéarisant la relation et en calculant les paramètres par régression linéaire. Ensuite, les paramètres obtenus sont retransformés dans leur forme initiale, et ils sont utilisés comme valeurs de départ. Par exemple, si nous décidons d’ajuster un modèle allométrique de Huxley, de type \\(y = a x^b\\)11, nous pouvons linéariser la relation en transformant les deux variables en logarithmes. En effet, \\(\\log(y) = \\log(a x^b)\\), ce qui est équivalent à une droite en développant: \\(\\log(y) = b \\log(x) + \\log(a)\\). Le paramètre \\(b\\) devient donc la pente de la droite, et \\(\\log(a)\\) devient l’ordonnée à l’origine dans le modèle linéaire transformé. Une fois la droite ajustée, on a directement la valeur de \\(b\\), et il suffit de calculer l’exponentielle de l’ordonnée à l’origine pour obtenir \\(a\\). Toutefois, comme les résidus sont différents dans la relation non transformée initiale, il ne s’agit pas de la solution recherchée mais d’une bon point de départ très proche de cette solution. Ainsi, on prendra les valeurs de \\(a\\) et de \\(b\\) ainsi calculées par la droite en double log comme point de départ, et on laissera l’algorithme de recherche du minimum terminer le travail. En fait, c’est exactement de cette façon que les modèles dits ‘selfStart’ fonctionnent dans R. Plus qu’une fonction, il s’agit en réalité d’un programme complet qui contient : la fonction elle-même, la résolution analytique de la dérivée première de la fonction en chaque point (utilisée par l’algorithme de convergence pour déterminer l’écart à apporter aux paramètres à l’itération suivante), du code optimisé pour choisir les valeurs initiales idéales (proches du minimum global pour la \\(f_{obj}\\)) automatiquement, du code pour optimiser la convergence, éventuellement. Toutes les fonctions ‘SelfStart’ ont un nom commençant par SS. nous pouvons donc les lister à l’aide de l’instruction suivante : apropos(&quot;^SS&quot;) # [1] &quot;SSasymp&quot; &quot;SSasympOff&quot; &quot;SSasympOrig&quot; &quot;SSbiexp&quot; &quot;SSD&quot; # [6] &quot;SSfol&quot; &quot;SSfpl&quot; &quot;SSgompertz&quot; &quot;SSlogis&quot; &quot;SSmicmen&quot; # [11] &quot;SSweibull&quot; Ensuite, nous recherchons l’aide relative à ces fonctions, par exemple via ?SSasymp. Références "],
["modeles-courants-en-biologie.html", "4.3 Modèles courants en biologie", " 4.3 Modèles courants en biologie Les domaines les plus courants où des modèles non linéaires sont utilisés en biologie concernent les cinétiques de réactions (chimiques, biochimiques), les courbes de type dose-réponse, les courbes de survie et les modèles de croissance. Nous verrons principalement divers modèles de croissance dans la section suivante. Certains de ces modèles, comme le modèle exponentiel, celui de Gompertz ou de Weibull sont aussi utilisés comme courbes de survie. De plus, la courbe logistique est un modèle classique de type dose-réponse. Ainsi, les différentes courbes de croissances recouvrent également une majorité des modèles utilisés dans d’autres domaines. 4.3.1 Modèle de Michaelis-Menten La corube de Michaelis-Menten est bien connue pour modéliser des cinétiques chimiques simples, enzymatiques en particulier. Son équation est : \\[V = \\frac{V_{max} \\cdot conc}{K + conc}\\] où \\(conc\\) est la concentration des réactifs au début de la réaction, c’est-à-dire, en absence de produits de cette réaction en mol/L, \\(V\\) est la vitesse de réaction en mol/min. Le modèle a deux paramètres \\(V_{max}\\) la vitesse maximale asymptotique en mol/min et \\(K\\) en mol/L correspondant à la concentration telle que la vitesse est la moitié de \\(V_{max}\\). Dans R, il existe un modèle ‘selfStart’ facile à utiliser pour ajuster une courbe de type Michaelis-Menten. Il s’agit de la fonction SSmicmen(). Voici le graphique d’un modèle Michaelis-Menten avec \\(V_{max} = 1\\) et \\(K = 0,4\\). Le trait horizontal en Vm = 1 représente la vitesse maximale possible (asymptote horizontale du modèle). Nous voyons que cette vitesse maximale n’est atteinte que très lentement ici, et il faudrait que l’axe des X s’étende beaucoup plus sur la droite pour l’observer. micmen_data &lt;- tibble( conc = seq(0, 10, by = 0.1), v = SSmicmen(conc, Vm = 1, K = 0.5) ) chart(data = micmen_data, v ~ conc) + geom_line() + xlab(&quot;Concentration [mol/L]&quot;) + ylab(&quot;Vitesse [mol/min]&quot;) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0.5, 1), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 0.4, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Vm&quot;, x = -0.4, y = 1) + annotate(&quot;text&quot;, label = &quot;Vm/2&quot;, x = -0.5, y = 0.5) + annotate(&quot;text&quot;, label = &quot;K&quot;, x = 0.5, y = 0.03) A vous de jouer ! Ajuste les pramètres du modèles afin de trouver le meilleur modèle de Michaelis-Menten dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04a_michaelis_menten&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04a_michaelis_menten&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04a_michaelis_menten Concernant les modèles utiles en chimie et biochimie, le modèle à compartiment de premier ordre permet de décrire la cinétique de transformation d’une substance au cours du temps. Voyez le modèle ?Ssfol. Il peut servir par exemple pour déterminer la cinétique d’élimination d’une substance du sang après injection. 4.3.2 Modèles de croissance Parmi les phénomènes biologiques courants qui sont essentiellement non linéaires, les modèles de croissance occupent une part importante. Il s’agit de phénomènes complexes, résultat d’un ensemble de processus, eux-même très complexes : l’anabolisme, ou élaboration de matière organique et le catabolisme qui la détruit pour la transformer en énergie, en ce qui concerne la croissance somatique individuelle. Un modèle typique de croissance individuelle est le modèle de von Bertalanffy (von Bertalanffy, 1938, 1957). Il existe un autre type de modèle de croissance : la croissance des populations. Ce type de modèle décrit l’évolution du nombre d’individus dans une population au cours du temps. Dans ce cas particulier, il s’agit également du résultat de plusieurs processus : la natalité, la mortalité et, éventuellement, les migrations. Une courbe type de modèle de croissance de population est la courbe logistique. Un autre modèle couramment utilisé est celui de Weibull pour décrire un nombre décroissant d’individus suite à une mortalité prédominante dans celle-ci. De nombreux modèles de croissance différents sont disponibles. Certains sont exclusivement des modèles de croissance individuelle, d’autres sont exclusivement des modèles de croissance de populations, mais beaucoup peuvent être utilisés indifféremment dans les deux cas. Tous ont comme particularité d’être l’une ou l’autre forme de modèle exponentiel, exprimant ainsi le fait que la croissance est fondamentalement un processus exponentiel (comprenez que l’augmentation de masse ou du nombre d’individus est proportionnelle à la masse ou au nombre préexistant à chaque incrément temporel). Nous allons décrire ci-dessous quelques un des modèles de croissance principaux. Ensuite, nous les utiliserons pour ajuster une courbe de croissance dans un jeu de donnée réel. 4.3.3 Courbe exponentielle En 1798, Thomas Malthus a décrit un modèle de croissance applicable pour décrire la croissance de la population humaine. Cependant, d’après Murray (1993), ce modèle a été suggéré en premier lieu par Euler. Quoi qu’il en soit, ce modèle n’est plus guère utilisé actuellement, mais son importance historique ne doit pas être négligée. Il s’agit, en effet, de la première modélisation mathématique d’une des caractéristiques les plus fondamentales de la croissance : son caractère exponentiel (positive ou négative). Malthus a observé que la population des Etat-Unis double tous les 25 ans. Il suggère alors que les populations humaines augmentent d’une proportion fixe \\(r\\) sur un intervalle de temps donné, lorsqu’elles ne sont pas affectées de contraintes environnementales ou sociales. Cette proportion \\(r\\) est par ailleurs indépendante de la taille initiale de la population : \\[y_{t+1} = (1+r) \\ y_t = k \\ y_t\\] Une forme continue du modèle précédent (intervalle de temps infinitésimal) donne une équation différentielle : \\[\\frac{d y(t)}{dt} = y&#39;(t) = k \\ y(t)\\] Cette équation différentielle admet la solution suivante : \\[y(t) = y_0 \\ e^{k \\ t}\\] avec \\(y_0\\), la taille initiale de la population au temps \\(t = 0\\). Ce modèle à deux paramètres est également intéressant parce qu’il montre une bonne manière de construire un modèle de croissance. Il suffit de décrire la croissance pour un accroissement infinitésimal de temps par le biais d’une équation différentielle, et ensuite de la résoudre (modélisation dynamique). Presque tous les modèles de croissance existants ont été élaborés de cette manière. Ainsi, la quasi-totalité des modèles de croissance correspondent en fait à une équation différentielle relativement simple. Dans R, nous pourrons utiliser la fonction suivante : exponent &lt;- function(x, y0, k) y0 * exp(k * x) Voici un exemple d’un modèle de croissance exponentiel avec \\(y_0 = 1,5\\) et \\(k = 0,9\\). Le paramètre \\(y_0\\) est indiqué sur le graphique. # Graphique avec y0 = 1.5 et k = 0.9 exponent_data &lt;- tibble( t = seq(0, 3, by = 0.1), y = exponent(t, y0 = 1.5, k = 0.9) ) chart(data = exponent_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + annotate(&quot;text&quot;, label = &quot;y0&quot;, x = -0.05, y = 1.5) Il existe aussi un modèle bi-exponentiel qui combine deux signaux exponentiels différents. Pour plus de détails, voyez ?SSbiexp. A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04b_exponent&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04b_exponent&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04b_exponent 4.3.4 Courbe logistique Le modèle exponentiel décrit une croissance infinie sans aucunes contraintes. Ce n’est pas une hypothèse réaliste. En pratique, la croissance est limitée par les ressources disponibles. Verhulst (1838), en travaillant aussi sur la croissance de populations, propose un modèle qui contient un terme d’auto-limitation \\([y_\\infty – y (t)] / y_\\infty\\) qui représente une quelconque limite théorique des ressources disponibles : \\[\\frac{dy(t)}{dt} = k \\ \\frac{y_\\infty - y(t)}{y_\\infty} \\ y(t) = - \\frac{k}{y_\\infty} \\ y(t)^2 + k \\ y(t)\\] Lorsque l’on résout et simplifie cette équation différentielle, on obtient : \\[y(t) = \\frac{y_\\infty}{1 + e^{-k \\ (t - t_0)}}\\] Ceci est une des formes de la courbe logistique. Cette fonction a deux asymptotes horizontales en \\(y(t) = 0\\) et \\(y(t) = y_\\infty\\) (voir schéma ci-dessous) et c’est une sigmoïde symétrique autour du point d’inflexion (les deux courbes du S sont identiques). Le modèle ‘selfStart’ correspondant dans R s’appelle SSlogis(). Ses paramètres sont Asym (= \\(y_\\infty\\)), xmid (= \\(t_0\\)), et scal (= \\(k\\)). Voici le graphique d’une courbe logistique avec \\(y_\\infty = 0,95\\), \\(t_0 = 5\\) et \\(k = 1\\). logis_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSlogis(t, Asym = 0.95, xmid = 5, scal = 1) ) chart(data = logis_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0, 0.95/2, 0.95), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 5, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;Asym/2&quot;, x = -0.5, y = 0.95/2) + annotate(&quot;text&quot;, label = &quot;xmid&quot;, x = 5.4, y = 0.03) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion&quot;, x = 6, y = 0.45) Cette courbe sigmoïdale est asymptotique en 0 et \\(y_\\infty\\), et elle est également symétrique autour de son point d’inflexion situé à \\({t_0, y_\\infty / 2}\\). A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04c_logistique&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04c_logistique&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04c_logistique Il est possible de généraliser ce modèle en définissant une courbe logistique dont l’asymptote basse peut se situer n’importe où ailleurs qu’en 0. Si cette asymptote se situe en \\(y_0\\), nous obtenons l’équation : \\[y(t) = y_0 + \\frac{y_\\infty - y_0}{1 + e^{-k \\ (t - t_0)}}\\] Ceci est le modèle logistique généralisé à quatre paramètres (modèle ‘selfSart’ SSfpl() dans R, pour four-parameters logistic). Les arguments sont A, la première asymptote horizontale (= \\(y_0\\)), B, la seconde asymptote horizontale (= \\(y_\\infty\\)), xmid (= \\(t_0\\)) et scal (= \\(k\\)). Le graphique ressemble très fort à celui de la fonction logistique, mais la première asymptote n’est plus nécessairement à 0 (ici, \\(y_0\\) = 0,15). fpl_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSfpl(t, A = 0.15, B = 0.95, xmid = 5, scal = 1) ) chart(data = fpl_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0.15, 0.8/2 + 0.15, 0.95), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 5, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;A&quot;, x = -0.4, y = 0.15) + annotate(&quot;text&quot;, label = &quot;B&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;xmid&quot;, x = 5.4, y = 0.13) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion&quot;, x = 6.1, y = 0.53) A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04d_logistique&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04d_logistique_gen&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04d_logistique_gen 4.3.5 Modèle de Gompertz Gompertz (1825) a observé de manière empirique que le taux de survie décroît souvent de manière proportionnelle au logarithme du nombre d’animaux qui survivent. Bien que ce modèle reste utilisé pour décrire des courbes de survie, elle trouve de nombreuses applications pour décrire également des données de croissance. L’équation différentielle du modèle de Gompertz est : \\[\\frac{dy(t)}{dt} = k \\ [ \\ln y_\\infty - \\ln y(t)] \\ y(t)\\] qui se résout et se simplifie en : \\[y(t) = y_\\infty \\ e^{-k \\ (t - t_0)} = y_\\infty \\ e^{-a \\cdot b^t} = y_\\infty \\ a^{e^{-k \\ t}} = y_\\infty \\ a^{b^t}\\] La dernière forme apparaît plus simple et est le plus souvent utilisée. La première forme est directement dérivée de l’équation différentielle et donne une meilleure comparaison avec la courbe logistique, puisque \\(t_0\\) correspond aussi à l’abscisse du point d’inflexion, qui n’est plus en position symétrique ici (voir figure ci-dessous). Le modèle ‘selfStart’ correspondant dans R s’appelle SSgompertz(). Sa paramétriqation correspond à la seconde forme, mais avec \\(a\\) appelé b2 et \\(b\\) appelé b3 (\\(y(t) = Asym \\ e^{-b2 \\cdot b3^t}\\)). Voici le graphique d’une courbe de Gompertz avec \\(y_\\infty = 0,95\\), \\(a = 5\\) et \\(b = 0,5\\). gomp_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSgompertz(t, Asym = 0.95, b2 = 5, b3 = 0.5) ) chart(data = gomp_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0, 0.95/exp(1), 0.95), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 2.3, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;Asym/e&quot;, x = -0.5, y = 0.95/exp(1)) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion&quot;, x = 3.5, y = 0.32) A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04e_gompertz&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04e_gompertz&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04e_gompertz 4.3.6 Modèles de von Bertalanffy Le modèle de von Bertalanffy, parfois appelé Brody-Bertalanffy (d’après les travaux de von Bertalanffy et Brody) ou modèle de Pütter (dans Ricker, 1979 notamment), est la première courbe de croissance qui a été élaborée spécifiquement pour décrire la croissance somatique individuelle. Il est basé sur une analyse bioénergétique simple. Un individu est vu comme un simple réacteur biochimique dynamique où les entrées (anabolisme) sont en compétition avec les sorties (catabolisme). Le résultat de ces deux flux étant la croissance. L’anabolisme est plus ou moins proportionnel à la respiration et la respiration est proportionnelle à une surface pour beaucoup d’animaux (la surface développée des poumons ou des branchies), soit encore, les 2/3 de la masse. Le catabolisme est toujours proportionnel à la masse. Ces relations mécanistiques sont rassemblées dans l’équation différentielle suivante où \\(y(t)\\) mesure l’évolution d’un volume ou d’un poids au cours du temps : \\[\\frac{dy(t)}{dt} = a \\ y(t)^{2/3} - b \\ y(t)\\] En résolvant cette équation, nous obtenons le modèle de croissance pondérale de von Bertalanffy : \\[y(t) = y_\\infty \\ (1 - e^{-k \\ (t - t_0)})^3\\] La forme la plus simple de ce modèle est obtenue lorsque nous mesurons des dimensions linéaires pour quantifier la taille de l’organisme, car une dimension linéaire est, en première approximation, la racine cubique d’une masse, proportionnelle au volume à densité constante (sans prendre en compte une possible allométrie). Le modèle de von Bertalanffy pour des mesures linéaires est alors simplement  : \\[y(t) = y_\\infty \\ (1 - e^{-k \\ (t - t_0)})\\] Un graphique des deux modèles est présenté ci-dessous. Le modèle de von Bertalanffy pour mesures linéaire n’a pas de point d’inflexion. La croissance est la plus rapide à la naissance et ne fait que diminuer avec le temps pour finalement atteindre zéro lorsque la taille maximale asymptotique est atteinte. Avec ce modèle, la croissance est donc déterminée et elle ne peut dépasser cette asymptote horizontale située en \\(y(t)= y_\\infty\\). A cause de la puissance cubique de la forme pondérale du modèle von Bertalanffy, cette dernière est une sigmoïde asymétrique, comme l’est le modèle de Gompertz également. Trois modèles ‘selfStart’ existent dans R : SSasympOff(), SSasymp() et SSasympOrig(). SSAsympOff() est définie comme \\(y(t) = y_\\infty \\ (1 - e^{-e^{lrc} \\ (t - t_0)})\\). Ici \\(k\\) est remplacé par \\(e^{lrc}\\). Cette astuce permet d’avoir un paramètre \\(k\\) qui ne prend pas de valeur négatives, puisque l’exponentielle d’une valeur négative est un nombre compris entre 0 et 1. Donc, un \\(lrc\\) négatif donne un \\(k\\) compris entre 0 et 1. Cela permet de contraindre un paramètre du modèle sans nécessité de passer obligatoirement par l’algorithme “Port”. A noter finalement que \\(y_\\infty\\) s’appelle Asym dans tous les modèles ‘selfStart’ dans R, y compris ici, et que \\(t_0\\) s’appelle ici c0. SSAsymp() est définie comme \\(y(t) = y_\\infty + (R_0 - y_\\infty) \\ e^{-e^{lrc} \\cdot t}\\). Par rapport à la forme habituelle, outre l’astuce de \\(e^{lrc}\\) à la place de \\(k\\), \\(t_0\\) est également reparamétré en \\(R_0\\) qui représente la taille initiale au temps \\(t = 0\\). Cela mets l’accent sur cette “taille à la naissance”. Reparamétriser un modèle consiste à exprimer la fonction mathématique qui le représente d’une façon différente. Etant donné que l’interprétation biologique des paramètres fait partie des objectifs de la méthode. On parle d’approche mécanistique, qui vise à décrypter le mécanisme sous-jacent versus une approche purement empirique basée sur les données uniquement avec un modèle polynomial tout venant, par exemple. Ainsi, L’équation du modèle de von Bertalanffy présentée au début montre une paramétrisation “classique”, implémentée dans SSasympOff() alors que SSasymp() mets la taille initiale en évidence via le paramètre \\(R_0\\). SSasympOrig() force le modèle à passer par l’origine des axes {0, 0}. Il n’y a donc plus que deux paramètres Asym = \\(y_\\infty\\) et lrc tel que \\(k = e^{lrc}\\). Ce modèle simplifié est souvent utile en pratique. Il a l’avantage d’être simple, avec seulement deux paramètres. Il est l’équivalent d’une droite forcée à zéro pour le modèle von Bertalanffy. Si R0 dans SSasymp() ou c0 dans SSasympOff() ne sont pas significativement différents de zéro (test t de Student dans le tableau des paramètres), vous pouvez envisager de simplifier le modèle vers SSasympOrig(). Le modèle von Bertalanffy en poids n’est pas implémenté, nous devons utiliser une fonction personnalisée dans ce cas (avec une paramétrisation similaire à SSasympOff() : asympOff3 &lt;- function(x, Asym, lrc, c0, m) Asym*(1 - exp(-exp(lrc) * (x - c0)))^3 Voici les deux modèles de von Bertalanffy présentés sur le même graphique avec \\(y_\\infty\\) (alias Asym) = 0,95, lrc = 0,1 et \\(t_0\\) (alias c0) = 0. vb_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSasympOff(t, Asym = 0.95, lrc = 0.1, c0 = 0), y3 = asympOff3(t, Asym = 0.95, lrc = 0.1, c0 = 0), ) chart(data = vb_data, y ~ t %col=% &quot;VB en taille&quot;) + geom_line() + geom_line(f_aes(y3 ~ t %col=% &quot;VB en poids&quot;)) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = 0.95, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + labs(color = &quot;Modèle&quot;) Dans les deux cas l’asymptote représentant la taille maximale possible vaut 0,95. Pour le modèle pondéral, c’est le cube de la valeur obtenue avec SSasympOff() appliquée sur la racine cubique des masses. Dans les deux cas, lrc = 0,1 donc \\(k = e^{0,1} = 1,1\\) et c0 = 0, la droite passe par l’origine (donc, nous aurions également pu utiliser SSasympOrig() pour générer ces données. 4.3.7 Modèle de Richards La forme généralisée du modèle de von Bertalanffy est : \\[y(t) = y_\\infty \\ (1 - e^{-k \\ (t - t_0)})^m\\] Von Bertalanffy (1938, 1957) a fixé \\(m\\) à 1 ou à 3. Richards (1959) permet à \\(m\\) de varier librement, et donc son modèle a un paramètre de plus. Cette dernière courbe est très flexible (voir schéma ci-dessous) et il est possible de démontrer que plusieurs autres modèles de croissance ne sont que des cas particuliers de ce modèle généraliste avec différentes valeurs de \\(m\\). Nous avons déjà observé que le modèle de Richards se réduit aux deux modèles de von Bertalanffy quand \\(m = 1\\) et \\(m = 3\\). Il se réduit aussi à une courbe logistique lorsque \\(m = -1\\) et il est possible de montrer qu’il converge vers le modèle de Gompertz lorsque \\(|m| \\rightarrow \\infty\\). Il n’existe aucun modèle ‘selfStart’ pour la courbe de Richards dans R. Par ailleurs, il s’agit d’un modèle particulièrement délicat à ajuster, comme nous le verrons dans un exemple concret plus loin dans la section “choix du modèle”. Avec une paramétrisation proche de celle de SSasympOff(), la fonction de Rcichards peut s’écrire comme ceci : richards &lt;- function(x, Asym, lrc, c0, m) Asym*(1 - exp(-exp(lrc) * (x - c0)))^m Voici l’allure de différentes courbes de Richards en fonction de la valeur de \\(m\\) (0,5, 1, 3, 6 et 9) avec \\(lrc = 0,1\\), \\(y_\\infty = 0,95\\) et \\(t_0 = 0\\) pour toutes les courbes. rich_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 1), y05 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 0.5), y3 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 3), y6 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 6), y9 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 9) ) chart(data = rich_data, y ~ t %col=% &quot;m = 1&quot;) + geom_line() + geom_line(f_aes(y05 ~ t %col=% &quot;m = 0.5&quot;)) + geom_line(f_aes(y3 ~ t %col=% &quot;m = 3&quot;)) + geom_line(f_aes(y6 ~ t %col=% &quot;m = 6&quot;)) + geom_line(f_aes(y9 ~ t %col=% &quot;m = 9&quot;)) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = 0.95, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + labs(color = &quot;Richards avec :&quot;) 4.3.8 Modèle de Weibull Depuis son introduction en 1951 par Weibull, ce modèle est présenté comme polyvalent. Il a été décrit à l’origine comme une distribution statistique. Il trouve de nombreuses applications en croissance de population (éventuellement négative), et il est utilisé également pour décrire la courbe de survie en cas de maladie ou dans des études de dynamique de populations. Il a parfois été utilisé comme un modèle de croissance. La forme la plus générale de ce modèle est  : \\[y(t) = y_\\infty - d \\ e^{-k \\ t^m}\\] avec \\(d = y_\\infty - y_0\\). Un modèle à trois paramètres est également utilisé où \\(y_0 = 0\\). La fonction est sigmoïdale lorsque \\(m &gt; 1\\), sinon elle ne possède pas de point d’inflexion (voir graphique ci-dessous). Dans R, le modèle ‘selfStart’ s’appelle SSweibull(). Comme pour le modèle von Bertalanffy, le paramètre \\(k\\) est contraint à une valeur positive via l’astuce \\(k = e^{lrc}\\). Comme d’habitude, \\(y_\\infty\\) se nomme Asym. \\(d\\) est ici appelé Drop et \\(m\\) est appelé pwr. Voici l’allure de différentes courbes de Weibull pour respectivement \\(m\\) (alias pwr) = 5, 2, 1 et 0,5, avec \\(lrc = -0,5\\) (donc, \\(k = e^{-0,5}\\) = 0,61), \\(y_\\infty\\) (alias Asym) = 0,95 et \\(y_0= 0,05\\), ce qui donne \\(d\\) (alias Drop) = 0,95 - 0,05 = 0,9. La courbe avec \\(m = 1\\) est équivalente à un modèle de von Bertalanffy linéaire. Toutes les courbes démarrent en \\(y_0\\) et passent par \\(y_\\infty - d \\ e^{-k}\\) qui est également le point d’inflexion pour les sigmoïdes lorsque \\(m &gt; 1\\). weib_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 1), y05 = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 0.5), y2 = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 2), y5 = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 5) ) chart(data = weib_data, y ~ t %col=% &quot;m = 1&quot;) + geom_line() + geom_line(f_aes(y05 ~ t %col=% &quot;m = 0.5&quot;)) + geom_line(f_aes(y2 ~ t %col=% &quot;m = 2&quot;)) + geom_line(f_aes(y5 ~ t %col=% &quot;m = 5&quot;)) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0.05, 0.95, 0.95 - 0.9 * exp(-exp(-0.5))), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;y0&quot;, x = -0.4, y = 0.05) + annotate(&quot;text&quot;, label = &quot;Asym-d*e^-k&quot;, x = 0, y = 0.95 - 0.9 * exp(-exp(-0.5))) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion si m &gt; 1&quot;, x = 3, y = 0.43) + labs(color = &quot;Weibull avec :&quot;) 4.3.9 Modèle Preece-Baines 1 Preece et Baines (1978) ont décrit plusieurs modèles spécifiques à la croissance humaine. Ces modèles combinent deux phases de croissance exponentielle pour représenter la croissance graduelle d’enfants suivie par une courte phase de croissance accélérée à l’adolescence, mais qui atteint rapidement un plateau correspondant à la taille adulte définitive (voir graphique ci-dessous). Ce type de modèle est naturellement très utile pour tous les mammifères, mais certains, comme le modèle 1 présenté ici, ont aussi été utilisés dans d’autres circonstances, profitant de sa grande flexibilité. Son équation est : \\[y(t) = y_\\infty - \\frac{2 (y_\\infty - d)}{e^{k1 \\ (t - t_0)} + e^{k2 \\ (t - t_0)}}\\] Dans R, la fonction à utiliser (avec une paramétrisation similaire à celle des autres modèles ‘selfStart’) est : preece_baines1 &lt;- function(x, Asym, Drop, lrc1, lrc2, c0) Asym - (2 * (Asym - Drop)) / (exp(exp(lrc1) * (x - c0)) + exp(exp(lrc2) * (x - c0))) \\(k1\\) et \\(k2\\) sont forcés à des valeurs positives ou nulles grace à l’astuce de passer par lrc1 et lrc2. \\(d\\) est Drop et \\(t_0\\) est c0. Ci-dessous un exemple de courbe Preece-Baines 1 avec \\(y_\\infty\\) (alias Asym) = 0,95, \\(d\\) (alias Drop) = 0,8, \\(k1 = 0,19\\) (alias lrc1 = log(0,19) = -1,7), \\(k2 = 2,5\\) (alias lrc2= log(2,5) = 0,92) et \\(t_0\\) (alias c0) = 6. pb1_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = preece_baines1(t, Asym = 0.95, Drop = 0.8, lrc1 = -1.7, lrc2 = 0.92, c0 = 6) ) chart(data = pb1_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = 0.95, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) 4.3.10 Modèle de Tanaka Tous les modèles précédents sont asymptotiques, à l’exception de la courbe exponentielle (mais cette dernière ne modélise valablement que la phase de croissance initiale). Tous ces modèles décrivent donc une croissance déterminée qui n’excédera jamais une taille maximale représentée par une asymptote horizontale en \\(y(t) = y_\\infty\\). Knight (1968) s’est demandé s’il s’agit d’une réalité biologique ou simplement d’un artefact mathématique. Dans le second cas, la croissance n’apparaîtrait déterminée que parce que les modèles choisis pour la représenter sont asymptotiques. Pour s’affranchir d’une telle contrainte, Tanaka (1982, 1988) a élaboré un nouveau modèle de croissance qui décrit une croissance non déterminée : \\[y(t) = \\frac{1}{\\sqrt{b}} \\ \\ln |2 \\ b \\ (t - t_0) + 2 \\ \\sqrt{b^2 \\ (t - t_0)^2 + a \\ b}| + d\\] Ce modèle complexe à quatre paramètres a une période initiale de croissance lente, suivie d’une période de croissance exponentielle qui se poursuit par une croissance continue mais plus faible tout au long de la vie de l’animal (voir graphique ci-dessous). Dans R, nous pouvons utiliser la fonction suivante pour ajuster un modèle de Tanaka : tanaka &lt;- function(x, a, b, c0, d) 1 / sqrt(b) * log(abs(2 * b * (x - c0) + 2 * sqrt(b^2 * (x - c0)^2 + a * b))) + d Ci-dessous, un exemple de courbe de Tanaka avec \\(a\\) = 3, \\(b\\) = 2,5, \\(d\\) = -0,2 et \\(t_0\\) (alias c0) = 2. tanaka_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = tanaka(t, a = 3, b = 2.5, c0 = 2, d = -0.2) ) chart(data = tanaka_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) "],
["choix-du-modele.html", "4.4 Choix du modèle", " 4.4 Choix du modèle Le choix d’un modèle non linéaire fait intervenir des critères identiques à ceux d’un modèle linéaire (qualité d’ajustement évaluée par l’AIC, inspection visuelle de l’ajustement dans le nuage de points), mais il fait aussi intervenir une dimension supplémentaire : le choix de la fonction marthématique à ajuster. Comme nous venons de le voir au travers de quelques modèles courants en biologie, le nombre de fonctions mathématiques résultant en des formes similaire, par exemple de type sigmoïde, est grand. Ainsi, le choix de la meilleure fonction à utiliser dans un cas particulier est rendu plus difficile. Nous allons illustrer l’ajustement d’une courbe non linéaire par le choix et l’ajustement d’un modèle de croissance dans un jeu de données en modélisant la croissance somatique de l’oursin Paracentrotus lividus (Grosjean 2001). Ce jeu de données est disponible dans le package data.io, sous le nom urchin_growth. SciViews::R urchins &lt;- read(&quot;urchin_growth&quot;, package = &quot;data.io&quot;, lang = &quot;fr&quot;) chart(data = urchins, diameter ~ age) + geom_point() Comme vous pouvez le voir, différents oursins ont été mesurés via le diamètre à l’ambitus du test (zone la plus large) en mm à différents âges (en années). Les mesures ont été effectuées tous les 3 à 6 mois pendant plus de 10 ans, ce qui donne un bon aperçu de la croissance de cet animal y compris la taille maximale asymptotique qui est atteinte vers les 4 à 5 ans (pour ce genre de modèle, il est très important de continuer à mesurer les animaux afin de bien quantifier cette taille maximale asymptotique). Ainsi, l’examen du graphique nous permet d’emblée de choisir un modèle à croissance finie (pas le modèle de Tanaka, donc), et de forme sigmoïdale. Les modèles logistique, Weibull ou Gompertz pourraient convenir par exemple. Nous pouvons à ce stade, essayer différents modèles et choisir celui qui nous semble le plus adapté. Le choix du meilleur modèle se fait grâce à deux critères : Les connaissances théoriques et a priori du modèle que l’on ajuste. En effet, il n’existe qu’un seul modèle linéaire, mais une infinité de modèles curvilinéaires qui peuvent s’ajuster dans les données. Le choix du meilleur modèle se fait en fonction de considérations sur le phénomène sous-jacent qui doivent se refléter dans les propriétés mathématiques de la courbe choisie. Par exemple, si on sait que la croissance est asymptotique vers une taille maximale, nous devrons choisir une courbe mathématique qui présente une asymptote horizontale à son maximum pour représente au mieux le phénomène étudié. Le coefficient de détermination \\(R^2\\) n’est pas calculé par R pour une régression non linéaire car sa validité est sujette à discussion entre les statisticiens (d’autres logiciels statistiques le calculent). Nous n’avons donc pas d’estimation de la qualité de l’ajustement par ce biais, comme dans le cas de la régression linéaire. Par contre, il est possible de calculer un autre critère plus fiable que nous avons déjà utilisé : le critère d’Akaïke (fonction AIC() dans R). Ce critère tient compte à la fois de la qualité d’ajustement et de la complexité du modèle, exprimée par le nombre de paramètres qu’il faut estimer. Plus le modèle est complexe, plus on peut s’attendre à ce qu’il s’ajuste bien aux données car il est plus flexible. Cependant, en ce domaine, la complexité n’est pas forcément un gage de qualité. On recherche plutôt un compromis entre meilleur ajustement et simplicité. Le critère d’information d’Akaiké quantifie précisément ce compromis, c’est-à-dire que le modèle qui a un AIC le plus faible est considéré comme le meilleur. Appliquons donc ce concept pour sélectionner le meilleur modèle de croissance pour décrire la croissance somatique de nos oursins après avoir sélectionné les modèles candidats les plus judicieux (modèle sigmoïdal avec asymptote horizontale au maximum). Notons toutefois que, comme les animaux sont mesurés aux mêmes âges, tous les 3 à 6 mois, certains points se superposent. Afin d’afficher tous les points, il est utile d’utiliser la fonction geom_jitter() à la place de geom_point() qui décale les points d’une valeur aléatoire pour éviter ces superpositions (l’argument width = indique le décalage maximum à appliquer). Voici ce que cela donne (en ajoutant également un titre avec formattage correct du nom en latin)  : urchins_plot &lt;- chart(data = urchins, diameter ~ age) + geom_jitter(width = 0.1, alpha = 0.2, color = &quot;darkgrey&quot;) + ggtitle(expression(paste(&quot;Croissance de l&#39;oursin &quot;, italic(&quot;Paracentrotus lividus&quot;)))) urchins_plot Nous avons ici également représenté les points de manière semi-transparente avec alpha = 0.2(transparence de 20%) pour encore mieux mettre en évidence les points de mesures qui se superposent. Ajustons maintenant un modèle de Gompertz (modèle ‘SelfStart’) : urchins_gomp &lt;- nls(data = urchins, diameter ~ SSgompertz(age, Asym, b2, b3)) summary(urchins_gomp) # # Formula: diameter ~ SSgompertz(age, Asym, b2, b3) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 57.403687 0.257588 222.85 &lt;2e-16 *** # b2 3.901916 0.046354 84.18 &lt;2e-16 *** # b3 0.434744 0.003852 112.86 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.49 on 7021 degrees of freedom # # Number of iterations to convergence: 3 # Achieved convergence tolerance: 2.655e-06 Utilisons maintenant notre fonction as.function.nls() pour ajouter la courbe sur le graphique. as.function.nls &lt;- function(x, ...) { nls_model &lt;- x name_x &lt;- names(nls_model$dataClasses) stopifnot(length(name_x) == 1) function(x) predict(nls_model, newdata = structure(list(x), names = name_x)) } A présent, nous pouvons faire ceci : urchins_plot + stat_function(fun = as.function(urchins_gomp), color = &quot;red&quot;, size = 1) L’ajustement de cette fonction semble très bon, à l’oeil. Voyons ce qu’il en est d’autres modèles. Par exemple, une courbe logistique : urchins_logis &lt;- nls(data = urchins, diameter ~ SSlogis(age, Asym, xmid, scal)) summary(urchins_logis) # # Formula: diameter ~ SSlogis(age, Asym, xmid, scal) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 54.628069 0.202985 269.1 &lt;2e-16 *** # xmid 2.055285 0.009568 214.8 &lt;2e-16 *** # scal 0.764830 0.007355 104.0 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.6 on 7021 degrees of freedom # # Number of iterations to convergence: 4 # Achieved convergence tolerance: 1.079e-06 Et voici le graphique avec les deux modèles superposés : urchins_plot + stat_function(fun = as.function(urchins_gomp), aes(color = &quot;Gompertz&quot;), size = 1) + stat_function(fun = as.function(urchins_logis), aes(color = &quot;logistique&quot;), size = 1) + labs(color = &quot;Modèle&quot;) Notez que ici, la couleur a été incluse dans le “mapping” (argument mapping =) de stat_function() en l’incluant dans aes(). Cela change fondamentalement la façon dont la couleur est perçue par ggplot2. Dans ce cas-ci, la valeur est interprétée non comme une couleur à proprement parler, mais comme un niveau (une couche) à inclure dans le graphique et à reporter via une légende. Ensuite, à l’aide de labs() on change le titre de la légende relatif à la couleur par un nom plus explicite : “Modèle”. Nous pouvons comparer ces modèles à l’aide du critère d’Akaïke. AIC(urchins_gomp, urchins_logis) # df AIC # urchins_gomp 4 43861.73 # urchins_logis 4 44139.13 Comme on peut le voir clairement sur le graphe, la courbe logistique donne une autre solution, cette dernière est à peine moins bonne que le modèle de Gompertz, selon le critère d’Akaiké. Pourtant, la courbe est assez bien démarquée de celle de Gompertz. Essayons maintenant un modèle de Weibull. Ce modèle est plus complexe car il a quatre paramètres au lieu de trois pour les deux modèles précédents : urchins_weib &lt;- nls(data = urchins, diameter ~ SSweibull(age, Asym, Drop, lrc, pwr)) summary(urchins_weib) # # Formula: diameter ~ SSweibull(age, Asym, Drop, lrc, pwr) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 56.80491 0.32704 173.69 &lt;2e-16 *** # Drop 56.81320 0.59393 95.66 &lt;2e-16 *** # lrc -1.57392 0.02865 -54.94 &lt;2e-16 *** # pwr 1.67880 0.02960 56.72 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.47 on 7020 degrees of freedom # # Number of iterations to convergence: 3 # Achieved convergence tolerance: 2.532e-06 Ajoutons ce nouveau modèle sur le graphique : urchins_plot + stat_function(fun = as.function(urchins_gomp), aes(color = &quot;Gompertz&quot;), size = 1) + stat_function(fun = as.function(urchins_logis), aes(color = &quot;logistique&quot;), size = 1) + stat_function(fun = as.function(urchins_weib), aes(color = &quot;Weibull&quot;), size = 1) + labs(color = &quot;Modèle&quot;) … et comparons à l’aide du critère d’Akaïke : AIC(urchins_gomp, urchins_logis, urchins_weib) # df AIC # urchins_gomp 4 43861.73 # urchins_logis 4 44139.13 # urchins_weib 5 43810.72 Ce modèle fait presque jeu égal avec le modèle de Gompertz en terme de critère d’Akaiké ; juste un tout petit peu mieux. En fait, les deux courbes sont pratiquement superposées l’une à l’autre, mais le modèle de Weibull à un démarrage de croissance plus lent au début, ce qui se reflète dans les données. Par contre, il est pénalisé par le fait que c’est un modèle plus complexe qui possède un paramètre de plus. L’un dans l’autre, le critère d’information d’Akaiké considère donc les deux modèles sur pratiquement sur le même plan du point de vue de la qualité de leurs ajustements respectifs. A ce stade, nous voudrions également essayer un autre modèle flexible à quatre paramètres : le modèle de Richards. Malheureusement, il n’existe pas de fonction ‘SelfStart’ dans R pour ce modèle. Nous sommes donc réduit “à mettre les mains dans le cambouis”, à définir la fonction nous même, à trouver de bonnes valeurs de départ, etc. Voici comment définir la fonction : richards &lt;- function(x, Asym, lrc, c0, m) Asym*(1 - exp(-exp(lrc) * (x - c0)))^m Pour les valeurs de départ, là ce n’est pas facile. Asym est l’ asymptote horizontale à la taille maximum. On voit qu’elle se situe aux environ de 55 mm sur le graphique. Pour les autres paramètres, c’est plus difficile à évaluer. Prenons par exemple 1 comme valeur de départ pour les trois autres paramètres, ce qui donne (les valeurs de départ sont obligatoires ici puisque ce n’est pas un modèle ‘SelfStart’) : urchins_rich &lt;- nls(data = urchins, diameter ~ richards(age, Asym, lrc, c0, m), start = c(Asym = 55, lrc = 0.1, c0 = 1, m = 1)) # Error in numericDeriv(form[[3L]], names(ind), env): Missing value or an infinity produced when evaluating the model … et voilà ! Un excellent exemple de plantage de l’algorithme de minimisation de la fonction objective suite à un comportement inadéquat de la fonction avec les valeurs testées. Ici, la fonction renvoie l’infini et l’algorithme ne peut donc effectuer la minimisation. La fonction de Richards est effectivement connue pour être difficile à ajuster pour cette raison. Il nous faut donc soit tester d’autres valeurs de départ, soit utiliser un autre algorithme de minimisation, soit les deux. Après différents essais il apparaît que le changement des valeurs de départ suffit dans le cas présent : urchins_rich &lt;- nls(data = urchins, diameter ~ richards(age, Asym, lrc, c0, m), start = c(Asym = 55, lrc = -0.7, c0 = 0, m = 1)) summary(urchins_rich) # # Formula: diameter ~ richards(age, Asym, lrc, c0, m) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 58.14348 0.37772 153.934 &lt; 2e-16 *** # lrc -0.27595 0.03152 -8.754 &lt; 2e-16 *** # c0 -0.87545 0.28464 -3.076 0.002109 ** # m 6.20711 1.82345 3.404 0.000668 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.487 on 7020 degrees of freedom # # Number of iterations to convergence: 11 # Achieved convergence tolerance: 7.11e-06 Ajoutons ce dernière modèle sur notre graphique (avec des traits un peu plus fins pour mieux distinguer les modèles les uns des autres) : urchins_plot + stat_function(fun = as.function(urchins_gomp), aes(color = &quot;Gompertz&quot;), size = 0.7) + stat_function(fun = as.function(urchins_logis), aes(color = &quot;logistique&quot;), size = 0.7) + stat_function(fun = as.function(urchins_weib), aes(color = &quot;Weibull&quot;), size = 0.7) + stat_function(fun = as.function(urchins_rich), aes(color = &quot;Richards&quot;), size = 0.7) + labs(color = &quot;Modèle&quot;) … et comparons à l’aide du critère d’Akaïke : AIC(urchins_gomp, urchins_logis, urchins_weib, urchins_rich) # df AIC # urchins_gomp 4 43861.73 # urchins_logis 4 44139.13 # urchins_weib 5 43810.72 # urchins_rich 5 43854.53 La courbe est très proche des modèles de Gompertz et Weibull aux jeunes âges, mais l’asymptote maximale est légèrement plus haute que pour les deux autres modèles (58 mm au lieu de 57 mm). Les trois courbes sont très, très proches l’une de l’autre. Le critère d’information d’Akaiké est marginalement moins bon pour le modèle de Richards que pour celui de Weibull, mais est tout juste meilleur que celui de Gompertz. En outre l’écart type pour le paramètre x0 est plus conséquent en comparaison de sa valeur, ce qui démontre une certaine instabilité de la fonction par rapport à ce paramètre, et par conséquent, une incertitude dans son estimation. Pour cette raison, det pour la difficulté à l’ajuster, le modèle de Richards sera écarté dans notre cas au benefice du modèle de Weibull, voire de celui de Gompertz plus simple. Le choix final entre Gompertz ou Weibull dépend de l’usage que l’on veut faire du modèle. Si la simplicité du modèle est primordiale, nous garderons Gompertz. Si la croissance des petits oursins est un aspect important de l’analyse, nous garderons Weibull qui semble mieux s’ajuster aux données à ce niveau. A vous de jouer ! Réalisez un cahier de laboratoire sur la croissance de bactérie en définissant un modèle non linéaire pertinent pour ces données. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/kSLMNWpw Lisez le README afin de prendre connaissance de l’exercice Références "],
["hierarchique.html", "Module 5 Classification hiérarchique", " Module 5 Classification hiérarchique Objectifs Comprendre la notion de distance et la matrice de distance. Appréhender la classification hiérarchique et le dendrogramme. Être capable d’effectuer un regroupement pertinent des individus d’un jeu de données multivarié à l’aide de ces techniques. Prérequis Vous devez être à l’aise avec l’utilisation de R et Rstudio, en particulier pour l’importation, le remaniement et la visualisation de données multivariées. Ceci correspond au cours SDD I. Il n’est pas nécessaire d’avoir acquis toutes les notions vue dans la partie Cours II :modélisation pour pouvoir comprendre cette seconde partie du cours. Si vous ne vous sentez pas assez à l’aise avec R et RStudio, c’est peut-être le bon moment pour refaire le premier “learnr” du package BioDataScience2 : Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01a_rappel&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["analyse-de-donnees.html", "5.1 Analyse de données", " 5.1 Analyse de données L’analyse de données (on dit aussi analyse exploratoire des données, EAD ou statistiques exploratoires) mets en œuvre des méthodes statistiques multivariées visant à découvrir de l’information pertinente dans un gros jeu de données via des approches multidimensionnelles et essentiellement descriptives. Ces méthodes se regroupent en deux grandes familles : Celles visant à réduire la dimensionnalité (travailler avec des tableaux ayant moins de colonnes). Elles permettent ensuite de présenter les données de manière synthétique pour observer des relations entre les variables ou les individus via des représentations graphiques. Nous aborderons ces techniques dans les modules suivants. Celles cherchant à classifier (ou regrouper) les individus. Il s’agit ici de synthétiser le gros tableau de données dans l’autre sens, selon les lignes. L’approche via la classification hiérarchique sera détaillée ici. La vidéo suivante introduit l’EAD (jusqu’à 2:11) : "],
["distance-entre-individus.html", "5.2 Distance entre individus", " 5.2 Distance entre individus Vous êtes en train d’analyser des données concernant les échantillons de plancton que vous avez prélevé sur votre lieu de recherche. Ce plancton a été numérisé (photo de chaque organisme) et les images ont été traitée avec un logiciel qui mesure automatiquement une vingtaine de variables telles que la surface de l’objet sur l’image, son périmètre, sa longueur, … Vous vous trouvez donc face à un jeu de données qui a une taille non négligeable : 20 colonnes par 1262 lignes, soit le nombre d’individus mesurés dans vos échantillons. zoo &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;) zoo # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # … with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; Vous voulez regrouper votre plancton en fonction de la ressemblance entre les organismes, c’est-à-dire, en fonction des écarts entre les mesures effectuées pour les 19 variables quantitatives, à l’exclusion de la vingtième colonne class qui est une variable factor). En raison de la taille du tableau, il est évident que cela ne pourra pas se faire de manière manuelle. Nous pouvons raisonnablement considérer que plus les mesures sont similaires entre deux individus, plus ils ont des chances d’être semblables, c’est-à-dire, d’appartenir au même groupe taxonomique. Mais comment faire pour synthétiser l’information de similarité ou différence contenue dans 19 paires de valeurs (une paire par variable)  ? Nous avons besoin d’une mesure de distance qui quantifie la similarité (ou à l’inverse la dissimilarité) en un seul nombre. Celle qui vient naturellement à l’esprit est la distance euclidienne. Prenons un cas simplifié. Quelle est la distance qui sépare deux individus A et B par rapport à trois variables x, y, z ? Ici, nous pouvons représenter l’information graphiquement dans un espace à trois dimensions. La distance qui nous intéresse est la distance linéaire entre les deux points dans l’espace. Autrement dit, c’est la longueur du segment de droite qui relie les deux points dans l’espace. Cette distance, nous pouvons la calculer à l’aide de la formule suivante (problème de niveau lycée, voir par exemple ici pour une résolution dans le plan) : \\[\\mathrm{D_{Euclidean}}_{A, B} = \\sqrt{(x_A - x_B)^2 + (y_A - y_B)^2 + (z_A - z_B)^2}\\] Notez que cette formule se généralise à n dimensions et s’écrit alors, pour n’importe quelle paire d’individus indicés j et k dans notre tableau et pour les différentes mesures de 1 à i notées yi : \\[\\mathrm{D_{Euclidean}}_{j, k} = \\sqrt{\\sum_{i=1}^{n}(y_{ij}-y_{ik})^2}\\] C’est la racine carré de la somme dans les n dimensions des écarts entre les valeurs au carré pour toutes les variables yi. Plus sa valeur est grande, plus les individus sont éloignés (différents). Pour cette raison, nous appelerons cette distance, une mesure de dissimilarité. 5.2.1 Matrice de distances Nous avons maintenant la possibilité de quantifier la similitude entre nos organismes plactoniques… mais nous en avons un grand nombre. Cela va être impossible à gérer autant de mesures qu’il y a de paires possibles parmi 1262 individus12. La matrice de distance est une matrice ici 1262 par 1262 qui rassemble toutes les valeurs possibles. Notez que sur la diagonale, nous comparons chaque individu avec lui-même. La distance euclidienne vaut donc systématiquement zéro sur la diagonale. \\[\\mathrm{D_{Euclidean}}_{j, j} = 0\\] De plus, de part et d’autre de cette diagonale, nous trouvons les paires complémentaires (j versus k d’un côté et k versus j de l’autre). Or qu’elle soit mesurée dans un sens ou dans l’autre, la distance du segment de droite qui relie deux points dans l’espace est toujours la même. \\[\\mathrm{D_{Euclidean}}_{j, k} = \\mathrm{D_{Euclidean}}_{k, j}\\] Par conséquent, seulement une portion (soit le triangle inférieur, soit le triangle supérieur hors diagonale) est informative. La diagonale ne porte aucune informartion utile, et l’autre triangle est redondant. Nous avons donc pour habitude de ne calculer et représenter que le triangle inférieur de cette matrice. Maintenant que cela est clair, nous pouvons créer un objet dist qui contiendra notre matrice de distances enclidiennes. Il suffit d’utiliser la fonction dist(), ou mieux vegan::vegdist() qui offre plus de possibilités13. Comme cela prendrait trop de place d’imprimer la matrice complète, nous allons réaliser le travail sur seulement les six premiers individus de notre tableau (et nous devons aussi éliminer la colonne class qui ne contient pas de données numériques et qui ne nous intéresse pas pour le moment) : zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class head(., n = 6) -&gt; zoo6 # Récupération des 6 premiers individus zoo6_dist &lt;- vegan::vegdist(zoo6, method = &quot;euclidean&quot;) zoo6_dist # En pratique, on n&#39;imprime généralement ce genre d&#39;objet ! # 1 2 3 4 5 # 2 8.2185826 # 3 2.5649705 5.7320911 # 4 0.8582142 7.8813537 2.2220079 # 5 4.8478629 4.2950067 3.0119468 4.6148173 # 6 2.4269520 10.6197317 4.9228255 2.8477882 7.1766853 Nous voyons bien ici que R n’imprime que le triangle inférieur de notre matrice 6 par 6. Notez aussi que les objets dist de tailles plus réalistes que vous génèrerez dans vos analyses ne sont prévue pour être imprimées et visualisées telles quelles. Il s’agit seulement de la première étape vers une représentation utile qui sera réalisée à la page suivante, à l’aide de la classification hiérarchisée. Félicitations ! Vous venez de calculer votre première matrice de distances. Nous verrons à la page suivante comment nous pouvons utiliser l’information qu’elle contient pour regrouper les individus de manière pertinente. Mais avant cela, nous avons besoin d’un peu de théorie pour bien comprendre quelle métrique choisir pour calculer nos distances et pourquoi. On parle aussi d’indices de similarité ou dissimilarité. Attention : nous n’avons pas considéré ici les unités respectives de nos variables. Une surface (mm2) ou une longeur (mm) ne sont pas mesurées dans les mêmes unités. Nous risquons alors de donner plus de poids aux valeurs élevées. Nous aurions le même effet si nous décidions par exemple d’exprimer une mesure longitudinale en µm au leiu de l’exprimer en mm. Dans ce cas, il vaut mieux standardiser d’abord le tableau (moyenne de zéro et écart type de un) selon les colonnes avant d’effectuer le calcul. Ceci sera fait à la page suivante. 5.2.2 Indices de (dis)similarité Un indice de similarité (similarity index en anglais) est une descripteur statistique (nombre unique) de la similitude de deux échantillons ou individus représentés par plusieurs variables dans un échantillon multivarié. Un indice de similarité prend une valeur comprise entre 0 (différence totale) et 1 ou 100% (similitude totale). Un indice de dissimilarité} est le complément d’un indice de similarité (dis = 1 – sim); sa valeur est comprise entre 100% (différence totale) et 0 (similitude totale). Attention : dans certains cas, un indice de dissimilarité peut varier de 0 à +\\(\\infty\\)**. Il n’existe alors pas d’in,dice de similarité complémentaire. C’est le cas précisément de la distance euclidienne que nous avons exploré jusqu’ici. Tous les indices de similarité / dissimilarité peuvent servir à construire des matrices de distances. 5.2.2.1 Indice de Bray-Curtis L’indice de dissimilarité de Bary-Curtis, aussi appelé coefficient de Czecanowski est calculé comme suit : \\[\\mathrm{D_{Bray-Curtis}}_{j,k}=\\frac{\\sum_{i=1}^{n}\\left|y_{ij}-y_{ik}\\right|}{\\sum_{i=1}^{n}(y_{ij}+y_{ik})}\\] Dans R nous utiliserons vegan::vegdist(DF, method = &quot;bray&quot;).Il s’utilise pour mesurer la similitude entre échantillon sur base du dénombrement d’espèces. Si le nombre d’individus est très variable (espèces dominantes versus espèces rares), nous devons transformer les données pour éviter de donner trop de poids aux espèces les plus abondantes (ex: \\(log(x+1)\\), double racine carrée, …). Une caractéristique essentielle de cet indice (contrairement à la distance euclidienne) est que toute double absence n’est pas prise en compte dans le calcul. C’est souvent pertinent dans le cadre de son utilisation comme le dénombrement d’espèces. En effet, quelle information utile retire-t-on de doubles zéros dans un tableau répertoriant la faune belge pour le crocodile du Nil et le tigre de Sibérie par exemple ? Aucune ! Ils sont tous deux systématiquement absents des dénombrements, mais cette double absence n’apporte aucune information utile pour caractériser la faune belge par ailleurs. L’indices de similarité de Bray-Curtis (sim) est complémentaire à l’indices de dssimilarité correspondant (dis tel que calculé ci-dessus) : \\[sim = 1 – dis\\] 5.2.2.2 Indice de Canberra L’indice de dissimilarité de Canberra est similaire à l’indice de Bray-Curtis mais il pondère les espèces en fonction du nombre d’occurrences afin de donner le même poids à chacune dans le calcul. Il se calcule comme suit : \\[\\mathrm{D_{Canberra}}_{j,k}=\\frac{1}{nz}\\sum_{i&#39;=1}^{nz}\\frac{\\left|y_{i&#39;j}-y_{i&#39;k}\\right|}{\\left|y_{i&#39;j}\\right|+\\left|y_{i&#39;k}\\right|}\\] où \\(nz\\) est le nombre de valeurs non nulles simultanément dans le tableau de départ. Toutes les espèces contribuent ici de manière égale. C’est un point positif, mais il faut faire attention à ce que cet indice a souvent tendnace à donner une surimportance aux espècces très rares observées une seule fois ou un petit nombre de fois ! Dans R, nous utiliserons vegan::vegdist(DF, method = &quot;canberra&quot;). Toute double absence n’est pas prise en compte ici aussi. Seuls les indices ne dépendant pas des doubles zéros sont utilisables pour des dénombrements d’espèces ou des présence-absence. Ainsi pour ce type de données, notre choix se portera sur : Bray-Curtis si l’on souhaite que le résultat soit dominé par les espèces les plus abondantes. Canberra si notre souhait est de donner la même importance à toutes les espèces, mais avec un risque de domination des espèces rares. Bray-Curtis sur données transformées (\\(log(x+1)\\) ou double racine carrée) pour un compromis entre les deux avec prise en compte de toutes les espèces, mais domination partielle des espèces les plus abondantes. C’est souvent un bon compromis. Attention : Si les volumes échantillonnés entre stations ne sont pas comparables, il faut standardiser (moyenne nulle et écart type un) les données selon les échantillons avant de faire les calculs de distances. De même que pour Bray-Curtis, l’indice de similarité sim se calcule à partir de l’indice de dissimilarité dis tel que ci-dessus comme \\(sim = 1 - dis\\). 5.2.2.3 Distance Euclidienne Nous savons déjà que c’est la distance géométrique entre les points dans un espace à n dimensions : \\[\\mathrm{D_{Euclidean}}_{j,k}=\\sqrt{\\sum_{i=1}^{n}(y_{ij}-y_{ik})^2}\\] Dans R, cette distance peut être calculée avec dist(DF) ou vegan::vegdist(DF, method = &quot;euclidean&quot;). Cet indice de dissimilarité est utile pour des mesures quantitatives, pour des données environnmentales, etc. Il faut que les mesures soient toutes effectuées dans les mêmes unités. Si ce n’est pas le cas, penser alors à standardiser les mesures avant le calcul comme nous l’avons fait plus dans l’exemple sur le zooplancton. Il n’existe pas d’indice de similarité complémentaire. 5.2.2.4 Distance de Manhattan La distance de Manhattan, encore appelée “city-block distance” est un indice de dissimilarité qui, contrairement à la distance euclidienne ne mesure pas la distance géométrique entre les points en ligne droite, mais via un trajet qui suit les parallèmes aux axes. C’est comme si la distance euclidenne reliait les points à vol d’oiseau, alors qu’avec la distance de Manhattan, on devait contourner les blocs de maisons du quartier pour aller d’un point A à un point B (d’où le nom de cette métrique). Elle se calcule comme suit : \\[\\mathrm{D_{Manhattan}}_{j,k}=\\sum_{i=1}^{n}|y_{ij}-y_{ik}|\\] Dans R, nous utiliserons vegan::vegdist(DF, method = &quot;manhattan&quot;). Ici aussi, seul l’indice de dissimilarité est défini. L’indice de similarité complémentaire n’existe pas car la valeur de l’indice de dissimlarité n’est pas borné à droite et peut varier de zéro (dissimilarité nulle, les deux individus soint identiques) à l’infini pour une différence maximale. 5.2.3 Utilisation des indices Les distances euclidienne ou de Manhattan sont à préférer pour les mesures environnementales ou de manière générale pour les variables quantitatives continues. Les distances de Bray-Curtis ou Canberra sont meilleure pour les dénombrements d’espèces (nombreux double zéro), ou de manière générale, pour les variables quantitatives discrètes prenant des valeurs nulles ou positives. 5.2.4 Propriétés des indices Les indices varient en 0 et 1 (0 et 100%), mais les distances sont utilisées aussi comme indices de dissimilarité et varient entre 0 et \\(+\\infty\\). Un indice est dit métrique si : Minimum 0 : \\(I_{j, k} = 0\\) si \\(j = k\\) Positif : \\(I_{j, k}&gt;0\\) si \\(j \\neq k\\) Symétrique : \\(I_{j, k}=I_{k, j}\\) Inégalité triangulaire : \\(I_{j, k} + I_{k, l} &gt;= I_{j, l}\\) La dernière propriété d’inégalité triangulaire est la plus difficile à obtenir, et n’est pas toujours nécessaire. Nous pouvons montrer que certains indices qui ne respectent pas cette dernière propriété sont pourtant utiles dans le contexte. Nous dirons alors d’un indice que c’est une semi-métrique s’il répond à toutes les conditions sauf la quatrième. Enfin, un indice est dit non métrique dans tous les autres cas. Le tableau suivant reprend les métriques que nous avons vu jusqu’ici, et rajoute d’autres candidats potentiels (la distance Chi carré, l’indice de correlation ou de variance/covariance) en indiquant leur type : Distance Type Bray-Curtis semi-métrique Canberra métrique Euclidienne métrique Manhattan métrique Chi carré métrique (correlation) (non métrique) (variance/covariance) (non métrique) Pour en savoir plus Vous pouver aussi transposer le tableau pour calculer la distance entre les variables en utilisant la fonction t() dans R (dist(t(DF))). Par exemple, dans le cas d’un tableau “espèces - station” (dénombrement d’espèces en différentes stations), nous pouvons souhaiter comparer les stations du point de vue de la composition en espèces, mais nous pouvons aussi comparer les espèces du point de vue de leur répartition entre les stations. Pour passer d’un calcul à l’autre, nous transposerons donc le tableau (les colonnes deviennent les lignes et inversément) avant d’utiliser dist() ou vegan::vegdist(). Pour bien comprendre la logique derrière les indices, il est utile de comprendre les équations correspondantes. Si ces équations sont pour vous trop abstraites, une façon efficace de comprendre consiste à faire le calcul à la main. Par exemple dans le cas de l’indice de Canberra, la notion de nombre de données non nulles \\(nz\\) n’est pas évident. Effectuons un calcul à la main détaillé sur le tableau fictif suivant concernant trois espèces A, B, et C dénombrées en trois stations Sta1, Sta2 et Sta3 dans le tableau nommé ex1 : A B C Sta1 4 0 2 Sta2 3 0 10 Sta3 1 8 0 Pour rappel, la dissimilarité de Canberra se calcule comme suit: \\[\\mathrm{D_{Canberra}}_{j,k}=\\frac{1}{nz}\\sum_{i&#39;=1}^{nz}\\frac{\\left|y_{i&#39;j}-y_{i&#39;k}\\right|}{y_{i&#39;j}+y_{i&#39;k}}\\] où: nz est le nombre d’observations non nulles simultanément dans les deux vecteurs comparés (les doubles zéros ne sont pas pris en compte) i’ est l’itérateur sur toutes les valeurs non double zéros Voici le détail du calcul (notez bien comment le double zéro pour l’espèce B entre les stations 1 et 2 est pris en compte dans le calcul) : Sta1_2 &lt;- (1/2) * ((abs(4 - 3)) / (4 + 3) + (abs(2 - 10)) / (2 + 10)) round(Sta1_2, 2) # [1] 0.4 Sta1_3 &lt;- (1/3) * (abs(4 - 1) / (4 + 1) + abs(0 - 8) / (0 + 8) + abs(2 - 0) / (2 + 0)) round(Sta1_3, 2) # [1] 0.87 Sta2_3 &lt;- (1/3) * (abs(3 - 1) / (3 + 1) + abs(0 - 8) / (0 + 8) + abs(10 - 0) / (10 + 0)) round(Sta2_3, 2) # [1] 0.83 La matrice finale est la suivante : # Sta1 Sta2 # Sta2 0.40 # Sta3 0.87 0.83 Vérifions en laissant R faire le calcul : vegan::vegdist(ex1, method = &quot;canberra&quot;) %&gt;.% round(., 2) # Sta1 Sta2 # Sta2 0.40 # Sta3 0.87 0.83 Attention ! dist(, method = &quot;canberra&quot;) ne multiplie pas la somme par \\(1/nz\\), mais par \\(n/nz\\) (variante). Ainsi, nous obtenons des distances trois fois plus grandes ici avec dist() qu’avec vegan::vegdist(), mais les deux calculs restent valables car les écarts relatifs au sein de la matrice de distance sont les mêmes. dist(ex1, method = &quot;canberra&quot;) %&gt;.% round(., 2) # Sta1 Sta2 # Sta2 1.21 # Sta3 2.60 2.50 A vous de jouer ! Réalisez le tutoriel afin de vérifier votre bonne compréhension des matrices de distance. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;05a_distance_matrix&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Réalisez un carnet de note par binôme sur le transect entre Nice et Calvi. Lisez attentivement le README Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/g/R45egn-p Lisez le README afin de prendre connaissance de l’exercice Le nombre de paires uniques et distinctes (pas j, j ou k, k) possibles parmi n items est \\(n(n-1)/2\\), soit ici pour 1262 éléments nous avons 795.691 paires.↩ La fonction dist() ne propose par exemple pas la méthode de Bray-Curtis qui est fréquemment utilisée en biologie et en écologie, contrairement à vegan::vegdist(method = &quot;bray&quot;).↩ "],
["regroupement-avec-cah.html", "5.3 Regroupement avec CAH", " 5.3 Regroupement avec CAH A partir du moment où nous avons une matrice de distances entre toutes les paires d’individus dans notre jeu de données, nous pouvons les regrouper en fonction de leur ressemblance. Deux approches radicalement différentes existent. Soit nous pouvons diviser l’échantillon progressivement jusqu’à obtenir des groupes homogènes (méthodes divisives, telle que les K-moyennes que nous aborderons dans le module suivant), soit nous pouvons regrouper les items semblables petit à petit jusqu’à avoir traité tout l’échantillon (méthodes agglomératives, telle que la classification ascendante hiérarchique étudiée ici). 5.3.1 Dendrogramme Le dendrogramme est une manière très utile de représenter graphiquement un regroupement. Il s’agit de réaliser un arbre dichotomique ressemblant un peu à un arbre phylogénétique qui est familier aux biologistes (par exemple ici) : Dans l’arbre, nous représentons des divisions dichotomiques (division d’une branche en deux) matérialisant les divergences au cours de l’évolution. Dans l’arbre présenté ici, l’axe des ordonnées est utilisé pour représenter le temps et les branchements sont placées à une hauteur correspondante en face de l’axe. Le dendrogramme est une représentation similaire de la CAH avec l’axe des ordonnées indiquant à quelle distance le rassemblement se fait. Dans R, la réalisation du dendrogramme se fait en deux étapes : Son calcul par CAH à l’aide de la fonction hclust() Sa représentation sur un graphique en utilisant plot() (pour un graphique de base) ou ggdendro::ggdendrogram() (pour un graphique ggplot2). Partons, pour étayer notre raisonnement, d’une matrice de distances euclidiennes sur les données de zooplancton. Au passage, abordons quelques fonctions de R utiles dans le contexte pour préparer correctement nos données. A la section précédente, nous avons suggéré qu’il peut être utile de standardiser nos données préalablement si une distance de type euclidienne ou manhattan est ensuite calculée et si les données numériques sont mesurées dans des unités différentes, comme c’est le cas ici. La fonction scale() se charge de cette standardisation colonne par colonne dans un tableau. Comme elle renvoie une matrice, nous devons ensuite retransformer le résultat en data.frame ou tibble. Nous choisissons ici d’utiliser la fonction as_tibble(). Limitons, pour l’instant notre ambition à la comparaison de six individus. Afin d’observer tous les cas possibles dans le dendrogramme, nous ne prendrons pas les six premières lignes du tableau, mais les lignes 13 à 18. Cela peut se faire à l’aide de la fonction slice() que nous n’avons pas encore beaucoup utilisée jusqu’ici. Cette fonction permet de spécifier explicitement les numéros de lignes à conserver, contrairement à filter() qui applique un test de condition pour décider qulles ligne(s) converser. Voici donc notre matrice de distances euclidiennes sur les données ainsi traitées. Les individus initiaux 13 à 18 sont renumérotés 1 à 6. Nous n’imprimons plus ici la matrice de distance obtenue car ce n’est que la première étape du travail vers une représentation plus utile (le dendrogramme). zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class scale(.) %&gt;.% # Standardisation des 19 colonnes as_tibble(.) %&gt;.% # Conversion de la matrice en data.frame +tibble slice(., 13:18) -&gt; zoo6 # Récupération des lignes 13 à 18 zoo6 %&gt;.% vegan::vegdist(., method = &quot;euclidean&quot;) -&gt; zoo6std_dist Notre objet zoo6std_dist est ensuite utilisé pour calculer notre CAH à l’aide de hclust(). Enfin, plot() (ou ggdendro::ggdendrogram()) se charge de tracer le dendrogramme. zoo6std_dist %&gt;.% hclust(.) -&gt; zoo6std_clust # Calcul du dendrogramme plot(zoo6std_clust) # ggdendro::ggdendrogram() peut aussi être utilisé ici Voici comment interpréter ce graphique. Les deux individus les plus semblables sont le 2 et le 5 (regroupepent effectué le plus bas, donc, avec la valeur de l’indice de dissimilarité le plus faible), également tous deux similaires au 3. Le 4 se rattache à ce groupe, mais bien plus haut sur l’axe, indiquant ainsi qu’il s’en différencie un peu plus, enfin, le 1 et le 6 sont rassemblés à un niveau à peu près équivalent. Finalement, le groupe de droite constitué des individus 2, 3, 4 et 5 et celui de gauche contenant le 1 et le 6 sont reliés encore plus haut suggérant ainsi la dissimilitude la plus forte entre ces deux groupes. Attention : La position des individus selon l’axe horizontal n’est pas importante. Il faut voir le dendrogramme comme un mobile qui peut tourner librement. C’est-à-dire que le groupe constitué de 2, 3, 4 et 5 aurait très bien pu être placé à la gauche de celui constitué de 1 et 6. De même, le sous-groupe 2, 3 et 5 aurait très bien pu être à la gauche du regroupement avec 4 à la droite, etc. Notez que nous n’avons pas utilisé l’information concernant les classes taxonomiques auxquelles les individus appartiennent (nous avons éliminé la variable class en tout début d’analyse). Pour cette raison, ce type de regroupement s’appelle une classification non supervisée parce que nous n’imposons pas les groupes que nous souhaitons réaliser14. Nous pouvons néanmoins révéler ces classes maintenant pour vérifier si le dendrogramme réalisé est logique (étape bien sûr facultative et souvent pas possible en pratique lorsque cette information n’est pas disponible) : zoo$class[13:18] # [1] Egg_round Poecilostomatoid Poecilostomatoid Decapod # [5] Calanoid Appendicularian # 17 Levels: Annelid Appendicularian Calanoid Chaetognath ... Protist Les individus 2, 3 et 5 (“Poecilostomatoid” ou “Calanoid”) sont des copépodes, des crustacés particulièrement abondants dans le zooplancton. Leur forme est similaire. Leur regroupement est logique. L’individu 4 est une larve de décapode, un autre crustacé. Ainsi le regroupement de 2, 3 et 5 avec 4 correspond aux crustacés ici. Enfin, les individus 1 et 6 sont très différents puisqu’il s’agit respectivement d’un œuf rond probablement de poisson et d’un appendiculaire. 5.3.2 Séparer les groupes Si notre dendrogramme est satisfaisant (nous le déterminons en l’étudiant et en vérifiant que le regroupement obtenu a un sens biologique par rapport à l’objectif de notre étude), nous concrétisons le regroupement en coupant l’arbre à une certaine hauteur à l’aide de la fonction cutree(). Nous pouvons matérialiser ce niveau de coupure en traçant un trait horizontal rouge avec abline(h = XXX, col = &quot;red&quot;) pour le graphe de base, ou en ajoutant + geom_hline(yintercept = XXX, col = &quot;red&quot;) au graphique ggplot2, avec ‘XXX’ la hauteur de coupure souhaitée. Par exemple, si nous voulons réaliser deux groupes, nous ferons : plot(zoo6std_clust) abline(h = 8, col = &quot;red&quot;) Pour réaliser trois groupes, nous couperons plus bas (ici version ggplot2) : ggdendro::ggdendrogram(zoo6std_clust) + geom_hline(yintercept = 6.5, col = &quot;red&quot;) Et ainsi de suite. A mesure que la hauteur de coupure descend, nous réaliserons quatre, puis cinq groupes. Quel est le meilleur niveau de coupure ? Il n’y en a pas un seul forcément car les différents niveaux correspondent à un point de vue de plus en plus détaillé du regroupement. Néanmoins, lorsque le saut sur l’axe d’un regroupement à l’autre est fort, nous pouvons considérer une séparation des groupes d’autant meilleure. Cela crédibilise d’autant le regroupement choisi. Ainsi la séparation en deux groupes apparait forte (entre 10,2 et 7,4, sur un intervalle de 2,8 unités donc). De même, la séparation de l’individu 4 par rapport au groupe constitué de 2, 3 et 5 se fait sur un intervalle de 4,7 unités (entre 7,4 et 2,7). En comparaison, la séparation de l’individu 3 du groupe 2 et 5 est nettement moins nette puisqu’elle apparait aux hauteurs entre 2,7 à 2.4, soit seulement sur un intervalle de 0,3 unités. Ces mesures d’intervalles n’ont aucune valeur absolue. Il faut les considérer uniquement de manière relatives les unes par rapport aux autres, et seulement au sein d’un même dendrogramme. Ici, deux niveaux de coupure se détachent. Nous utilisons aussi la fonction rect.hclust() pour matérialiser ces groupes sur le dendrogramme, et cutree() pour obtenir une nouvelle variable que nous pourrons ajouter dans notre tableau de données qui concrétise le regroupement choisi : en deux groupes, nous avons les crustacés séparés des autres. plot(zoo6std_clust) abline(h = 8, col = &quot;red&quot;) rect.hclust(zoo6std_clust, h = 8, border = c(&quot;blue&quot;, &quot;red&quot;)) (group2 &lt;- cutree(zoo6std_clust, h = 8)) # [1] 1 2 2 2 2 1 Les individus 1 et 6 sont dans le groupe 1, et les autres dans le groupe 2. en quatre groupes, nous avons les copépodes séparés des trois autres items chacun dans un groupe séparé. plot(zoo6std_clust, hang = -1) # Prolonger les tiges jusqu&#39;en bas abline(h = 5, col = &quot;red&quot;) rect.hclust(zoo6std_clust, h = 5, border = c(&quot;blue&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;)) Le groupe des individus 2, 3 et 5 est encadré en rouge. Les trois autres groupes des individus uniques 1, 6 et 4 respectivement sont ici encadrés en bleu. (group4 &lt;- cutree(zoo6std_clust, h = 5)) # [1] 1 2 2 3 2 4 L’individu 1 est dans le groupe 1, les individus 2, 3 et 5 sont dans le groupe 2, l’individu 4 est dans le groupe 3 et enfin l’individu 6 est dans le groupe 4. Donc, cutree() numérote ses groupes en fonction du premier item présent dedans dans l’ordre du tableau de données. Nous pouvons rajouter ces regroupements dans le tableau de données, et utiliser cette information pour en faire d’autres choses utiles. Par exemple, nous représentons un nuage de point des données et colorons nos points en fonction du premier regroupement effectué comme suit : zoo6$group2 &lt;- as.factor(group2) # Utiliser une variable facteur ici chart(zoo6, area ~ circularity %col=% group2) + geom_point() Nous voyons que les copépodes (groupe 2 en turquoise) sont plus grands (area) et moins circulaires (circularity) que les deux autres particules du groupe 1 en rouge. Nous pouvons ainsi réexplorer nos données en fonction du regroupement pour mieux le comprendre. 5.3.2.1 Méthodes de CAH Tant que l’on compare des individus isolés entre eux, il n’y a pas d’ambiguïté. Par contre, dès que nous comparons un groupe avec un individu isolé, ou deux groupes entre eux, nous avons plusieurs stratégies possible pour calculer leurs distances (argument method = de hclust()) : Liens simples (single linkages en anglais) hclust(DIST, method = &quot;single&quot;) : la distance entre les plus proches voisins au sein des groupes est utilisée. zoo6std_dist %&gt;.% hclust(., method = &quot;single&quot;) %&gt;.% plot(.) Les données tendent à être aggrégées de proche en proche en montrant une gradation d’une extrême à l’autre. Dans le cas d’une variation progressive, par exemple lors d’un passage graduel d’une situation A vers une situation B le long d’un transect, le dendrogramme obtenu à l’aide de cette méthode représentera la situation au mieux. Liens complets (complete linkages) hclust(DIST, method = &quot;complete&quot;), méthode utilisée par défault si non précisée : la distance entre les plus lointains voisins est considérée. C’est le dendrogramme que nous avons obtenu au début. Le dendrogramme a tendance à effectuer des groupes séparés plus nettement les uns des autres qu’avec les liens simples. Liens moyens (group-average) hclust(DIST, method = &quot;average&quot;) encore appelée méthode UPGMA : moyenne des liens entre toutes les paires possibles intergroupes. Nous obtenons une situation intermédiaire entre liens simples et liens moyens. zoo6std_dist %&gt;.% hclust(., method = &quot;average&quot;) %&gt;.% plot(.) Dans ce cas-ci, le résultat est similaire aux liens simples, mais ce n’est pas forcément le cas à chaque fois. Méthode de Ward hclust(DIST, method = &quot;ward.D2&quot;). Considérant le partitionnement de la variance totale du nuage de points (on parle aussi de l’inertie du nuage de points) entre variance interclasse et variance intraclasse, la méthode vise à maximiser la variance interclasse et minimiser la variance intraclasse, ce qui revient d’ailleurs au même. Cette technique fonctionne souvent très bien pour obtenir des groupes bien individualisés. zoo6std_dist %&gt;.% hclust(., method = &quot;ward.D2&quot;) %&gt;.% plot(.) Ici nous obtenos un dendrogramme très proche des liens complets. Encore une fois, ce n’est pas forcément le cas dans toutes les situations. Liens médians, centroïdes, …, constituent encore d’autre méthodes possibles, mais moins utilisées. Elles ont l’inconvénient de produire parfois des inversions dans le dendrogramme, c’est-à-dire qu’un regroupement plus avant se fait parfois à une hauteur plus basse sur l’axe des ordonnées, ce qui rend le dendrogramme peu lisible et beaucoup moins esthétique. Dans notre exemple, la méthode centroïde crée une telle inversion à la hauteur de 5 environ sur l’axe des ordonnées entre l’individu 6 et l’individu 1. Alors, qui est regroupé en premier ? Pas facile à déterminer dans ce cas ! zoo6std_dist %&gt;.% hclust(., method = &quot;centroid&quot;) %&gt;.% plot(.) 5.3.3 Étude complète Voici ce que cela donne si nous effectuons une CAH sur le jeu zooplancton complet avec ses 1262 lignes. zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class scale(.) %&gt;.% # Standardisation des 19 colonnes as_tibble(.) %&gt;.% # Transformation en data.frame + tibble vegan::vegdist(., method = &quot;euclidean&quot;) %&gt;.% # Matrice de distances hclust(., method = &quot;ward.D2&quot;) -&gt; zoo_clust # CAH avec Ward plot(zoo_clust, labels = FALSE) # Dendrogramme sans labels des individus abline(h = 70, col = &quot;red&quot;) # Séparation en 3 groupes Naturellement, avec 1262 branches, notre dendrogramme est très encombré ! Cependant, il reste analysable tant que nous nous intéressons aux regroupements de plus haut niveau (vers le haut du dendrogramme). Notez comme les vlaeurs sur l’axe des ordonnées ont changé par rapport à nos cas simple à six items (ne jamais comparer les hauteurs entre dendrogrammes différents). Notre CAH est terminée, mais nous pouvons matérialiser le regroupement sous forme d’une variable supplémentaire dans le tableau et l’utiliser ensuite. zoo_clust %&gt;.% cutree(., h = 70) %&gt;.% # Matérialisation des groupes as.factor(.) -&gt; # Conversion en variable facteur zoo$Groupes # Ajout au tableau comme variable Groupes chart(zoo, compactness ~ ecd %col=% Groupes) + geom_point() + # Exploration visuelle des groupes (exemple) coord_trans(x = &quot;log10&quot;, y = &quot;log10&quot;) # Axes en log10 Et de manière optionnelle, si un classement est disponible par ailleurs (c’est le cas ici avec la variable class), nous pouvons réaliser un tableau de contingence à double entrées entre le regroupement obtenu et le classement pour comparaison. table(zoo$class, zoo$Groupes) # # 1 2 3 # Annelid 31 13 6 # Appendicularian 0 36 0 # Calanoid 9 237 42 # Chaetognath 0 17 34 # Cirriped 1 21 0 # Cladoceran 47 3 0 # Cnidarian 3 5 14 # Cyclopoid 0 50 0 # Decapod 121 5 0 # Egg_elongated 2 48 0 # Egg_round 44 0 5 # Fish 4 46 0 # Gastropod 48 2 0 # Harpacticoid 0 39 0 # Malacostracan 27 66 28 # Poecilostomatoid 20 136 2 # Protist 0 50 0 Ainsi, nous pouvons constater que le groupe 1 contient une fraction importante des annélides, des cladocères,des décapodes, des œufs ronds et des gastéropodes. Le groupe 2 contient un maximum des copépodes représentés par les calanoïdes, les cyclopoïdes, les harpacticoïdes et les poecilostomatoïdes. Il contient aussi tous les appendiculaires, tous les protistes, presque tous les œufs allongés, les poissons, et une majorité des malacostracés. Enfin, le groupe 3 contient une majorité des chaetognathes et des cnidaires. Le regroupement a été réalisé uniquement en fonction de mesures effectuées sur les images. Il n’est pas parfait, mais des tendances se dégagent tout de même. Pour en savoir plus La vidéo suivante présente la matière que nous venons d’étudier de manière légèrement différente. Plus d’explications sont également apportées concernant la méthode de Ward. Une autre explication encore ici. Pour bien comprendre la façon dont une CAH est réalisée, il est utile de détailler le calcul étape par étape sur un exemple simple. Voici une matrice de distances euclidiennes fictive entre 6 stations  : A B C D E B 15 C 6.4 10.86 D 5.2 13.04 5.48 E 5.1 12.37 7.28 7.81 F 10.39 7.42 5.57 9.64 9.49 Effectuons une CAH manuellement par liens complets et traçons le dendrogramme correspondant. Etape 1 : Nous repérons dans la matrice de distance la paire qui a l’indice de dissimilarité le plus petit et effectuons un premier regroupement. A_E = groupe I à la distance 5,1. La matrice de distances est simplifiées par rapport à ce groupe I en considérant la règle utilisée (ici, liens complets, donc on garde la plus grande distance entre toutes les paires possibles lorsqu’il y a des groupes). * Distance entre B et I = B_A = 15 * Distance entre C et I = C_E = 7,28 * Distance entre D et I = D_E = 7,81 * Distance entre F et I = F_A = 10,39 Matrice de distance recalculée : I B C D B 15.00 C 7.28 10.86 D 7.81 13.04 5.48 F 10.39 7.42 5.57 9.64 Etape 2 : on répète le processus. C_D = groupe II à la distance 5,48. * Distance entre I et II = I_D = 7,81 * Distance entre B et II = B_D = 13,04 * Distance entre F et II = F_D = 9,64 Matrice de distance recalculée : I B II B 15.00 II 7.81 13.04 F 10.39 7.42 9.64 Etape 3 : B_F = groupe III à la distance 7,42 * Distance entre I et III = I_B = 15 * Distance entre II et III = II_B = 13,04 Matrice de distance recalculée  : I III III 15.00 II 7.81 13.04 Etape 4 : I_II = groupe IV à la distance 7,81 * Distance entre III et IV = III_I = 15 Matrice de distance recalculée : III IV 15 Etape 5 : III - IV = groupe V à la distance 15. fini ! Voici le dendrogramme résultant : Les techniques complémentaires de classification supervisées, recommandées dans le cas du jeu de données zooplancton seront abordées dans le cours de Science des Données Biologiques III l’an prochain.↩ "],
["k-moyenne-som.html", "Module 6 K-moyenne &amp; SOM", " Module 6 K-moyenne &amp; SOM Objectifs TODO Prérequis TODO "],
["k-moyennes.html", "6.1 K-moyennes", " 6.1 K-moyennes "],
["cartes-auto-adaptatives.html", "6.2 Cartes auto adaptatives", " 6.2 Cartes auto adaptatives La méthode des cartes auto-adaptatives se nomme self-organizing map (SOM) en anglais. "],
["acp-afc.html", "Module 7 ACP &amp; AFC", " Module 7 ACP &amp; AFC Objectifs TODO Prérequis TODO "],
["analyse-en-composantes-principales.html", "7.1 Analyse en composantes principales", " 7.1 Analyse en composantes principales "],
["analyse-factorielle-des-correspondances.html", "7.2 Analyse factorielle des correspondances", " 7.2 Analyse factorielle des correspondances "],
["afm.html", "Module 8 AFM", " Module 8 AFM Objectifs TODO Prérequis TODO "],
["analyse-factorielle-multiple-afm.html", "8.1 Analyse factorielle multiple (AFM)", " 8.1 Analyse factorielle multiple (AFM) L’analyse factorielle multiple (AFM) se nomme principal component analysis (PCA) en anglais. "],
["svbox.html", "A Installation de la SciViews Box", " A Installation de la SciViews Box Pour ce cours SDD 2, nous utiliserons la même SciViews Box que pour le cours 1… mais actualisée (version de l’année). Vous allez donc devoir installer la nouvelle version. La procédure n’a changé que sur des points de détails. Référez-vous à l’appendice A1 du cours SDD 1. Vous pouvez conserver l’ancienne SciViews Box en parallèle avec cette nouvelle version, mais vérifiez si vous avez assez d’espace sur le disque dur pour contenir les deux simultanément. Comptez par sécurité 20Go par version. Si vous manquez de place, vous pouvez éliminer l’ancienne version avant d’installer la nouvelle (vos projets ne seront pas effacés). "],
["migration-des-projets.html", "A.1 Migration des projets", " A.1 Migration des projets Concernant les projets réalisés dans une version précédente de la SciViews Box, ceux-ci restent disponibles, même si vous éliminez l’ancienne. Plusieurs cas de figure se présentent : Vous conserver deux ou plusieurs version de la SciViews Box en parallèle. Dans ce cas, nous conseillons fortement de garder chaque projet accessible à partir de la version dans laquelle il a été créé. Seulement les projets que vous décidez de migrer explicitement (voir ci-dessous) seront à déplacer dans le dossier shared de la nouvelle SciViews Box. Vous aurez à faire cette manipulation, par exemple, si vous devez recommencer un cours l’année suivante afin d’être en phase (même version de la svbox) par rapport à vos nouveaux collègues. Vous ne conservez que la dernière version de la SciViews Box, mais ne devez pas accéder fréquemment vos anciens projets, et dans ce cas, vous pouvez réinstaller temporairement l’ancienne version de svbox. Dans ce cas, ne migrez pas vos anciens projets. Éliminez simplement l’ancienne svbox, tout en laisant vos projets intacts dans son répertoire shared. Lors de la réinstallation de l’ancienne svbox, vous retrouverez alors tous vos anciens projets intactes. Vous ne conservez pas d’ancienne version de la svbox et vous ne souhaitez pas devoir la réinstaller. Il est possible de migrer vos anciens projets en les déplaçant de l’ancien répertoire shared vers le nouveau. Soyez toutefois conscients que vos documents R Markdown et scripts R ne fonctionneront pas forcément dans la nouvelle svbox et qu’une adaptation sera peut-être nécessaire ! "],
["configuration-git-et-github.html", "A.2 Configuration Git et Github", " A.2 Configuration Git et Github A chaque nouvelle installation de la SciViews Box, vous devez la reconfigurer via la boite de dialogue SciViews Box Configuration. En particulier, il est très important d’indiquer correctement votre identifiant et email Git (zone encadrée en rouge dans la copie d’écran ci-dessous). Assurez-vous (si ce n’est déjà fait) que vous possédez un compte Github valide. Vous pouvez cliquer sur le bouton Go to Github par facilté dans la même boite de dialogue. Choisissez de manière judicieuse votre login. Vous pourriez être amenés à l’utiliser bien plus longtemps que vous ne le pensez, y compris plus tard dans votre carrière. Donc, lisez les conseils ci-dessous (inspirés et adaptés de Happy Git and Github for the UseR - Register a Github Account : Incluez votre nom réel. Les gens aiment savoir à qui ils ont affaire. Rendez aussi votre nom/login facile à deviner et à retenir. Philippe Grosjean a comme login phgrosjean, par exemple. Vous pouvez réutiliser votre login d’autres contextes, par exemple Twitter ou Slack (ou Facebook). Choisissez un login que vous pourrez échanger de manière confortable avec votre futur boss. Un login plus court est préférable. Soyez unique dans votre login, mais à l’aide d’aussi peu de caractères que possible. Github propose parfois des logins en auto-complétion. Examinez ce qu’il propose. Rendez votre login invariable dans le temps. Par exemple, n’utilisez pas un login lié à votre université (numéro de matricule, ou nom de l’université inclue dans le login). Si tout va bien votre login vous suivra dans votre carrière, … donc, potentiellement loin de l’université où vous avez fait vos études. N’utilisez pas de logins qui sont aussi des mots ayant une signification particulière en programmation, par exemple, n’utilisez pas NA, même si c’est vos initiales ! Une fois votre compte Github créé, et votre login/email pour votre identification Git correctement enregistrés dans la SciViews Box, vous devez pouvoir travailler, faire des “pushs”, des “pulls” et des “commits”15. Cependant, RStudio vous demandera constamment vos logins et mots de passe… à la longue, c’est lassant ! La procédure ci-dessous vous enregistre une fois pour toutes sur votre compte Github dans RStudio. A.2.1 Compte Github dans RStudio RStudio offre la possibilité d’enregistrer une clé publique/privée dans votre SciViews Box afin de vous enregistrer sur Github de manière permanente. L’avantage, c’est que vous ne devrez plus constamment entrer votre login et mot de passe à chaque opération sur Github ! Nous vous le conseillons donc vivement. Entrez dans Rstudio Server, et allez dans le menu Tools -&gt; Global Options.... Ensuite, cliquez dans la rubrique Git/SVN dans la boite de dialogue. Ensuite, cliquez sur le bouton Create RSA key.... La phrase de passe n’est pas nécessaire (il est même préférable de la laisser vide si vous voulez utiliser Github sans rien devoir taper à chaque fois). Cliquez sur le bouton Create. Vous obtenez alors une fenêtre similaire à celle ci-dessous (bien sûr avec des données différentes). Ceci confirme que votre clé cryptographique a été créée localement. Fermez cette fenêtre pour revenir à la boite de dialogue de configuration de RStudio Server. Dans la boite de dialogue de configuration de RStudio Server, section Git/SVN cliquez sur le lien View public key qui apparait une fois la clé créée : La clé apparait dans une fenêtre, déjà présélectionnée. Copiez-là dans le presse-papier (Ctrl-C ou clic bouton droit et sélection de Copy dans le menu contextuel), puis fermez cette fenêtre. Dans votre navigateur web favori, naviguez vers https://github.com, loggez-vous, et accédez aux paramètres de votre compte Github (menu déroulant en haut à droite, entrée Settings) : Dans les paramètres de votre compte, cliquez sur la rubrique SSH and GPG keys, ensuite sur le bouton vert New SSH key Collez-y votre clé à partir du presse-papier dans la zone Key. Vous pouvez lui donner un nom évocateur dans le champ Title. Ensuite, cliquez sur Add SSH key. Déloggez, puis reloggez-vous dans RStudio Server pour que les changements soient pris en compte. La prochaine action sur Github depuis RStudio pourrait encore déclencher la demande de votre login et mot de passe, mais ensuite, les opérations devraient se faire directement. Si vous éprouvez toujours des difficultés à faire collaborer R et RStudio avec Git et Github, voyez https://happygitwithr.com (en anglais) qui explique les différentes procédures bien plus en détails. Vérifiez toujours lors de votre premier commit que Github vous reconnait bien. Pour cela, naviguez vers le dépôt où vous avez commité avec votre explorateur web, et vérifiez l’identité prise en compte lors de votre commit.↩ "],
["references.html", "Références", " Références "]
]
