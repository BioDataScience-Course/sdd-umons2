[
["index.html", "Science des données biologiques 2 Préambule", " Science des données biologiques 2 Philippe Grosjean &amp; Guyliann Engels 2019-11-07 Préambule Cet ouvrage interactif est le second d’une série de trois ouvrages traitant de la science des données biologiques. L’écriture de cette suite de livres a débuté au cours de l’année académique 2018-2019. Pour l’année académique 2019-2020, cet ouvrage interactif sera le support du cours suivant : Science des données II : Analyse et modélisation, UMONS dont le responsable est Grosjean Philippe Cet ouvrage est conçu pour être utilisé de manière interactive en ligne. En effet, nous y ajoutons des vidéos, des démonstrations interactives, et des exercices sous forme de questionnaires interactifs également. Ces différents éléments ne sont, bien évidemment, utilisables qu’en ligne. Le matériel dans cet ouvrage est distribué sous licence CC BY-NC-SA 4.0. "],
["vue-generale-des-cours.html", "Vue générale des cours", " Vue générale des cours Le cours de Science des données II: analyse et modélisation est dispensé aux biologistes de troisième Bachelier en Faculté des Sciences de l’Université de Mons à partir de l’année académique 2019-2020. La matière est divisée en huit modules de 6h chacun en présentiel. Il nécessitera environ un tiers de ce temps (voir plus, en fonction de votre rythme et de votre technique d’apprentissage) en travail à domicile. Cette matière fait suite au premier cours dont le contenu est considéré comme assimilé (voir https://biodatascience-course.sciviews.org/sdd-umons/). La première moitié du cours est consacrée à la modélisation, un domaine particulièrement important de la science des données qui étend les concepts déjà vu au cours 1 d’analyse de variance et de corrélation entre deux variables. Ces quatre modules formeront aussi un socle sur lequel nous pourrons élaborer les techniques d’apprentissage machine (classification supervisée), et puis ensuite l’apprentissage profond à la base de l’intelligence artificielle qui seront abordées plus tard dans le cours 3. Cette partie est dense, mais ultra importante ! La seconde moitié s’intéressera à l’exploration des données, encore appelée analyse des données qui vise à découvrir des caractéristiques intéressantes dans des très gros jeux de données. Ces techniques sont d’autant plus utiles que les données volumineuses deviennent de plus en plus courantes en biologie. "],
["materiel-pedagogique.html", "Matériel pédagogique", " Matériel pédagogique Le matériel pédagogique, rassemblé dans ce syllabus interactif est aussi varié que possible. Vous pourrez ainsi piocher dans l’offre en fonction de vos envies et de votre profil d’apprenant pour optimiser votre travail. Vous trouverez: le présent ouvrage en ligne, des tutoriaux interactifs (réalisés avec un logiciel appelé learnr). Vous pourrez exécuter ces tutoriaux directement sur votre ordinateur, et vous aurez alors accès à des pages Web réactives contenant des explications, des exercices et des quizzs en ligne, des slides de présentations, des dépôts Github Classroom dans la section BioDataScience-Course pour réaliser et documenter vos travaux personnels. des renvois vers des documents externes en ligne, types vidéos youtube ou vimeo, des ouvrages en ligne en anglais ou en français, des blogs, des tutoriaux, des parties gratuites de cours Datacamp ou équivalents, des questions sur des sites comme “Stackoverflow” ou issues des “mailing lists” R, … Tout ce matériel est accessible à partir du site Web du cours, du présent syllabus interactif (et de Moodle pour les étudiants de l’UMONS). Ces derniers ont aussi accès au dossier SDD sur StudentTemp en Intranet à l’UMONS. Les aspects pratiques seront à réaliser en utilisant la ‘SciViews Box’, une machine virtuelle préconfigurée. Nous installerons ensemble la nouvelle version de cette SciViews Box au premier cours. Il est donc très important que vous soyez présent à ce cours, et vous pouvez venir aussi si vous le souhaitez avec votre propre ordinateur portable comme pour le cours 1. Enfin, vous pourrez poser vos questions par mail à l’adresse sdd@sciviews.org. System information sessioninfo::session_info() # ─ Session info ────────────────────────────────────────────────────────── # setting value # version R version 3.5.3 (2019-03-11) # os Ubuntu 18.04.2 LTS # system x86_64, linux-gnu # ui X11 # language (EN) # collate en_US.UTF-8 # ctype en_US.UTF-8 # tz Europe/Brussels # date 2019-11-07 # # ─ Packages ────────────────────────────────────────────────────────────── # package * version date lib source # assertthat 0.2.1 2019-03-21 [2] CRAN (R 3.5.3) # bookdown 0.9 2018-12-21 [2] CRAN (R 3.5.3) # cli 1.1.0 2019-03-19 [2] CRAN (R 3.5.3) # colorspace 1.4-1 2019-03-18 [2] CRAN (R 3.5.3) # crayon 1.3.4 2017-09-16 [2] CRAN (R 3.5.3) # digest 0.6.18 2018-10-10 [2] CRAN (R 3.5.3) # dplyr 0.8.0.1 2019-02-15 [2] CRAN (R 3.5.3) # evaluate 0.13 2019-02-12 [2] CRAN (R 3.5.3) # farver 1.1.0 2018-11-20 [2] CRAN (R 3.5.3) # gganimate 1.0.3 2019-04-02 [2] CRAN (R 3.5.3) # ggplot2 3.1.1 2019-04-07 [2] CRAN (R 3.5.3) # glue 1.3.1 2019-03-12 [2] CRAN (R 3.5.3) # gtable 0.3.0 2019-03-25 [2] CRAN (R 3.5.3) # hms 0.4.2 2018-03-10 [2] CRAN (R 3.5.3) # htmltools 0.3.6 2017-04-28 [2] CRAN (R 3.5.3) # inline 0.3.15 2018-05-18 [2] CRAN (R 3.5.3) # knitr 1.22 2019-03-08 [2] CRAN (R 3.5.3) # lazyeval 0.2.2 2019-03-15 [2] CRAN (R 3.5.3) # magick 2.0 2018-10-05 [2] CRAN (R 3.5.3) # magrittr 1.5 2014-11-22 [2] CRAN (R 3.5.3) # munsell 0.5.0 2018-06-12 [2] CRAN (R 3.5.3) # pillar 1.3.1 2018-12-15 [2] CRAN (R 3.5.3) # pkgconfig 2.0.2 2018-08-16 [2] CRAN (R 3.5.3) # plyr 1.8.4 2016-06-08 [2] CRAN (R 3.5.3) # prettyunits 1.0.2 2015-07-13 [2] CRAN (R 3.5.3) # progress 1.2.0 2018-06-14 [2] CRAN (R 3.5.3) # purrr 0.3.2 2019-03-15 [2] CRAN (R 3.5.3) # R6 2.4.0 2019-02-14 [2] CRAN (R 3.5.3) # Rcpp 1.0.1 2019-03-17 [2] CRAN (R 3.5.3) # rlang 0.3.4 2019-04-07 [2] CRAN (R 3.5.3) # rmarkdown 1.12 2019-03-14 [2] CRAN (R 3.5.3) # rstudioapi 0.10 2019-03-19 [2] CRAN (R 3.5.3) # scales 1.0.0 2018-08-09 [2] CRAN (R 3.5.3) # sessioninfo 1.1.1 2018-11-05 [2] CRAN (R 3.5.3) # stringi 1.4.3 2019-03-12 [2] CRAN (R 3.5.3) # stringr 1.4.0 2019-02-10 [2] CRAN (R 3.5.3) # tibble 2.1.1 2019-03-16 [2] CRAN (R 3.5.3) # tidyselect 0.2.5 2018-10-11 [2] CRAN (R 3.5.3) # tweenr 1.0.1 2018-12-14 [2] CRAN (R 3.5.3) # withr 2.1.2 2018-03-15 [2] CRAN (R 3.5.3) # xfun 0.6 2019-04-02 [2] CRAN (R 3.5.3) # yaml 2.2.0 2018-07-25 [2] CRAN (R 3.5.3) # # [1] /home/sv/R/x86_64-pc-linux-gnu-library/3.5 # [2] /usr/local/lib/R/site-library # [3] /usr/lib/R/site-library # [4] /usr/lib/R/library "],
["lm.html", "Module 1 Régression linéaire I", " Module 1 Régression linéaire I Objectifs Retrouver ses marques avec R, RStudio et la SciViews Box et découvrir les fonctions supplémentaires de la nouvelle version. Découvrir la régression linaire de manière intuitive. Découvrir les outils de diagnostic de la régression linéaire, en particulier l’analyse des résidus. Prérequis Avant de nous lancer tête baissée dans de la matière nouvelle, nous allons installer la dernière version de la SciViews Box. Une nouvelle version est disponible chaque année début septembre. Reportez-vous à l’appendice A pour son installation et pour la migration éventuelle de vos projets depuis la version précédente. Une fois la box installée, consacrez un petit quart d’heure à repérer les icônes nouvelles dans le Dock et dans le menu Applications. Vous retrouverez R et RStudio, mais dans des version plus récentes qui apportent également leur lot de nouveautés. Lancez RStudio et repérez ici aussi les nouveaux onglets et les nouvelles entrées de menu. Aidez-vous de l’aide en ligne ou de recherches sur le net pour vous familiariser avec ces nouvelles fonctionnalités. Une fois la nouvelle SciViews Box fonctionnelle sur votre ordinateur, vous allez réaliser une séance d’exercice couvrant les points essentiels des notions abordées dans le livre science des données biologiques partie 1, histoire de rafraîchir vos connaissances. Les tutoriaux learnr auxquels vous êtes maintenant habitués seront là pour vous aider à auto-évaluer votre progression. Pour le cours 2, ces tutoriaux sont dans le package BioDataScience2 que vous venez normalement d’installer si vous avez bien suivi toutes les instructions de configuration de votre SciViews Box (sinon, vérifiez votre configuration). Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01a_rappel&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["modele.html", "1.1 Modèle", " 1.1 Modèle Qu’est-ce qu’un “modèle” en science des données et en statistique ? Il s’agit d’une représentation simplifiée sous forme mathématique du mécanisme responsable de la distribution des observations. Rassurez-vous, dans ce module, le côté mathématique du problème sera volontairement peu développé pour laisser une large place à une compréhension intuitive du modèle. Les seules notions clés à connaître ici concernent l’équation qui définit une droite quelconque dans le plan \\(xy\\) : \\[y = a \\ x + b\\] Cette équation comporte : deux variables \\(x\\) et \\(y\\) qui sont matérialisées par les axes des abscisses et des ordonnées dans le plan \\(xy\\). Ces variables prennent des valeurs bien définies pour les observations réalisées sur chaque individu du jeu de données. deux paramètres \\(a\\) et \\(b\\), respectivement la pente de la droite (\\(a\\)) et son ordonnée à l’origine (\\(b\\)). Ecrit de la sorte, \\(a\\) et \\(b\\) peuvent prendre n’importe quelle valeur et l’équation définit de manière généraliste toutes les droites possibles qui existent dans le plan \\(xy\\). Paramétrer ou paramétriser le modèle consiste à définir une et une seule droite en fixant les valeurs de \\(a\\) et de \\(b\\). Par exemple, si je décide de fixer \\(a = 0.35\\) et \\(b = -1.23\\), mon équation définit maintenant une droite bien précise dans le plan \\(xy\\) : \\[y = 0,35 \\ x - 1.23\\] La distinction entre variable et paramètre dans les équations précédentes semble difficile pour certaines personnes. C’est pourtant crucial de pouvoir le faire pour bien comprendre la suite. Alors, c’est le bon moment de relire attentivement ce qui est écrit ci-dessus et de le mémoriser avant d’aller plus avant ! 1.1.1 Pourquoi modéliser ? Le but de la modélisation consiste à découvrir l’équation mathématique de la droite (ou plus généralement, de la fonction) qui décrit au mieux la forme du nuage de points matérialisant les observations dans le plan \\(xy\\) (ou plus généralement dans un hyper-espace représenté par les différentes variables mesurées). Cette équation mathématique peut ensuite être utilisée de différentes façons, toutes plus utiles les unes que les autres : Aide à la compréhension du mécanisme sous-jacent qui a généré les données. Par exemple, si une droite représente bien la croissance pondérale d’un organisme dans le plan représenté par le logarithme du poids (P) en ordonnée et le temps (t) en abscisse, nous pourrons déduire que la croissance de cet organisme est probablement un mécanisme de type exponentiel (puisqu’une transformation inverse, c’est-à-dire logarithmique, linéarise alors le nuage de points). Attention ! Le modèle n’est pos le mécanisme sous-jacent de génération des données, mais utilisé habilement, ce modèle peut donner des indices utiles pour aider à découvrir ce mécanisme. Effectuer des prédictions. Le modèle paramétré pourra être utilisé pour prédire, par exemple, le poids probable d’un individu de la même population après un certain laps de temps. Comparer différents modèles. En présence de plusieurs populations, nous pourrons ajuster un modèle linéaire pour chacune d’elles et comparer ensuite les pentes des droites pour déterminer quelle population a le meilleur ou le moins bon taux de croissance. Explorer les relations entre variables. Sans aucunes connaissances sur le contexte qui a permit d’obtenir nos données, un modèle peut fournir des informations utiles pour orienter les recherches futures. Idéalement, un modèle devrait pouvoir servir à ces différentes applications. En pratique, comme le modèle est forcément une simplification de la réalité, des compromis doivent être concédés pour arriver à cette simplification. En fonction de son usage, les compromis possibles vont différer. Il s’en suit une spécialisation des modèles en modèles mécanistiques qui décrivent particulièrement bien le mécanisme sous-jacent (fréquents en physique, par exemple), les modèles prédictifs conçus pour calculer des nouvelles valeurs (que l’intelligence artificielle affectionne particulièrement), les modèles comparatifs, et enfin, les modèles exploratroires (utilisés dans la phase initiale de découverte et de description des données). Retenez simplement qu’un même modèle est rarement efficace sur les quatre tableaux simultanément. 1.1.2 Quand modéliser ? A chaque fois que deux ou plusieurs variables (quantitatives dans le cas de la régression) forment un nuage de points qui présente une forme particulière non sphérique, autrement dit, qu’une corrélation significative existe dans les données, un modèle peut être utile. Etant donné deux variables quantitatives, trois niveaux d’association de force croissante peuvent être définies entre ces deux variables : La corrélation quantifie juste l’allongement dans une direction préférentielle du nuage de points à l’aide des coefficients de corrélation linéaire de Pearson ou non linéaire de Spearman. Ce niveau d’association a été traité dans le module 12 du cours 1. Il est purement descriptif et n’implique aucunes autres hypothèses sur les données observées. La relation considère que la corrélation observée entre les deux variables est issue d’un mécanisme sous-jacent qui nous intéresse. Un modèle mathématique de l’association entre les deux variables matérialise de manière éventuellement simplifiée, ce mécanisme. Il permet de réaliser ensuite des calculs utiles. Nous verrons plus loin que des contraintes plus fortes doivent être supposées concernant le distribution des deux variables. La causalité précise encore le mécanisme sous-jacent dans le sens qu’elle exprime le fait que c’est la variation de l’une de ces variables qui est directement ou indirectement la cause de la variation de la seconde variable. Bien que des outils statistiques existent pour inférer une causalité (nous ne les aborderons pas dans ce cours), la causalité est plutôt étudiée via l’expérimentation : le biologiste contrôle et fait varier la variable supposée causale, toutes autres conditions par ailleurs invariables dans l’expérience. Il mesure alors et constate si la seconde variable répond ou non à ces variations1 et en déduit une causalité éventuelle. La distinction entre ces trois degrés d’association de deux variables est cruciale. Il est fréquent d’observer une confusion entre corrélation (ou relation) et causalité chez ceux qui ne comprennent pas bien la différence. Cela peut mener à des interprétations complètement erronées ! Comme ceci est à la fois crucial mais subtil, voici une vidéo issue de la série “les statistiques expliquées à mon chat” qui explique clairement le problème. Une troisième variable confondante peut en effet expliquer une corrélation, rendant alors la relation et/ou la causalité entre les deux variables fallacieuse… 1.1.3 Entraînement et confirmation En statistique, une règle universelle veut qu’une observation ne peut servir qu’une seule fois. Ainsi, toutes les données utilisées pour calculer le modèle ne peuvent pas servir simultanément à la confirmer. Il faut échantillonner d’autres valeurs pour effectuer cette confirmation. Il s’en suit une spécialisation des jeux de données en : jeu d’entraînement qui sert à établir le modèle jeu de confirmation ou de test qui sert à vérifier que le modèle est génaralisable car il est capable de prédire le comportement d’un autre jeu de données indépendant issu de la même population statistique. C’est une pratique cruciale de toujours confirmer son modèle, et donc, de prendre soin de séparer ses données en jeu d’entraînement et de test. Les bonnes façons de faire cela seront abordées au cours 3 dans la partie consacrée à l’apprentissage machine. Ici, nous nous focaliserons uniquement sur l’établissement du modèle dans la phase d’entraînement. Par conséquent, nous utiliserons toutes nos données pour cet entraînement, mais qu’il soit d’emblée bien clair qu’une confirmation du modèle est une seconde phase également indispensable. En biologie, le vivant peut être étudié essentiellement de deux manières complémentaires : par l’observation du monde qui nous entoure sans interférer, ou le moins possible, et par l’expérimentation où le biologiste fixe alors très précisément les conditions dans lesquelles il étudie ses organismes cibles. Les deux approches se prêtent à la modélisation mais seule l’expérimentation permet d’inférer avec certitude la causalité.↩ "],
["regression-lineaire-simple.html", "1.2 Régression linéaire simple", " 1.2 Régression linéaire simple Nous allons découvrir les bases de la régression linéaire de façon intuitive. Nous utilisons le jeu de données trees qui rassemble la mesure du diamètre, de la hauteur et du volume de bois de cerisiers noirs. # importation des données trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) Rapellons-nous que dans le chapitre 12 du livre science des données 1, nous avons étudié l’association de deux variables quantitatives (ou numériques). Nous utilisons donc une matrice de corrélation afin de mettre en évidence la corrélation entre nos trois variables qui composent le jeu de donnée trees. La fonction correlation() nous renvoie un tableau de la matrice de correlation avec l’indice de Pearson (corrélation linéaire) par défaut. C’est précisement ce coefficient qui nous intéresse dans le cadre d’une régression linéaire comme description préalable des données autant que pour nous guider dans le choix de nos variables. (trees_corr &lt;- correlation(trees)) # Matrix of Pearson&#39;s product-moment correlation: # (calculation uses everything) # diameter height volume # diameter 1.000 0.519 0.967 # height 0.519 1.000 0.597 # volume 0.967 0.597 1.000 Nous pouvons également observer cette matrice sous la forme d’un graphique plus convivial. plot(trees_corr, type = &quot;lower&quot;) Cependant, n’oubliez pas qu’il est indispensable de visualiser les nuages de points pour ne pas tomber dans le piège mis en avant par le jeu de données artificiel appelé “quartet d’Anscombe” qui montre très bien comment des données très différentes peuvent avoir même moyenne, même variance et même coefficient de corrélation. Un graphique de type matrice de nuages de points est tout indiqué ici. GGally::ggscatmat(as.data.frame(trees), 1:3) Nous observons une plus forte corrélation linéaire entre le volume et le diamètre. Intéressons nous à cette association. chart(trees, volume ~ diameter) + geom_point() Si vous deviez ajouter une droite permettant de représenter au mieux les données, où est ce que vous la placeriez ? Pour rappel, une droite respecte l’équation mathématique suivante : \\[y = a \\ x + b\\] dont a est la pente (slope en anglais) et b est l’ordonnée à l’origine (intercept en anglais). # Sélection de pentes et d&#39;ordonnées à l&#39;origine models &lt;- tibble( model = paste(&quot;model&quot;, 1:4, sep = &quot;-&quot;), slope = c(5, 5.5, 6, 0), intercept = c(-0.5, -0.95, -1.5, 0.85) ) chart(trees, volume ~ diameter) + geom_point() + geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + labs( color = &quot;Modèle&quot;) Nous avons quatre droites candidates pour représenter au mieux les observations. Quel est la meilleure d’entre elles selon vous ? 1.2.1 Quantifier l’ajustement d’un modèle Nous voulons identifier la meilleure régression, c’est-à-dire la régression le plus proche de nos données. Nous avons besoin d’une règle qui va nous permettre de quantifier la distance de nos observations à notre modèle afin d’obtenir la régression avec la plus faible distance possible de l’ensemble de nos observations. Décomposons le problème étape par étape et intéressons nous au model-1 (droite en rouge sur le graphique précédent). Calculer les valeurs de \\(y_i\\) prédites par le modèle que nous noterons par convention \\(\\hat y_i\\) (prononcez “y chapeau” ou “y hat” en anglais) pour chaque observation \\(i\\). # Calculer la valeur de y pour chaque valeur de x suivant le model souhaité # Création de notre fonction model &lt;- function(slope, intercept, x) { prediction &lt;- intercept + slope * x attributes(prediction) &lt;- NULL prediction } # Application de notre fonction yhat &lt;- model(slope = 5, intercept = -0.5, x = trees$diameter) # Affichage des résultats yhat # [1] 0.555 0.590 0.620 0.835 0.860 0.870 0.895 0.895 0.910 0.920 0.935 # [12] 0.950 0.950 0.985 1.025 1.140 1.140 1.190 1.240 1.255 1.280 1.305 # [23] 1.340 1.530 1.570 1.695 1.720 1.775 1.785 1.785 2.115 Calculer la distance entre les observations \\(y_i\\) et les prédictions par notre modèle \\(\\hat y_i\\), soit \\(y_i - \\hat y_i\\) Les distances que nous souhaitons calculer, sont appelées les résidus du modèle et sont notés \\(\\epsilon_i\\) (epsilon). Nous pouvons premièrement visualiser ces résidus graphiquement (ici en rouge par rapport à model-1) : Nous pouvons ensuite facilement calculer leurs valeurs comme ci-dessous : # Calculer la distance entre y et y barre # Création de notre fonction de calcul des résidus distance &lt;- function(observations, predictions) { residus &lt;- observations - predictions attributes(residus) &lt;- NULL residus } # Utilisation de la fonction resid &lt;- distance(observations = trees$volume, predictions = yhat) # Impression des résultats resid # [1] -0.263 -0.298 -0.331 -0.371 -0.328 -0.312 -0.453 -0.380 -0.270 -0.357 # [11] -0.250 -0.355 -0.344 -0.382 -0.484 -0.511 -0.183 -0.414 -0.512 -0.550 # [21] -0.303 -0.407 -0.312 -0.445 -0.364 -0.126 -0.143 -0.124 -0.327 -0.341 # [31] 0.065 Définir une règle pour obtenir une valeur unique qui résume l’ensemble des distances de nos observations par rapport aux prédictions du modèle. Une première idée serait de sommer l’ensemble de nos résidus comme ci-dessous : sum(resid) # [1] -10.175 Appliquons ces calculs sur nos quatre modèles afin de les comparer… Le modèle pour lequel notree critère serait le plus proche de zéro serait alors considéré comme le meilleur. model-1 model-2 model-3 model-4 -10.175 -1.441 10.393 0.135 Selon notre méthode, il en ressort que le modèle 4 est le plus approprié pour représenter au mieux nos données. Qu’en pensez vous ? Intuitivement, nous nous aperçevons que le modèle 4 est loin d’être le meilleur. Nous pouvons en déduire que la somme des résidus n’est pas un bon critère pour ajuster un modèle linéaire. Le problème lorsque nous sommons des résidus, est la présence de résidus positifs et de résidus négatifs (ici par rapport à model-4). Ainsi avec notre première méthode naïve de somme des résidus, il suffit d’avoir autant de résidus positifs que négatifs pour avoir un résultat proche de zéro. Mais cela n’implique pas que les observations soient prochent de la droite pour autant. Avez-vous une autre idée que de sommer les résidus ? Sommer le carré des résidus aurait des propriétés intéressantes car d’une part les carrés de nombres positifs et négatifs sont tous positifs, et d’autre part, plus une observation est éloignée plus sa distance au carré pèse fortement dans la somme2. Nous obtenons les résultats suivants : model-1 model-2 model-3 model-4 3.842211 0.4931095 3.929195 6.498511 Sommer les valeurs absolues des résidus mène également à des contributions toutes positives, mais sans pénaliser outre mesure les observations les plus éloignées. Nous obtenons les résultats suivants : model-1 model-2 model-3 model-4 10.305 3.186 10.393 11.525 Nous avons trouvé deux solutions intéressantes pour quantifier la distance de notre droite par rapport à nos observations. En effet, dans les deux cas, la valeur minimale est obtenue pour le model-2 (en vert sur le graphique) qui est visuellement le meilleur des quatre. La méthode utilisant les carrés des résidus s’appelle une régression par les moindres carrés. Notre objectif est donc de trouver les meilleures valeurs des paramètres \\(a\\) et \\(b\\) de la droite pour minimiser ce critère. Il en résulte une fonction dite objective qui dépend de \\(x\\) et de nos paramètres \\(a\\) et \\(b\\) à minimiser. Cette approche s’appelle la régression par les moindres carrés et elle est la plus utilisée. L’approche utilisant la somme de la valeur absolue des résidus est également utilisable (et elle est d’ailleurs préférable en présence de valeurs extrêmes potentiellement suspectes). Elle s’apppelle régression par la médiane, un cas particulier de la régression quantile, une approche intéressante dans le cas de non normalité des résidus et/ou de présence de valeurs extrêmes suspectes. 1.2.2 Trouver la meilleure droite Essayez de trouver le meilleur modèle par vous-même dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;01a_lin_mod&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/01a_lin_mod&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. Nous pouvons nous demander si notre modèle 2 qui est la meilleure droite de nos quatre modèles est le meilleur modèle possible dans l’absolu. Pour se faire nous allons devoir définir unez technique d’optimisation qui nous permet de déterminer quelle est la droite qui minimise notre fonction objective. Dans la suite, nous garderonsq le critère des moindres carrés (des résidus). Imaginons que nous n’avons pas quatre mais 5000 modèles linéaires avec des pentes et des ordonnées à l’origine différentes. Quelle est la meilleure droite ? set.seed(34643) models1 &lt;- tibble( intercept = runif(5000, -5, 4), slope = runif(5000, -5, 15)) chart(trees, volume ~ diameter) + geom_point() + geom_abline(aes(intercept = intercept, slope = slope), data = models1, alpha = 1/6) Nous voyons sur le graphique qu’un grand nombre de droites différentes sont testées, mais nous ne distinguons pas grand chose de plus. Cependant, sur ces 5000 modèles, nous pouvons maintenant calculer la somme des carrés des résidus et ensuite déterminer quel est le meilleur d’entre eux. # Fonction de calcul de la somme des carrés des résidus measure_distance &lt;- function(slope, intercept, x, y) { ybar &lt;- x * slope + intercept resid &lt;- y - ybar sum(resid^2) } # Test de la fonction #measure_distance(slope = 0, intercept = 0.85, x = trees$diameter, y = trees$volume) # Fonction adaptée pour être employé avec purrr:map() pour distribuer le calcul trees_dist &lt;- function(intercept, slope) { measure_distance(slope = slope, intercept = intercept, x = trees$diameter, y = trees$volume) } models1 &lt;- models1 %&gt;% mutate(dist = purrr::map2_dbl(intercept, slope, trees_dist)) Si nous réalisons un graphique de valeurs de pentes, d’ordonnées à l’origine et de la valeur de la fonction objective (distance) en couleur, nous obtenons le graphique ci-dessous. plot &lt;- chart(models1, slope ~ intercept %col=% dist) + geom_point() + geom_point(data = filter(models1, rank(dist) &lt;= 10), shape = 1, color = &#39;red&#39;, size = 3) + labs( y = &quot;Pente&quot;, x = &quot;Ordonnée à l&#39;origine&quot;, color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) plotly::ggplotly(plot) Les 10 valeurs les plus faibles sont mises en évidence sur le graphique par des cercles rouges. Le modèle optimal que nous recherchons se trouve dans cette région. best_models &lt;- models1 %&gt;.% filter(., rank(dist) &lt;= 10) Nous pouvons afficher les 10 meilleurs modèles sur notre graphique : chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + labs(color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) En résumé, nous avons besoin d’une fonction qui calcule la distance d’un modèle par rapport à nos observations et d’un algorithme pour la minimiser. Il n’est cependant pas nécessaire de chercher pendant des heures la meilleure fonction et le meilleur algorithme. Il existe dans R, un outil spécifiquement conçu pour adapter des modèles linéaires sur des observations, la fonction lm(). Dans le cas particulier de la régression par les moindres carrés, la solution s’obtient très facilement par un simple calcul : \\[a = \\frac{cov_{x, y}}{var_x} \\ \\ \\ \\textrm{et} \\ \\ \\ b = \\bar y - a \\ \\bar x\\] où \\(\\bar x\\) et \\(\\bar y\\) sont les moyennes pour les deux variables, \\(cov\\) est la covariance et \\(var\\) est la variance. Vous avez à votre disposition des snippets dédiés aux modèles linéaires (tapez ..., ensuite choisissez models, ensuite models : linear et choisissez le snippet qui vous convient dans la liste. (lm. &lt;- lm(data = trees, volume ~ diameter)) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Coefficients: # (Intercept) diameter # -1.047 5.652 Nous pouvons reporter ces valeurs sur notre graphique afin d’observer ce résultat par rapport à nos modèles aléatoires. nous avons en rouge la droite calculée par la fonction lm(). chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) 1.2.3 La fonction lm() Suite à notre découverte de manière intuitive de la régression linéaire simple, nous pouvons récapituler quelques points clés : Une droite suit l’équation mathématique suivante : \\[y = a \\ x + b\\] dont \\(a\\) est la pente (slope en anglais) et \\(b\\) est l’ordonnée à l’origine (intercept en anglais), tous deux les paramètres du modèle, alors que \\(x\\) et \\(y\\) en sont les variables. La distance entre une valeur observée \\(y_i\\) et une valeur prédite \\(\\hat y_i\\) se nomme le résidu (\\(\\epsilon_i\\)) et se mesure toujours parallèlement à l’axe \\(y\\). Cela revient à considérer que toute l’erreur du modèle se situe sur \\(y\\) et non sur \\(x\\). Cela donne l’équation complète de notre modèle statistique : \\[y_i = a \\ x_i + b + \\epsilon_i\\] avec \\(y_i\\) est la valeur mesurée pour le point i sur l’axe y, \\(a\\) est la pente, \\(x_i\\) est la valeur mesurée pour le point i sur l’axe x, b est l’ordonnée à l’origine et \\(\\epsilon_i\\) les résidus. On peut montrer (nous ne le ferons pas ici pour limiter les développements mathématiques) que le choix des moindres carrés des résidus comme fonction objective revient à considérer que nos résidus suivent une distribution normale centrée autour de zéro et avec un écart type \\(\\sigma\\) constant/ : \\[\\epsilon_i \\approx N(0, \\sigma)\\] Dans le cas de la régression linéaire simple, la meilleure droite s’obtient très facilement par la minimisation de la somme des carrés des résidus. En effet, la pente \\(a = \\frac{cov_{x, y}}{var_x}\\) et l’ordonnée à l’origine \\(b = \\bar y - a \\ \\bar x\\). La fonction lm() permet de faire ce calcul très facilement dans R. # Régression linéaire lm. &lt;- lm(data = trees, volume ~ diameter) # Graphique de nos observations et de la droite obtenue avec la fonction lm() chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) La fonction lm() crée un objet spécifique qui contient de nombreuses informations pour pouvoir ensuite analyser notre modèle linéaire. La fonction class() permet de mettre en avant la classe de notre objet. class(lm.) # [1] &quot;lm&quot; 1.2.4 Résumé avec summary() Avec la fonction summary() nous obtenons un résumé condensé des informations les plus utiles pour interpréter notre régression linéaire. summary(lm.) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.231211 -0.087021 0.003533 0.100594 0.271725 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.04748 0.09553 -10.96 7.85e-12 *** # diameter 5.65154 0.27649 20.44 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1206 on 29 degrees of freedom # Multiple R-squared: 0.9351, Adjusted R-squared: 0.9329 # F-statistic: 417.8 on 1 and 29 DF, p-value: &lt; 2.2e-16 Call : Il s’agit de la formule employée dans la fonction lm(). C’est à dire le volume en fonction du diamètre du jeu de données trees. Residuals : La tableau nous fournit un résumé via les 5 nombres de l’ensemble des résidus (que vous pouvez récupérer à partir de lm.$residuals). fivenum(lm.$residuals) # 20 7 12 9 31 # -0.231210947 -0.087020695 0.003532709 0.100594122 0.271724973 Coefficients : Il s’agit des résultats associés à la pente et à l’ordonnée à l’origine dont les valeurs estimées des paramètres (Estimate). Les mêmes valeurs peuvent être obtenues à partir de lm.$coefficients : lm.$coefficients # (Intercept) diameter # -1.047478 5.651535 On retrouve également les écart-types calculés sur ces valeurs (Std.Error) qui donnent une indication de la précision de leur estimation, les valeurs des distibutions de Student sur les valeurs estimées (t value) et enfin les valeurs p (PR(&gt;|t|)) liées à un test de Student pour déterminer si le paramètre p correspondant est significativement différent de zéro (avec \\(H_0: p = 0\\) et \\(H_a: p \\neq 0\\)). Pour l’instant, nous nous contenterons d’interpréter et d’utiliser les informations issues de summary() dans sa partie supérieure. Le contenu des trois dernières lignes sera détaillé dans le module suivant. A partir des données de ce résumé, nous pouvons maintenant paramétrer l’équation de notre modèle : \\[y = ax + b\\] devient3 : \\[volume \\ de \\ bois = 5.65 \\ diamètre \\ à \\ 1.4 \\ m - 1.05 \\] Utiliser le carré des résidus a aussi d’autres propriétés statistiques intéressantes qui rapprochent ce calcul de la variance (qui vaut la somme de la distance au carré à la moyenne pour une seule variables numérique).↩ Lors de la paramétrisation du modèle, pensez à arrondir la valeur des paramètres à un nombre de chiffres significatifs raisonnables. Inutile de garder 5, ou même 3 chiffres derrière la virgule si vous n’avez que quelques dizaines d’obserrvations pour ajuster votre modèle.↩ "],
["outils-de-diagnostic.html", "1.3 Outils de diagnostic", " 1.3 Outils de diagnostic Une fois la meilleure droite de régression obtenue, le travail est loin d’être terminé. Il se peut que le nuage de point ne soit pas tout-à-fait linéaire, que sa dispersion ne soit pas homogène, que les résidus n’aient pas une distribution normale, qu’il existe des valeurs extrêmes aberrantes, ou qui tirent la droite vers elle de manière excessive. Nous allons maintenant devoir diagnostiquer ces possibles problèmes. L’analyse des résidus permet de le faire. Ensuite, si deux ou plusieurs modèles sont utilisable, il nous faut décider lequel conserver. Enfin, nous pouvons aussi calculer et visualiser l’enveloppe de confiance du modèle et extraire une série de données de ce modèle. 1.3.1 Analyse des résidus Le tableau numérique obtenu à l’aide de summary() peut faire penser que l’étude d’une régression linéaire se limite à quelques valeurs numériques et divers tests d’hypothèses associés. C’est un premier pas, mais c’est oublier que la technique est essentiellement visuelle. Le graphique du nuage de points avec la droite superposée est un premier outil diagnostic visuel indispensable, mais il n’est pas le seul ! Plusieurs graphiques spécifiques existent pour mettre en évidence diverses propriétés des résidus qui peuvent révéler des problèmes. Leur inspection est indispensable et s’appelle l’analyse des résidus. Les différents graphiques sont faciles à obtenir à partir des snippets. Le premier de ces graphique permet de vérifier la distribution homogène des résidus. Dans une bonne régression, nous aurons une distribution équilibrée de part et d’autre du zéro sur l’axe Y. Que pensez-vous de notre graphique d’anayse des résidus ? Nous avons une valeur plus éloignée du zéro qui est mise en avant par la courbe en bleu qui montre l’influence générale des résidus. #plot(lm., which = 1) lm. %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) Le second graphique permet de vérifier la normalité des résidus (comparaison par graphique quantile-quantile à une distribution normale). #plot(lm., which = 2) lm. %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) Le troisième graphique va standardiser les résidus, et surtout, en prendre la racine carrée. Cela a pour effet de superposer les résidus négatifs sur les résidus positifs. Nous y diagnostiquons beaucoup plus facilement des problèmes de distribution de ces résidus. A nouveau, nous pouvons observer qu’une valeur influence fortement la régression. #plot(lm., which = 3) lm. %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) Le quatrième graphique met en évidence l’influence des individus sur la régression linéaire. Effectivement, la régression linéaire est sensible aux valeurs extrêmes. Nous pouvons observer que nous avons une valeur qui influence fortement notre régression. On utilise pour ce faire la distance de Cook que nous ne détaillerons pas dans le cadre de ce cours. #plot(lm., which = 4) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Le cinquième graphique utilise l’effet de levier (Leverage) qui met également en avant l’influence des individus sur notre régression. Il répond à la question suivante : “est-ce qu’un ou plusieurs points sont tellement influents qu’ils tirent la régression vers eux de manière abusive ?” Nous avons à nouveau une valeur qui influence fortement notre modèle. #plot(lm., which = 5) lm. %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) Le sixième graphique met en relation la distance de Cooks et l’effet de levier. Notre unique point d’une valeur supérieur à 0.5 m de diamètre influence fortement notre modèle. #plot(lm., which = 6) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii] / (1 - h[ii]))) A l’issue de l’analyse des résidus, nous abservons donc différents problèmes qui suggèrent que le modèle choisi n’est peut être pas le plus adapté. Nous comprendrons pourquoi plus loin. A vous de jouer Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01b_reg_lin&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Pièges et astuces : extrapolation Notre régression linéaire a été réalisée sur des cerisiers noirs dont le diamètre est compris entre 0.211 et 0.523 mètre. Pensez vous qu’il soit acceptable de prédire des volumes de bois pour des arbres dont le diamètre est inférieur ou supérieur à nos valeurs minimales et maximales mesurées (extrapolation) ? Utilisons notre régression linéaire afin de prédire 10 volumes de bois à partir d’arbre dont le diamètre varie entre 0.1 et 0.8m. new &lt;- data.frame(diameter = seq(0.1, 0.7, length.out = 8)) Ajoutons une variable pred qui contient les prédictions en volume de bois. Observez-vous un problème particulier sur base du tableau ci-dessous ? new %&gt;.% modelr::add_predictions(., lm.) -&gt; new new # diameter pred # 1 0.1000000 -0.482324424 # 2 0.1857143 0.002092891 # 3 0.2714286 0.486510206 # 4 0.3571429 0.970927522 # 5 0.4428571 1.455344837 # 6 0.5285714 1.939762152 # 7 0.6142857 2.424179468 # 8 0.7000000 2.908596783 Il est peut-être plus simple de voir le problème sur un nuage de points. Pour un diamètre de 0.1857143 m de diamètre, le volume de bois est de 0 mis en avant par l’intersection des lignes pointillées bleues. chart(trees, volume~diameter) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) + geom_vline(xintercept = new$diameter[2], linetype = &quot;twodash&quot;, color = &quot;blue&quot;) + geom_hline(yintercept = new$pred[2], linetype = &quot;twodash&quot;, color = &quot;blue&quot;) + geom_point() + geom_abline(aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2])) + geom_point(data = new, f_aes(pred~diameter), color = &quot;red&quot;) Le volume de bois prédit est négatif ! Notre modèle est-il alors complètement faux ? Rappelons-nous qu’un modèle est nécessairement une vision simplifiée de la réalité. En particulier, notre modèle a été entraîné avec des données comprises dans un intervalle. Il est alors valable pour effectuer des interpolations à l’intérieur de cet intervalle, mais ne peut pas être utilisé pour effectuer des extrapolations en dehors, comme nous venons de le faire. trees %&gt;.% modelr::add_predictions(., lm.) -&gt; trees chart(trees, volume~diameter) + geom_point() + geom_line(f_aes(pred ~ diameter)) Pièges et astuces : significativité fortuite Gardez toujours à l’esprit qu’il est possible que votre jeu de données donne une régression significative, mais purement fortuite. Les données supplémentaires de test devraient alors démasquer le problème. D’où l’importance de vérifier/valider votre modèle. Le principe de parcimonie veut que l’on ne teste pas toutes les combinaisons possibles deux à deux des variables d’un gros jeu de données, mais que l’on restreigne les explorations à des relations qui ont un sens biologique afin de minimiser le risque d’obtenir une telle régression de manière fortuite. 1.3.2 Enveloppe de confiance De même que l’on peut définir un intervalle de confiance dans lequel la moyenne d’un échantillon se situe avec une probabilité donnée, il est aussi possible de calculer et de tracer une enveloppe de confiance qui indique la région dans laquelle le “vrai” modèle se trouve avec une probabilité donnée (généralement, on choisi cette probabilité à 95%). Voici ce que cela donne : lm. %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x))(.) Cette enveloppe de confiance est en réalité basée sur l’écart type conditionnel (écart type de \\(y\\) sachant quelle est la valeur de \\(x\\)) qui se calcule comme suit : \\[s_{y|x}\\ =\\ \\sqrt{ \\frac{\\sum_{i = 0}^n\\left(y_i - \\hat y_i\\right)^2}{n-2}}\\] A partir de là, il est possible de définir également un intervalle de confiance conditionnel à \\(x\\) : \\[CI_{1-\\alpha}\\ =\\ \\hat y_i\\ \\ \\pm \\ t_{\\frac{\\alpha}{2}}^{n-2} \\frac{s_{y|x}\\ }{\\sqrt{n}}\\] C’est cet intervalle de confiance conditionnel qui est matérialisé par l’enveloppe de confiance autour de la droite de régression représentée sur le graphique. 1.3.3 Extraire les données d’un modèle La fonction tidy() du package broom extrait facilement et rapidement sous la forme d’un tableau différentes valeurs associées à votre régression linéaire. (DF &lt;- broom::tidy(lm.)) # # A tibble: 2 x 5 # term estimate std.error statistic p.value # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 (Intercept) -1.05 0.0955 -11.0 7.85e-12 # 2 diameter 5.65 0.276 20.4 9.09e-19 Pour extraire facilement et rapidement sous la forme d’un tableau de données les paramètres de votre modèle vouys pouvez aussi utiliser la fonction glance(). (DF &lt;- broom::glance(lm.)) # # A tibble: 1 x 11 # r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.935 0.933 0.121 418. 9.09e-19 2 22.6 -39.2 -34.9 # # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Vous avez des snippets à votre disposition pour ces deux fonctions : ... -&gt; models ..m -&gt; .models tools .mt -&gt; .mtmoddf ou encore ... -&gt; models ..m -&gt; .models tools .mt -&gt; .mtpardf A vous de jouer Vous avez à votre disposition la première assignation GitHub Classroom : https://classroom.github.com/a/bvqsukEO "],
["lm2.html", "Module 2 Régression linéaire II", " Module 2 Régression linéaire II Objectifs Savoir utiliser les outils de diagnostic de la régression linéaire correctement, en particulier l’analyse des résidus. Appréhender les différentes formes de régressions linéaires par les moindres carrés. Choisir sa régression linéaire de manière judicieuse. Prérequis Le module précédent est une entrée en matière indispensable qui est complétée par le contenu du présent module. "],
["outils-de-diagnostic-suite.html", "2.1 Outils de diagnostic (suite)", " 2.1 Outils de diagnostic (suite) La régression linéaire est une matière complexe et de nombreux outils existent pour vous aider à déterminer si le modèle que vous ajustez tient la route ou non. Il est très important de le vérifier avant d’utiliser un modèle. Ajuster un modèle quelconque dans des données est à la portée de tout le monde, mais choisir un modèle pertinent et pouvoir expliquer pourquoi est nettement plus difficile ! 2.1.1 Résumé avec summary()(suite) Reprenons la sortie renvoyée par summary() appliqué à un objet lm. trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) lm. &lt;- lm(data = trees, volume ~ diameter) summary(lm.) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.231211 -0.087021 0.003533 0.100594 0.271725 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.04748 0.09553 -10.96 7.85e-12 *** # diameter 5.65154 0.27649 20.44 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1206 on 29 degrees of freedom # Multiple R-squared: 0.9351, Adjusted R-squared: 0.9329 # F-statistic: 417.8 on 1 and 29 DF, p-value: &lt; 2.2e-16 Nous n’avons pas encore étudié la signification des trois dernières lignes de ce résumé. Voici de quoi il s’agit. Residual standard error : Il s’agit de l’écart-type résiduel, considérant que les degrés de liberté du modèle est le nombre d’observations \\(n\\) (ici 31) soustrait du nombre de paramètres à estimer (ici 2, la pente et l’ordonnée à l’origine de la droite). C’est donc une mesure globale de l’importance (c’est-à-dire de l’étendue) des résidus de manière générale. \\[\\sqrt{\\frac{\\sum(y_i - ŷ_i)^2}{n-2}}\\] Multiple R-squared : Il s’agit de la valeur du coefficient de détermination du modèle noté R^2 de manière générale ou r2 dans le cas d’une régression linéaire simple. Il exprime la fraction de variance exprimée par le modèle. Autrement dit, le R2 quantifie la capacité du modèle à prédire la valeur de \\(y\\) connaissant la valeur \\(x\\) pour le même individu. C’est dons une indication du pouvoir prédictif de notre modèle autant que de sa qualité d’ajustement (goodness-of-fit en anglais). Souvenons-nous que la variance totale respecte la propiété d’additivité. La variance est composée au numérateur d’une somme de carrés, et au dénominateur de degrés de liberté. La somme des carrés totaux (de la variance) peut elle-même être décomposée en une fraction expliquée par notre modèle, et la fraction qui ne l’est pas (les résidus) : \\[SC(total) = SC(rég) + SC(résidus)\\] avec : \\[SC(total) = \\sum_{i=0}^n(y_i - \\bar y_i)^2\\] \\[SC(rég) = \\sum_{i=0}^n(ŷ_i - \\bar y_i)^2\\] \\[SC(résidus) = \\sum_{i=0}^n(y_i - ŷ_i)^2\\] A partir de la décomposition de ces sommes de carrés, le coefficient R2 (ou r2) se définit comme : \\[R^2 = \\frac{SC(rég)}{SC(total)} = 1 - \\frac{SC(résidus)}{SC(total)}\\] La valeur du R2 est comprise entre 0 (lorsque le modèle est très mauvais et n’explique rien) et 1 (lorsque le modèle est parfait et “capture” toute la variance des données ; dans ce cas, tous les résidus valent zéro). Donc, plus le coefficient R2 se rapproche de un, plus le modèle explique bien les données et aura un bon pouvoir de prédiction. Dans R, le R2 multiple se réfère simplement au R2 (ou au r2 pour les régressions linéaires simples) calculé de cette façon. L’adjectif multiple indique simplement que le calcul est valable pour une régression multiple telle que nous verrons plus loin. Par contre, le terme au dénominateur considère en fait la somme des carrés totale par rapport à un modèle de référence lorsque la variable dépendante \\(y\\) ne dépend pas de la ou des variables indépendantes \\(x_i\\). Les équations indiquées plus haut sont valables lorsque l’ordonnée à l’origine n’est pas figée (\\(y = a \\ x + b\\)). Dans ce cas, la valeur de référence pour \\(y\\) est bien sa moyenne, \\(\\bar y\\). D’un autre côté, si l’ordonnée à l’origine est fixée à zéro dans le modèle simplifié \\(y = a \\ x\\) (avec \\(b = 0\\) obtenu en indiquant la formule y ~ x + 0 ou y ~ x - 1), alors le zéro sur l’axe \\(y\\) est considéré comme une valeur appartenant d’office au modèle et devient valeur de référence. Ainsi, dans les équations ci-dessus il faut remplacer \\(\\bar y\\) par 0 partout. Le R2 est alors calculé différemment, et sa valeur peut brusquement augmenter si le nuage de points est très éloigné du zéro sur l’axe y. Ne comparez donc jamais les R2 obtenus avec et sans forçage à zéro de l’ordonnée à l’origine ! Adjusted R-squared : La valeur du coefficient R2 ajustée, noté \\(\\bar{R^2}\\) n’est pas utile dans le cadre de la régression linéaire simple, mais est indispensable avec la régression multiple. En effet, à chaque fois que vous rendez votre modèle plus complexe en ajoutant une ou plusieurs variables indépendantes, le modèle s’ajustera de mieux en mieux dans les données, même par pur hasard. C’est un phénomène que l’on appelle l’inflation du R2. A la limite, si nous ajoutons une nouvelle variable fortement corrélée avec les précédentes4, l’apport en terme d’information nouvelle sera négligeable, mais le R2 augmentera malgré tout un tout petit peu. Alors dans quel cas l’ajout d’une nouvelle variable est-il pertinent ou non ? Le R2 ajusté apporte l’information désirée ici. Sa valeur n’augmentera pour l’ajout d’un nouveau prédicteur que si l’ajustement est meilleur que ce que l’on obtiendrait par le pur hasard. Le R2 ajusté se calcule comme suit (il n’est pas nécessaire de retenir cette formule, mais juste de constater que l’ajustement fait intervenir p, le nombre de paramètres du modèle et n, la taille de l’échantillon) : \\[ \\bar{R^2} = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1} \\] F-statistic : Tout comme pour l’ANOVA, le test de la significativité de la régression car \\(MS(rég)/MS(résidus)\\) suit une distribution F à respectivement 1 et \\(n-2\\) degré de liberté, avec \\(MS\\) les carrés moyens, c’est-à-dire les sommes des carrés \\(SC\\) divisés par leurs degrés de liberté respectifs. p-value : Il s’agit de la valeur p associé à la statistique de F, donc à l’ANOVA associée à la régression linéaire. Pour cette ANOVA particulière, l’hypothèse nulle est que la droite n’apporte pas plus d’explication des valeurs de y à partir des valeurs de x que la valeur moyenne de y (ou zéro, dans le cas paerticulier d’un modèle dont l’ordonnée à l’origine est forcé à zéro). L’hypothèse alternative est donc que le modèle est significatif au seuil \\(\\alpha\\) considéré. Donc, notre objectif est de rejetter H0 pour cet test ANOVA pour que le modèle ait un sens (valeur p plus petite quez le seuil \\(\\alpha\\) choisi). Le tableau complet de l’ANOVA associée au modèle peut aussi être obtenu à l’aide de la fonction anova() : anova(lm.) # Analysis of Variance Table # # Response: volume # Df Sum Sq Mean Sq F value Pr(&gt;F) # diameter 1 6.0762 6.0762 417.8 &lt; 2.2e-16 *** # Residuals 29 0.4218 0.0145 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On y retrouve les mêmes informations, fortement résumées en une ligne à la fin de la sortie de summary(), mais ici sous une forme plus classique de tableau de l’analyse de la variance. 2.1.2 Comparaison de régressions Vous pouvez à présent comparer ces résultats avec un tableau et les six graphiques d’analyse des résidus sans la valeur supérieure à 0.5m de diamètre. Attention, On ne peut supprimer une valeur sans raison valable. La suppression de points aberrants doit en principe être faite avant de débuter l’analyse. La raison de la suppression de ce point est liée au fait qu’il soit seul et unique point supérieur à 0.5m de diamètre. Nous le faisons ici à titre de comparaison. trees_red &lt;- filter(trees, diameter &lt; 0.5) lm1 &lt;- lm(data = trees_red, volume ~ diameter) chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) + geom_abline( aes(intercept = lm1$coefficients[1], slope = lm1$coefficients[2]), color = &quot;blue&quot;, size = 1.5) La droite en bleu correspond à la régression sans utiliser l’arbre de diamètre supérieur à 0,5m. Tentez d’analyser le tableau de notre régression en bleu (astuce : comparez avec ce que la régeression précédente donnait). summary(lm1) # # Call: # lm(formula = volume ~ diameter, data = trees_red) # # Residuals: # Min 1Q Median 3Q Max # -0.215129 -0.068502 -0.001149 0.070522 0.181398 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -0.94445 0.09309 -10.15 6.98e-11 *** # diameter 5.31219 0.27540 19.29 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1082 on 28 degrees of freedom # Multiple R-squared: 0.93, Adjusted R-squared: 0.9275 # F-statistic: 372.1 on 1 and 28 DF, p-value: &lt; 2.2e-16 Tentez d’analyser également les graphiques d’analyse des résidus ci-dessous. #plot(lm1, which = 1) lm1 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm1, which = 2) lm1 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm1, which = 3) lm1 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm1, which = 4) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) #plot(lm1, which = 5) lm1 %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) #plot(lm1, which = 6) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii] / (1 - h[ii]))) Au travers de cet exemple, nous constatons que la comparaison de modèles, dans le but de choisir le meilleur est un travail utile. Cela apparaitra d’autant plus utile que la situation va passablement se complexifier (dans le bon sens) avec l’introduction de la régression multiple et polynomiale ci-dessous. Heureusement, nous terminerons ce module avec la découverte d’une métrique qui va nous permettre d’effectuer le choix du meilleur modèle de manière fiable : le critère d’Akaike. A vous de jouer ! Réalisez une nouvelle assignation individuelle : Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/jkh3ruyX La corrélation entre les prédicteurs dans un modèle linéaire multiple est un gros problème et doit être évité le plus possible. Cela s’appelle la colinéarité ou encore multicollinéairité. Ainsi, il est toujours préférable de choisir un ensemble de variables indépendantes peu corrélées entre elles dans un même modèle, mais ce n’est pas toujours possible.↩ "],
["regression-lineaire-multiple.html", "2.2 Régression linéaire multiple", " 2.2 Régression linéaire multiple Dans le cas de la régression linéaire simple, nous considèrions le modèle stqatistique suivant (avec \\(\\epsilon\\) représentant les résidus, terme statistique dans l’équation) : \\[y = a \\ x + b + \\epsilon \\] Dans le cas de la régression, nous introduirons plusieurs variables indépendantes notés \\(x_1\\), \\(x_2\\), …, \\(x_n\\) : \\[y = a_1 \\ x_1 + a_2 \\ x_2 + ... + a_n \\ x_n + b + \\epsilon \\] La bonne nouvelle, c’est que tous les calculs, les métriques et les tests d’hypothèses relatifs à la régression linéaire simple se généraliser simplement et naturellement, tout comme nous sommes passés dans le cours SDD 1 de l’ANOVA à 1 facteur à un modèle plus complexe à 2 ou plusoieurs facteurs. Voyons tout de suite ce que cela donne si nous voulions utiliser à la fois le diamètree et la hauteur des cerisiers noirs pour prédire leur volume de bois : summary(lm2 &lt;- lm(data = trees, volume ~ diameter + height)) # # Call: # lm(formula = volume ~ diameter + height, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.180423 -0.074919 -0.006874 0.062244 0.241801 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.63563 0.24462 -6.686 2.95e-07 *** # diameter 5.25643 0.29594 17.762 &lt; 2e-16 *** # height 0.03112 0.01209 2.574 0.0156 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1104 on 28 degrees of freedom # Multiple R-squared: 0.9475, Adjusted R-squared: 0.9438 # F-statistic: 252.7 on 2 and 28 DF, p-value: &lt; 2.2e-16 D’un point de vue pratique, nous voyons que la formule qui spécifie le modèle peut très bien comporter plusieurs variables séparées par des +. Nous avons ici trois paramètres dans notre modèle : l’ordonnée à l’origine qui vaut -1,63, la pente relative au diamètre de 5,25, et la pente relative à la hauteur de 0,031. Le modèle lm2 sera donc paramétré comme suit : volume de bois = 5,25 . diamètre + 0,031 . hauteur - 1,63. Notons que la pente relative à la hauteur (0,031) n’est pas significativement différente de zéro au seuil \\(\\alpha\\) de 5% (mais l’est seulement pour \\(\\alpha\\) = 1%). En effet, la valeur t du test de Student associé (H0 : le paramètre vaut zéro, H1 : le paramètre est différent de zéro) vaut 2,574. Cela correspond à une valeur p du test de 0,0156, une valeur moyennement significative donc, matérialisée par une seule astérisque à la droite du tableau. Cela dénote un plus faible pouvoir de prédiction du volume de bois via la hauteur que via le diamètre de l’arbre. Nous l’avions déjà observé sur le graphique matrice de nuages de points réalisé initialement, ainsi que via les coefficients de correlation respectifs. La représentation de cette régression nécessite un graphique à trois dimensions (diamètre, hauteur et volume) et le modèle représente en fait le meilleur plan dans cet espace à 3 dimensions. Pour un modèle comportant plus de deux variables indépendantes, il n’est plus possible de représenter graphiquement la régression. library(rgl) knitr::knit_hooks$set(webgl = hook_webgl) car::scatter3d(data = trees, volume ~ diameter + height, fit = &quot;linear&quot;, residuals = TRUE, bg = &quot;white&quot;, axis.scales = TRUE, grid = TRUE, ellipsoid = FALSE) # Loading required namespace: mgcv rgl::rglwidget(width = 800, height = 800) Utilisez la souris pour zoomer (molette) et pour retourner le graphique (cliquez et déplacer la souris en maintenant le bouton enfoncé) pour comprendre ce graphique 3D. La régression est matérialisée par un plan en bleu. Les observations sont les boules jaunes et les résidus sont des traits cyans lorsqu’ils sont positifs et magenta lorsqu’ils sont négatifs. Les graphes d’analyse des résidus sont toujours disponibles (nous ne représentons ici que les quatre premiers) : #plot(lm2, which = 1) lm2 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm2, which = 2) lm2 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm2, which = 3) lm2 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm2, which = 4) lm2 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Est-ce que ce modèle est préférable à celui n’utilisant que le diamètre ? Le R2 ajusté est passé de 0,933 avec le modèle simple lm. utilisant uniquement le diamètre à 0,944 dans le présent modèle lm2 utilisant le diamètre et la hauteur. Cela semble une amélioration, mais le test de significativité de la pente pour la hauteur ne nous indique pas un résultat très significatif. De plus, cela a un coût en pratique de devoir mesurer deux variables au lieu d’une seule pour estimer le volume de bois. Cela en vaut-il la peine ? Nous sommes encore une fois confrontés à la question de comparer deux modèles, cette fois-ci ayant une complexité croissante. Dans le cas particulier de modèles imbriqués (un modèle contient l’autre, mais rajoute un ou plusieurs termes), une ANOVA est possible en décomposant la variance selon les composantes reprises respectivement par chacun des deux modèles. La fonction anova() est programmée pour faire ce calcul en lui indiquant chacun des deux objets contenant les modèles à comparer : anova(lm., lm2) # Analysis of Variance Table # # Model 1: volume ~ diameter # Model 2: volume ~ diameter + height # Res.Df RSS Df Sum of Sq F Pr(&gt;F) # 1 29 0.42176 # 2 28 0.34104 1 0.080721 6.6274 0.01562 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notez que dans le cas de l’ajout d’un seul terme, la valeur p de cette ANOVA est identique à la valeur p de test de significativité du paramètre (ici, cette valeur p est de 0,0156 dans les deux cas). Donc, le choix peut se faire directement à partir de summary() pour ce terme unique. La conclusion est similaire : l’ANOVA donne un résultat seulement moyennent significatif entre les 2 modèles. Dans un cas plus complexe, la fonction anova() de comparaison pourra être utile. Enfin, tous les modèles ne sont pas nécessairement imbriqués. Dans ce cas, il nous faudra un autre moyen de les départager, … mais avant d’aborder cela, étudions une variante intéressante de la régression multiple : la régression polynomiale. A vous de jouer ! Réalisez une nouvelle assignation individuelle : Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;02a_reg_multi&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["regression-lineaire-polynomiale.html", "2.3 Régression linéaire polynomiale", " 2.3 Régression linéaire polynomiale Pour rappel, un polynome est une expression mathématique du type (notez la ressemblance avec l’équation de la régression multiple) : \\[ a_0 + a_1 . x + a_2 . x^2 + ... + a_n . x^n \\] Un polynome d’ordre 2 (terme jusqu’au \\(x^2\\)) correspond à une parabole dans le plan xy. Que se passe-t-il si nous calculons une variable diametre2 qui est le carré du diamètre et que nous prétendons faire une régression multiple en utilisant à la fois diamètre et diamètre2/ ? trees %&gt;.% mutate(., diameter2 = diameter^2) -&gt; trees summary(lm(data = trees, volume ~ diameter + diameter2)) # # Call: # lm(formula = volume ~ diameter + diameter2, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # diameter2 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 Il semble que R ait pu réaliser cette analyse. Cette fois-ci, nous n’avons cependant pas une droite ou un plan ajusté, mais par ce subterfuge, nous avons pu ajuster une courbe dans les données ! Nous pourrions augmenter le degré du polynome (ajouter un terme en diameter^3, voire encore des puissance supérieures). Dans ce cas, nous obtiendrons une courbe de plus en plus flexible, toujours dans le plan xy. Ceci illustre parfaitement d’ailleurs l’ambiguité de la complexité du modèle qui s’ajuste de mieux en mieux dans les données, mais qui ce faisant, perd également progressivement son pouvoir explicatif. En effet, on sait qu’il existe toujours une droite qui passe entre deux points dans le plan. De même, il existe toujours une parabole qui passe par 3 points quelconques dans le plan. Et par extension, il existe une courbe correspondant à un polynome d’ordre n - 1 qui passe par n’importe quel ensemble de n points dans le plan. Un modèle construit à l’aide d’un tel polynome aura toujours un R2 égal à un, … mais en même temps ce modèle ne sera d’aucune utilité car il ne contient plus aucune information pertinente. C’est ce qu’on appelle le surajustement (overfitting en anglais). La figure ci-dessous (issue d’un article écrit par Anup Bhande ici) illuste bien ce phénomène. Devoir calculer les différentes puissance des variables au préalable devient rapidement fastidieux. Heureusement, R autorise de glisser ce cacul directement dans la formule, mais à condition de lui indiquer qu’il ne s’agit pas du nom d’une variable diameter^2, mais d’un calcul effectué sur diameter en utilisant la fonction, d’identité I(). Ainsi, sans rien calculer au préalable, nous pouvons utiliser la formule volume ~ diameter + I(diameter^2). Un snippet est d’ailleurs disponible pour ajuster un polynome d’ordre 2 ou d’ordre 3, et il est accompagné du code nécessaire pour représenter également graphiquement cette régression polynomiale. Le code ci-dessous qui construit le modèle lm3 l’utilise. summary(lm3 &lt;- lm(data = trees, volume ~ diameter + I(diameter^2))) # # Call: # lm(formula = volume ~ diameter + I(diameter^2), data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # I(diameter^2) 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 lm3 %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2)))(.) Remarquez sur le graphique comment, à présent, la courbe s’ajuste bien mieux dans le nuage de point et comme l’arbre le plus grand avec un diamètre supérieur à 0,5m est à présent presque parfaitement ajusté par le modèle. Faites donc très attention que des points influents ou extrêmes peuvent apparaitre également comme tel à cause d’un mauvais choix de modèle ! L’analyse des résidus nous montre aussi un comportement plus sain. #plot(lm3, which = 1) lm3 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm3, which = 2) lm3 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm3, which = 3) lm3 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm3, which = 4) lm3 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Revenons un instant sur le résumé de ce modèle. summary(lm3) # # Call: # lm(formula = volume ~ diameter + I(diameter^2), data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # I(diameter^2) 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 La pente relative au diameter nécessite quelques éléments d’explication. En effet, que signifie une pente pour une courbe dont la dérivée première (“pente locale”) change constamment ? En fait, il faut comprendre ce paramètre comme étant la pente de la courbe au point x = 0. Si le modèle est très nettement significatif (ANOVA, valeur p &lt;&lt;&lt; 0,001), et si le R2 ajusté grimpe maintenant à 0,959, seul le paramètre relatif au diamètre2 est significatif cette fois-ci. Ce résultat suggère que ce modèle pourrait êtrte simplifié en considérant que l’ordonnée à l’origine et la pente pour le terme diameter valent zéro. Cela peut être tenté, mais à condition de refaire l’analyse. On ne peut jamais laisser tomber un paramètre dans une analyse et considérer que les autres sont utilisable tels quels. tous les paramètres caculés sont interconnectés. Voyons ce que cela donne (la formule devient volume ~ I(diameter^2) - 1 ou volume ~ I(diameter^2) + 0, ce qui est identique) : summary(lm4 &lt;- lm(data = trees, volume ~ I(diameter^2) - 1)) # # Call: # lm(formula = volume ~ I(diameter^2) - 1, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.19474 -0.07234 -0.04120 0.04522 0.18240 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # I(diameter^2) 7.3031 0.1397 52.26 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1027 on 30 degrees of freedom # Multiple R-squared: 0.9891, Adjusted R-squared: 0.9888 # F-statistic: 2731 on 1 and 30 DF, p-value: &lt; 2.2e-16 Notez bien que quand on réajuste un modèle simplifié, les paramètres restants doivent être recalculés. En effet, le paramètre relatif au diamètre2 vallait 11,2 dans le modèle lm3 plus haut. Un fois les autres termes éliminés, ce paramètre devient 7,30 dans ce modèle lm4 simplifié. Le modèle lm4 revient aussi (autre point de vue) à transformer d’abord le diamètre en diamètre2 et à effectuer ensuite une régression linéaire simple entre deux variables, volume et diametre2 : summary(lm4bis &lt;- lm(data = trees, volume ~ diameter2 - 1)) # # Call: # lm(formula = volume ~ diameter2 - 1, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.19474 -0.07234 -0.04120 0.04522 0.18240 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # diameter2 7.3031 0.1397 52.26 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1027 on 30 degrees of freedom # Multiple R-squared: 0.9891, Adjusted R-squared: 0.9888 # F-statistic: 2731 on 1 and 30 DF, p-value: &lt; 2.2e-16 Notez qu’on obtient bien évidemment exactement les mêmes résultats si nous transformons d’abord les données ou si nous intégrons le calcul à l’intérieur de la formule qui décrit le modèle. Faites bien attention de ne pas comparer le R2 acvec ordonnée à l’origine fixée à zéro ici dans notre modèle lm4 avec les R2 des modèles lm. ou lm3 qui ont ce paramètre estimé. Rappelez-vous que le R2 est calculé différemment dans les deux cas ! Donc, nous voilà une fois de plus face à un nouveau modèle pour lequel il nous est difficile de décider s’il est meilleur que les précédents. Avant de comparer, élaborons un tout dernier modèle, le plus complexe, qui reprend à la fois notre régression polynomiale d’ordre 2 sur le diamètre et la hauteur. Autrement dit, une régression à la fois multiple et polynomiale. summary(lm5 &lt;- lm(data = trees, volume ~ diameter + I(diameter^2) + height)) # # Call: # lm(formula = volume ~ diameter + I(diameter^2) + height, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.12169 -0.04806 -0.00237 0.05156 0.12559 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -0.267097 0.284895 -0.938 0.356798 # diameter -3.281421 1.463401 -2.242 0.033354 * # I(diameter^2) 11.891724 2.019252 5.889 2.83e-06 *** # height 0.034788 0.008169 4.259 0.000223 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.07436 on 27 degrees of freedom # Multiple R-squared: 0.977, Adjusted R-squared: 0.9745 # F-statistic: 382.8 on 3 and 27 DF, p-value: &lt; 2.2e-16 Ah ha, ceci est bizarre ! Le R2 ajusté nous indique que le modèle serait très bon puisqu’il grimpe à 0,975. Le terme en diamètre2 reste très significatif, … mais la pente relative à la hauteur est maintenant elle aussi très significative alors que dans le modèle multiple lm2 ce n’était pas le cas. De plus, la pente à l’origine en face du diamètre semble devenir un peu plus significative. Bienvenue dans les instabilités liées aux intercorrelations entre paramètres dans les modèles linéaires complexes. "],
["rmse-critere-dakaike.html", "2.4 RMSE &amp; critère d’Akaike", " 2.4 RMSE &amp; critère d’Akaike Le R2 (ajusté) n’est pas la seule mesure d’ajustement d’un modèle. Il existe d’autres indicateurs. Par exemple, l’erreur quadratique moyenne, (root mean square error, ou RMSE en anglais) est la racine carrée de la moyenne des résidus au carré. Elle représente en quelque sorte la distance “typique” des résidus. Comme cette distance est exprimée dans les mêmes unités que l’axe y, cette mesure est particulièrement parlante. Nous pouvons l’obtenir par exemple comme ceci : modelr::rmse(lm., trees) # [1] 0.1166409 Cela signifie que l’on peut s’attendre à ce que, en moyenne, les valeurs prédites de volume de bois s’écartent (dans un sens ou dans l’autre) de 0,117 m3 de la valeur effectivement observée. Evidemment, plus un modèle est bon, plus le RMSE est faible, contrairement au R2 qui lui doit être élevé. Si le R2 comme le RMSE sont utiles pour quantifier la qualité d’ajustement d’une régression, ces mesures sont peu adaptées pour la comparaison de modèles entre eux. En effet, nous avons vu que plus le modèle est complexe, mieux il s’ajuste dans les données. Le R2 ajusté tente de remédier partiellement à ce problème, mais cette métrique reste peu fiable pour comparer des modèles très différents. Le critère d’Akaike, du nom du statisticien japonais qui l’a conçu, est une métrique plus adaptée à de telles comparaisons. Elle se base au départ sur encore une autre mesure de la qualité d’ajustement d’un modèle : la log-vraisemblance. Les explications relatives à cette mesure sont obligatoirement complexes d’un point de vue mathématique et nous vous proposons ici d’en retenir la définition sur un plan purement conceptuel. Un estimateur de maximum de vraisemblance est une mesure qui permet d’inférer le meilleur ajustement possible d’une loi de probabilité par rapport à des données. Dans le cas de la régression par les moindres carrés, la distribution de probabilité à ajuster est celle des résidus (pour rappel, il s’agit d’une distribution Normale de moyenne nulle et d’écart type constant \\(\\sigma\\)). La log-vraisemblance, pour des raisons purement techniques est souvent préféré au maximum de vraissemblance. Il s’agit simplement du logarithme de sa valeur. Donc, plus la log-vraisemblance est grande, mieux les données sont compatibles avec le modèle probabiliste considéré. Pour un même jeu de données, ces valeurs sont comparables entre elles… même pour des modèles très différents. Mais cela ne règle pas la question de la complexité du modèle. C’est ici qu’Akaike entre en piste. Il propose le critère suivant : \\[ \\textrm{AIC} = -2 . \\textrm{log-vraisemblance} + 2 . \\textrm{nbrpar} \\] où nbrpar est le nombre de paramètres à estimer dans le modèle. Donc ici, nous prenons comme point de départ moins deux fois la log-vraisemblance, une valeur a priori à minimiser, mais nous lui ajoutons le second terme de pénalisation en fonction de la complexité du modèle valant 2 fois le nombre de paramètres du modèle. Notons d’ailleurs que le terme multiplicateur 2 ici est modifiable. Si nous voulons un modèle le moins complexe possible, nous pourrions très bien multiplier par 3 ou 4 pour pénaliser encore plus. Et si nous voulons être moins restrictifs, nous pouvons aussi diminuer ce facteur multiplicatif. Dans la pratique, le facteur 2 est quand même très majoritairement adapté par les praticiens, mais la possibilité de changer l’impact de complexité du modèle est inclue dans le calcul de facto. Dès lors que ce critère peut être calculé (et R le fait pour pratiquement tous les modèles qu’il propose), une comparaison est possible avec pour objectif de sélectionner le, ou un des modèles qui a l’AIC la plus faible. N’oubliez toutefois pas de comparer visuellement les différents modèles ajustés et d’interpréter les graphiques d’analyse des résidus respectifs en plus des valeurs d’AIC. C’est l’ensemble de ces outils qui vous orientent vers le meilleur modèle, pas l’AIC seul ! Calculons maintenant les critères d’Akaike pour nos 6 modèles lm. à lm5… AIC(lm.) # Linéaire diamètre # [1] -39.24246 AIC(lm2) # Multiple diamètre et hauteur # [1] -43.82811 AIC(lm3) # Polynomial diamètre # [1] -53.50964 AIC(lm4) # Diamètre^2 # [1] -50.15027 AIC(lm5) # Multiple et polynomial # [1] -67.4391 D’après ce critère, le modèle linéaire est le moins bon, et le dernier modèle le plus complexe serait le meilleur. Notez toutefois que la différence est relativement minime (en regard du gain total) entre le modèle polynomial complet lm3 et la version simplifié au seul terme diamètre2 en lm4, ce qui permet de penser que cette simplification est justifiée. Dans l’hypothèse où nous déciderions de conserver le modèle lm5, en voici l’analyse des résidus qui est bonne dans l’ensemble : #plot(lm5, which = 1) lm5 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm5, which = 2) lm5 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm5, which = 3) lm5 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm5, which = 4) lm5 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Naturellement, même si c’est le cas ici, ce n’est pas toujours le modèle le plus complexe qui “gagne” toujours. Même ici, nous pourrions nous demander si le modèle polynomial utilisant uniquement le diamètre ne serait pas plus intéressant en pratique car son ajustement est tout de même relativement bon (même si son critère d’Akaike est nettement moins en sa faveur), mais d’un point de vue pratique, il nous dispense de devoir mesurer la hauteur des arbres pour prédire le volume de bois. Ce n’est peut-être pas négligeable comme gain, pour une erreur de prédiction légèrement supérieure si on compare les valeurs de RMSE. modelr::rmse(lm5, trees) # Multiple et polynomial # [1] 0.06939391 modelr::rmse(lm3, trees) # Polynomial diamètre # [1] 0.08972287 L’erreur moyenne d’estimation du volume de bois passe de 0,07 m3 pour le modèle le plus complexe lm5 utilisant à la fois le diamètre et la hauteur à 0,09 m3. C’est à l’exploitant qu’il appartient de déterminer si le gain de précision vaut la peine de devoir effectuer deux mesures au lieu d’une seule. Mais au moins, nous sommes capables, en qualité de scientifiques des données, de lui proposer les alternatives possible et d’en quantifier les effets respectifs. Différentes méthodes d’ajustement par xkcd. A vous de jouer ! Après cette longue lecture avec énormément de nouvelles matières, nous vous proposons les exercices suivants : Répondez aux questions d’un learnr afin de vérifier vos acquis. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;02b_reg_poly&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Réalisez un carnet de laboratoire sur la biométrie des oursins avec l’assignation ci-dessous. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/5hI-HSOv Réalisez un rapport scientifique sur la croissance des escargots géants d’Afrique. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/_wJZDbNp "],
["mod-lineaire.html", "Module 3 Modèle linéaire", " Module 3 Modèle linéaire Objectifs Comprendre le modèle linéaire (ANOVA et régression linéaire tout en un) Appréhender la logique des matrices de contraste Découvrir l’ANCOVA Comprendre le mécanisme du modèle linéaire généralisé Prérequis L’ANOVA (modules 10 &amp; 11 du cours SDD 1), ainsi que la régression linéaires (modules 1 et 2 du présent cours) doivent être maitrisés avant d’aborder cette matière. "],
["variables-numeriques-ou-facteurs.html", "3.1 Variables numériques ou facteurs", " 3.1 Variables numériques ou facteurs L’ANOVA analyse une variable dépendante numérique en fonction d’une ou plusieurs variables indépendantes qualitatives. Ces variables sont dites “facteurs” non ordonnés (objets de classe factor), ou “facteurs” ordonnés (objets de classe ordered) dans R. La régression linéaire analyse une variable dépendante numérique en fonction d’une ou plusieurs variables indépendantes numérique (quantitatives) également. Ce sont des objets de classe numeric (ou éventuellement integer, mais assimilé à numeric concrètement) dans R. Donc, la principale différence entre ANOVA et régression linéaire telles que nous les avnos abordés jusqu’ici réside dans la nature de la ou des variables indépendantes, c’est-à-dire, leur type. Pour rappel, il existe deux grandes catégories de variables : quantitatives et qualitatives, et deux sous-catégories pour chacune d’elle. Cela donne quatyre types principaux de variables, formant plus de 90% des cas rencontrés : variables quantitatives continues représentables par des nombres réels (numeric dans R), variables quantitatives discrètes pour des dénombrements d’événements finis par exemple, et représentables par des nombres entiers (integer dans R), variables qualitatives ordonnées pour des variables prenant un petit nombre de valeurs, mais pouvant être ordonnées de la plus petite à la plus grande (ordered dans R), variables qualitatives non ordonnées prenant également un petit nombre de valeurs possibles, mais sans ordre particulier (factor dans R). Par la suite, un encodage correct des variables sera indispensable afin de distinguer correctement ces différentes situations. En effet, R considèrera automatiquement comment mener l’analyse en fonction de la classe des variables fournies. Donc, si la classe est incorrecte, l’analyse le sera aussi ! Si vous avez des doutes concernant les types de variables, relisez la section type de variables avant de continuer ici. "],
["anova-et-regression-lineaire.html", "3.2 ANOVA et régression linéaire", " 3.2 ANOVA et régression linéaire Avez-vous remarqué une ressemblance particulière entre la régression linéaire que nous avons réalisé précédement et l’analyse de variance (ANOVA) ? Les plus observateurs auront mis en avant que la fonction de base dans R est la même dans les deux cas : lm(). Cette fonction est donc capable de traiter aussi bien des variables réponses qualitatives que quantitatives, et effectue alors une ANOVA dans un cas ou une régression linéaire dans l’autre. Par ailleurs, nous avons vu que l’ANOVA et la régression linéaire se représentent par des modèles semblables : \\(y = \\mu + \\tau_i + \\epsilon\\) pour l’ANOVA et \\(y = \\beta_1 + \\beta_2 x + \\epsilon\\) pour la régression linéaire, avec \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\) dans les deux cas. Donc, nous retrouvons bien au niveau du modèle mathématique sous-jacent la différence principale entre les deux qui réside dans le type de variable indépendante (ou explicative) : Variable qualitative pour l’ANOVA, Variable quantitative pour la régression linéaire. Le calcul est, en réalité, identique en interne. Il est donc possible de généraliser ces deux approches en une seule appelée modèle linéaire, mais à condition d’utiliser une astuce pour modifier nos modèles afin qu’ils soient intercompatibles. 3.2.1 Modèle linéaire commun Le nœud du problème revient donc à transformer nos modèles mathématiques pour qu’ils puissent être fusionnés en un seul. Comment homogénéiser ces deux modèles ? \\(y = \\mu + \\tau_i + \\epsilon\\) pour l’ANOVA et \\(y = \\beta_1 + \\beta_2 x + \\epsilon\\) pour la régression linéaire. Avant de poursuivre, réfléchisser un peu par vous-même. Quelles sont les différences qu’il faut contourner ? Est-il possible d’effectuer une ou plusieurs transformations des variables pour qu’elles se comportent de manière similaire dans les deux cas ? 3.2.2 Réencodage des variables de l’ANOVA Considérons dans un premier temps, un cas très simple : une ANOVA à un facteur avec une variable indépendante qualitative (factor) à deux niveaux5. Nous pouvons écrire : \\[ y = \\mu + \\tau_1 I_1 + \\tau_2 I_2 + \\epsilon \\] avec \\(I_i\\), une variable dite indicatrice créée de toute pièce qui prend la valeur 1 lorsque le niveau correspond à i, et 0 dans tous les autres cas. Vous pouvez vérifier par vous-même que l’équation ci-dessus fonctionnera exactement de la même manière que le modèle utilisé jusqu’ici pour l’ANOVA. En effet, poiur un individu de la population 1, \\(I_1\\) vaut 1 et \\(\\tau_1\\) est utilisé, alors que comme \\(I_2\\) vaut 0, \\(\\tau_2\\) est annulé dans l’équation car \\(\\tau_2 I_2\\) vaut également 0. Et c’est exactement l’inverse qui se produit pour un individu de la population 2, de sorte que c’est \\(\\tau_2\\) qui est utilisé cette fois-ci. Notez que notre nouvelle formulation, à l’aide de variables indicatrices ressemble fortement à la régression linéaire. La seule différence par rapport à cette dernière est que nos variables \\(I_i\\) ne peuvent prendre que des valeurs 0 ou 1 (en tous cas, pour l’instant), alors que les \\(x_i\\) dans la régression linéaire multiple sont des variables quantitatives qui peuvent prendre une infinité de valeurs différentes (nombres réels). Nouys pouvons encore réécrire notre équation comme suit pour qu’elle se rapproche encore plus de celle de la régression linéaire simple. Passons par l’introduction de deux termes identiques \\(\\tau_1 I_2\\) additionné et soustrait, ce qui revient au même qu’en leur absence : \\[ y = \\mu + \\tau_1 I_1 + \\tau_1 I_2 - \\tau_1 I_2 + \\tau_2 I_2 + \\epsilon \\] En considérant \\(\\beta_2 = \\tau_2 - \\tau_1\\), cela donne : \\[ y = \\mu + \\tau_1 I_1 + \\tau_1 I_2 + \\beta_2 I_2 + \\epsilon \\] En considérant \\(\\beta_1 = \\mu + \\tau_1 = \\mu + \\tau_1 I_1 + \\tau_1 I_2\\) (car quelle que soit la population à laquelle notre individu appartient, il n’y a jamais qu’une seule des deux valeurs \\(I_1\\) ou \\(I_2\\) non nulle et dans tous les cas le résultat est donc égal à \\(\\tau_1\\)), on obtient : \\[ y = \\beta_1 + \\beta_2 I_2 + \\epsilon \\] Cette dernière formulation est strictement équivalente au modèle de la régression linéaire simple dans laquelle la variable \\(x\\) a simplement été remplacée par notre variable indicatrice \\(I_2\\). Ceci se généralise pour une variable indépendante à \\(k\\) niveaux, avec \\(k - 1\\) variables indicatrices au final. En prenant soin de réencoder le modèle de l’ANOVA relatif aux variables indépendantes qualitatives, nous pouvons à présent mélanger les termes des deux modèles en un seul : notre fameux modèle linéaire. Nous aurons donc, quelque chose du genre (avec les \\(x_i\\) correspondant aux variables quantitatives et les \\(I_j\\) des variables indicatrices pour les différents niveaux des variables qualitatives) : \\[ y = \\beta_1 + \\beta_2 x_1 + \\beta_3 x_2 + ... + \\beta_n I_1 + \\beta_{n+1} I_2 ... + \\epsilon \\] Concrètement, un cas aussi simple se traite habituellement à l’aide d’un test t de Student, mais pour notre démonstration, nous allons considérer ici utiliser une ANOVA à un facteur plutôt.↩ "],
["matrice-de-contraste.html", "3.3 Matrice de contraste", " 3.3 Matrice de contraste La version que nous avons étudié jusqu’ici pour nos variables indicatrices, à savoir, une seule prend la valeur 1 lorsque toutes les autres prend une valeur zéro, n’est qu’un cas particulier de ce qu’on appelle les contrastes appliqués à ces variables indicatrices. En réalité, nous pouvons leurs donner bien d’autres valeurs (on parle de poids), et cela permettra de considérer dses contrastes différents, eux-mêmes représentatifs de situations différentes. Afin de mieux comprendre les contrastes appliqués à nos modèles linéaires, les statisticiens ont inventé les matrices de contrastes. Ce sont des tableaux à deux entrées indiquant pour chaque niveau de la variable indépendante qualitative quelles sont les valeurs utilisées pour les différentes variables indicatrices présentées en colonne. Dans le cas de notre version simplifiée du modèle mathématique où nous avons fait disparaitre \\(I_1\\) en l’assimilant à la moyenne \\(\\mu\\) pour obteniur \\(\\beta_1\\). Dans le cas où notre variable qualitative a quatre niveaux, nous avons donc le modèle suivant : \\[ y = \\beta_1 + \\beta_2 I_2 + \\beta_3 I_3 + \\beta_4 I_4 + \\epsilon \\] Cela revient à considérer le premier niveau comme niveau de référence et à établir tous les contrastes par rapport à ce niveau de référence. C’est une situation que l’on rencontre fréquemment lorsque nos testons l’effet de différents médicaments ou de différents traitement par rapport à un contrôle (pas de traitement, ou placébo). La matrice de contrastes correspondante, dans un cas où on aurait trois traitements en plus du contrôle (donc, notre variable factor à quatre niveaux) s’obteint facilement dans R à l’aide de la fonction contr.treatment() : contr.treatment(4) # 2 3 4 # 1 0 0 0 # 2 1 0 0 # 3 0 1 0 # 4 0 0 1 Les lignes de cette matrice sobnt numérotées de 1 à 4. Elles correspondent aux quatres niveaux de notre variable factor, avec le niveau 1 qui doit nécessairement correspondre à la situation de référtence, donc au contrôle. Les colonnes de cette matrice correspondent aux trois variables indicatrices \\(I_1\\), \\(I_2\\) et \\(I_3\\) de l’équation au dessus. Nous voyons que pour une individu contrôle, de niveau 1, les trois \\(I_i\\) prennent la valeur 0. Nous sommes bien dans la situation de référence. En d’autres terme, le modèle de base est ajusté sur la moyenne des individus contrôle. Notre modèle se réduit à : \\(y = \\beta_1 + \\epsilon\\). Donc, seule la moyenne des individus contrôles, \\(\\beta_1\\) est considérée, en plus des résidus \\(\\epsilon\\) bien sûr. Pour le niveau deux, nous observons que \\(I_2\\) vaut 1 et les deux autres \\(I_i\\) valent 0. Donc, cela revient à considérer un décalage constant \\(\\beta_2\\) appliqué par rapport au modèle de référence matérialisé par \\(\\beta_1\\). En effezt, notre équation se réduit dans ce cas à : \\(y = \\beta_1 + \\beta_2 + \\epsilon\\). Le même raisonnement peut être fait pour les niveaux 3 et 4, avec des décalages constants par rapport à la situation cxontrôle de respectivement \\(\\beta_3\\) et \\(\\beta_4\\). En d’autres termes, les contrastes qui sont construits ici font tous référence au contrôle, et chaque médicament est explicitement comparté au contrôle (mais les médicaments ne sont pas comparés entre eux). Nous voyons donc que les variables indicatrices etr la matrice de contrastes permet de spécifier quelles sont les contrastes pertinents et éliminent ceux qui ne le sont pas (nous n’utilisons donc pas systématiquement toutes les comparaisons deux à deux des différents niveaux6). 3.3.1 Contraste orthogonaux Les contrastes doivent être de préférence orthogonaux par rapport à l’ordonnée à l’origine, ce qui signifie que la somme de leurs pondérations doit être nulle pour tous les contrastes définis (donc, en colonnes). Bien que n’étant pas obligatoire, cela confère des propriétés intéressantes au modèle (l’explication et la démonstration sortent du cadre de ce cours). Or, les contrastes de type traitement ne sont pas orthogonaux puisque toutes les sommes par colonnes vaut un. 3.3.2 Autres matrices de contrastes courantes Somme à zéro. Ces constraste, toujours pour une variable à quatre niveaux, se définissen t comme suit en utilisant la fonction contr.sum() dans R : contr.sum(4) # [,1] [,2] [,3] # 1 1 0 0 # 2 0 1 0 # 3 0 0 1 # 4 -1 -1 -1 Ici nous avons bien des contrastes orthogonaux puisque toutes les sommes par colonnes valeur zéro. Dans le cas présent, aucun niveau n’est considéré comme référence, mais les n - 1 niveaux sont systématiquement contrastés avec le dernier et nîème^ niveau. Ainsi, un contraste entre deux niveaux particuliers peut s’élaborer en indiquant une pondération de 1 pour le premier niveau à comparer, une pondération de -1 pour le second à comparer et une pondération de 0 pour tous les autres. Matrice de contrastes de Helmert : chaque niveau est comparé à la moyenne des niveaux précédents. La matrice de constrastes correspondant pour une variable à quatre niveaux s’obtient à l’aide de la fonction R contr.helmert() : contr.helmert(4) # [,1] [,2] [,3] # 1 -1 -1 -1 # 2 1 -1 -1 # 3 0 2 -1 # 4 0 0 3 Cette matrice est également orthogonale avec toutes les sommes par colonnes qui valent zéro. Ici, nous découvrons qu’il est possible de créer un contrastye entre un niveau et la moyenne de plusieurs autres niveaux en mettant le poids du premier à m (le nombre de populations à comparer de l’autre côté du contraste), et les poids des autres populations tous à -1. Ainsi, la colonne 4 compare le niveau quatre avec pondération 3 aux trois autres niveaux qui reçoivent tous une pondération -1. Matrice de contrastes polynomiaux : adapté aux facteurs ordonnés (ordered dans R) pourvlesquels on s’attend à une certaine évolution du modèle du niveau le plus petit au plus grand. Donc ici aussi une comparaison deux à deux de tous les niveaux n’est pas souhaitable, mais une progression d’un effet qui se propage de manière graduelle du plus petit niveau au plus grand. A priori cela parait difficile à métérialiser dans une matrice de contraste… et pourtant, c’est parfaitement possible ! Il s’agit de constrastes polynomiaux où nous ajustons de polynomes de degré croissant comme pondération des différents contrastes étudiés. La fonction contr.poly() permet d’obtenir ce type de contraste dans R. Pour une variable ordonnée à quatre niveaux, cela donne : contr.poly(4) # .L .Q .C # [1,] -0.6708204 0.5 -0.2236068 # [2,] -0.2236068 -0.5 0.6708204 # [3,] 0.2236068 -0.5 -0.6708204 # [4,] 0.6708204 0.5 0.2236068 Ici, les pondérations sont plus difficiles à expliquer rien qu’en observant la matrice de contrastes. De plus, les colonnes portent ici des noms particuliers .L pour un contraste linéaire (polynome d’ordre 1), .Q pour un contraste quadratique (polynome d’ordre 2), et .C pour un contraste conique (ou polynome d’ordre 3). Les pondérations appliquées se comprennent mieux lorsqu’on augmente le nombre de niveaux etr que l’on représente graphiquement la valeur des pondérations choisées. Par exemple, pour une variable facteur ordonnée à dix niveaux, nous représentrons graphiquement les 3 premeirs contrastes (linéaire, quadratique et conique) comme suit : plot(contr.poly(10)[, 1], type = &quot;b&quot;) plot(contr.poly(10)[, 2], type = &quot;b&quot;) plot(contr.poly(10)[, 3], type = &quot;b&quot;) Sur le graphique, l’axe X nommé index correspiond en réalité à la succession des 10 niveaux de la variable présentés dans l’ordre du plus petit au plus grand. Nous voyons maintenant clairement comment les contrastes sont construits ici. Pour le conbtraste linéaire, on contraste les petits niveaux avec les grands, et ce, de manière proportionnelle par rapport à la progression d’un niveau à l’autre (polynome d’ordre un = droite). Pour le contraste quadratique, on place “dans le même sac” les petits et greand niveaux qui sont contrastés avec les niveaux moyens (nous avons une parabole ou polynome d’ordre 2). Pour le troisième graphique, la situation se complexifie en encore un peu plus avec un polynome d’ordre 3, et ainsi de suite pour des polynomes d’ordres croissants jusqu’à remplir complètement la matrice de contrastes. R utilise par défaut des contrastes de traitement pour les facteurs non ordonnés et des contrastes polynomiaux pour des facteurs ordonnés. Ces valeurs par défaut sont stockées dans l’option contrasts que l’on peut lire à l’aide de getOption(). Bien sûr, il est possible de changer ces contrastes, tant au niveau global qu’au niveau de la construction d’un modèle en particulier. getOption(&quot;contrasts&quot;) # unordered ordered # &quot;contr.treatment&quot; &quot;contr.poly&quot; Attention : le fait d’utiliser une matrtice de contraste qui restreint ceux utilisés dans le modèle est indépendant des tests post hoc de comparaisons multiples, qui restent utilisables par après. Les comparaisons deux à deux des médicaments restent donc accessibles, mais ils ne sont tout simplement pas mis en évidence dans le modèle de base.↩ "],
["ancova.html", "3.4 ANCOVA", " 3.4 ANCOVA Avant l’apparition du modèle linéaire, une version particulière d’un mélange de régression linéaire et d’une ANOVA avec une variable indépendante quantitative et une autre variable indépendante qualitative s’appelait une ANCOVA (ANalyse de la COVariance). Un tel modèle d’ANCOVA peut naturellement également se résoudre à l’aide de la fonction lm() qui, en outre, peut faire bien plus. Nous allons maintenant ajuster un tel modèle à titre de première application concrète de tout ce que nous venons de voir sur le modèle linéaire et sur les matrices de contrastes associées. 3.4.1 Bébés à la naissance Nous étudions la masse de nouveaux nés en fonction du poids de la mère et du fait qu’elle fume ou non. Nous avons donc ici une variable dépendante wt, la masse des bébés qui est quantitative, et deux variables indépendantes ou prédictives wt1, la masse de la mère, et smoke le fait que la mère fume ou non. Or la première de ces variables explicatives est quantitative (wt1) et l’autre (smoke) est une variable facteur à quatre niveaux (0 = la mère n’a jamais fumé, 1 = elle fume y compris pendant la grossesse, 2 = elle fumait mais a arrêté à la grossesses, et 3 = la mère a fumé, mais a arrêté, et ce, bien avant la grossesse. Un dernier niveau 9 = inconnu encode de manière non orthodoxe les valeurs manquantes dans notre tableau de données (valeurs que nous éliminerons). De même les masses des nouveaux nés et des mères sont des des unités impériales (américaines) respectivement en “onces” et en “livres”. Enfin, nous devons prendre soin de bien encoder la variable smoke comme une variable factor (ici nous ne considèrerons pas qu’il s’agit d’un facteur ordonné et nous voulons faire un contraste de type traitement avec comparaison à des mères qui n’ont jamais fumé). Un reminement soigneux des données est donc nécessaire avant de pouvoir appliquer notre modèle ! SciViews::R babies &lt;- read(&quot;babies&quot;, package = &quot;UsingR&quot;) knitr::kable(head(babies)) id pluralty outcome date gestation sex wt parity race age ed ht wt1 drace dage ded dht dwt marital inc smoke time number 15 5 1 1411 284 1 120 1 8 27 5 62 100 8 31 5 65 110 1 1 0 0 0 20 5 1 1499 282 1 113 2 0 33 5 64 135 0 38 5 70 148 1 4 0 0 0 58 5 1 1576 279 1 128 1 0 28 2 64 115 5 32 1 99 999 1 2 1 1 1 61 5 1 1504 999 1 123 2 0 36 5 69 190 3 43 4 68 197 1 8 3 5 5 72 5 1 1425 282 1 108 1 0 23 5 67 125 0 24 5 99 999 1 1 1 1 5 100 5 1 1673 286 1 136 4 0 25 2 62 93 3 28 2 64 130 1 4 2 2 2 Ce tableau est “brut de décoffrage”. Voyez help(&quot;babies&quot;, package = &quot;UsingR&quot;) pour de plus amples informations. Nous allons maintenant remanier tout cela correctement. # wt = masse du bébé à la naissance en onces et 999 = valeur manquante # wt1 = masse de la mère à la naissance en livres et 999 = valeur manquante # smoke = 0 (non), = 1 (oui), = 2 (jusqu&#39;à grossesse), # = 3 (plus depuis un certain temps) and = 9 (inconnu) babies %&gt;.% select(., wt, wt1, smoke) %&gt;.% # Garder seulement wt, wt1 &amp; smoke filter(., wt1 &lt; 999, wt &lt; 999, smoke &lt; 9) %&gt;.% # Eliminer les valeurs manquantes mutate(., wt = wt * 0.02835) %&gt;.% # Transformer le poids en kg mutate(., wt1 = wt1 * 0.4536) %&gt;.% # Idem mutate(., smoke = as.factor(smoke)) -&gt; # S&#39;assurer d&#39;avoir une variable factor Babies # Enregistrer le résultat dans Babies knitr::kable(head(Babies)) wt wt1 smoke 3.40200 45.3600 0 3.20355 61.2360 0 3.62880 52.1640 1 3.48705 86.1840 3 3.06180 56.7000 1 3.85560 42.1848 2 Description des données : skimr::skim(Babies) # Skim summary statistics # n obs: 1190 # n variables: 3 # # ── Variable type:factor ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts # smoke 0 1190 1190 4 0: 531, 1: 465, 3: 102, 2: 92 # ordered # FALSE # # ── Variable type:numeric ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # wt 0 1190 1190 3.39 0.52 1.56 3.06 3.4 3.71 4.99 # wt1 0 1190 1190 58.3 9.49 39.46 51.82 56.7 62.6 113.4 # hist # ▁▁▂▆▇▅▁▁ # ▂▇▆▂▁▁▁▁ chart(data = Babies, wt ~ wt1 %col=% smoke) + geom_point() + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt ~ smoke) + geom_boxplot() + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt1 ~ smoke) + geom_boxplot() + ylab(&quot;Masse de la mère [kg]&quot;) Visuellement, nous ne voyons pas d’effet marquant. Peut-être la condition 1 de smoke (mère qui fume pendant la grossesse) mène-t-il à des bébés moins gros, mais est-ce significatif ? Pour cela, ajustons notre modèle ANCOVA avec matrice traitement (choix par défaut pour une la variable factor smoke). Comme nous savons déjà utiliser lm(), c’est très simple. Cela fonctionne exactement comme avant7. # ANCOVA Babies_lm &lt;- lm(data = Babies, wt ~ smoke * wt1) summary(Babies_lm) # # Call: # lm(formula = wt ~ smoke * wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.9568 -0.3105 0.0133 0.3136 1.4989 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.000663 0.128333 23.382 &lt; 2e-16 *** # smoke1 -0.303614 0.196930 -1.542 0.123405 # smoke2 0.901888 0.371393 2.428 0.015314 * # smoke3 -0.035502 0.371379 -0.096 0.923858 # wt1 0.008117 0.002149 3.777 0.000167 *** # smoke1:wt1 0.001153 0.003346 0.345 0.730444 # smoke2:wt1 -0.015340 0.006390 -2.401 0.016523 * # smoke3:wt1 0.001177 0.006147 0.191 0.848258 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4992 on 1182 degrees of freedom # Multiple R-squared: 0.08248, Adjusted R-squared: 0.07705 # F-statistic: 15.18 on 7 and 1182 DF, p-value: &lt; 2.2e-16 anova(Babies_lm) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.9636 1.158e-15 *** # wt1 1 6.162 6.1621 24.7325 7.559e-07 *** # smoke:wt1 3 1.653 0.5511 2.2117 0.08507 . # Residuals 1182 294.497 0.2492 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 L’analyse de variance montre que la masse de la mère a un effet significatif au seuil alpha de 5%, de même si la mère fume. Par contre, il n’y a pas d’interactions entre les deux. Le fait de pouvoir meurer des interactions entre variables qualitatives et quantitatives est ici bien évidemment un plus du modèle linéaire par rapport à ce qu’on pouvait faire avant ! Le résumé de l’analyse nous montre que la régression de la masse des bébés en fonction de la masse de la mère (ligne wt1 dans le tableau des coefficients), bien qu’étant significative, n’explique que 8% de la variance totale (le \\(R^2\\)). Les termes smoke1, smoke2 et smoke3 sont les contrastes appliqués par rapport au contrôle (smoke == 0). On voit ici qu’aucun de ces contrastes n’est significatif au seuil alpha de 5%. Cela signifie que le seul effet significatif est celui lié à une ordonnée à l’origine non nulle (Intercept) matérialisant la condition smoke == 0. Cela signifie que des mères de masse nulle n’ayant jamais fumé engendreraient des bébés pesant environ 3kg. Dans le contexte présent, cette constatation n’a bien sûr aucun sens, et l’interprétation de l’ordonnée à l’origine ne doit pas être faite. Donc, le modèle linéaire, en offrant plus de contrôle dans notre ajustement et une définition de contrastes “utiles” matérialisés par les lignes smoke1, smoke2 et smoke3 du tableau nous permet de faire des tests plus utiles dans le contexte de notre analyse. N’oublions pas non plus la possibilité de déterminer si des interactions entre smoke et wt1 existent pour ces différents contrastes, interactions testées respectivements aux lignes smoke1:wt1, smoke2:wt1, et smoke3:wt1du tableau des coefficients. Dans le cas présent, aucune de ces interactions n’est siginificative au seuil alpha de 5%. Pour comprendre à quoi tout cela fait référence, il faut considérer le modèle de base comme une droite de régression ajustée entre wt et wt1 pour la population de référence smoke == 0. Ainsi, si nous faisons : summary(lm(data = Babies, wt ~ wt1, subset = smoke == 0)) # # Call: # lm(formula = wt ~ wt1, data = Babies, subset = smoke == 0) # # Residuals: # Min 1Q Median 3Q Max # -1.95685 -0.25825 0.01476 0.25464 1.49890 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.000663 0.123572 24.283 &lt; 2e-16 *** # wt1 0.008117 0.002069 3.922 9.92e-05 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4806 on 529 degrees of freedom # Multiple R-squared: 0.02826, Adjusted R-squared: 0.02642 # F-statistic: 15.38 on 1 and 529 DF, p-value: 9.924e-05 Nous voyons en effet que les pentes et ordonnées à l’origine sont ici parfaitement identiques au modèle ANCOVA complet (mais pas les tests associés). Maintenant plus difficile : à quoi correspond une régression entre wt et wt1 pour smoke == 1 ? summary(lm(data = Babies, wt ~ wt1, subset = smoke == 1)) # # Call: # lm(formula = wt ~ wt1, data = Babies, subset = smoke == 1) # # Residuals: # Min 1Q Median 3Q Max # -1.70870 -0.35089 0.01034 0.33576 1.39420 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.697048 0.153270 17.597 &lt; 2e-16 *** # wt1 0.009270 0.002632 3.522 0.000471 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.5122 on 463 degrees of freedom # Multiple R-squared: 0.02609, Adjusted R-squared: 0.02399 # F-statistic: 12.4 on 1 and 463 DF, p-value: 0.0004711 Nous avons une ordonnées à l’origine qui vaut 2,70 ici. Notons que cela correspond aussi à (Intercept) + smoke1 = 3,00 - 0,30 = 2,70. Donc, l’ordonnées à l’origine pour smoke == 1 est bien la valeur de référence additionnée de la valeur fournie à la ligne smoke1 dans l’ANCOVA. Cela se vérifie aussi pour les deux autres droites pour smoke2 et smoke3. Maintenant, la pente pour notre droite ajustée sur la population smoke == 1 uniquement vaut 0,00927. Dans l’ANCOVA, nous avions une pente wt1 de 0,00812 et une interaction smoke1:wt1 claculée comme 0,00115. Notez alors que la pente de la droite seule 0,00927 = 0,00812 + 0,00115. Donc, tout comme smoke1 correspond au décalage de l’ordonnée à l’origine du modèle de référence, les interactions smoke1:wt1 correspondent au décalage de la pente par rapport au modèle de référence. Cela se vérifie également pour smoke2:wt1 et smoke3:wt1. Donc, notre modèle complet ne fait rien d’autre que d’ajuster les quatre droites correspondant aux relations linéaires entre wt et wt1, mais en décompose les effets, niveau par niveau de la variable qualitative smoke en fonction de la matrice de contraste que l’on a choisie. En bonnus, nous avons la possibilité de tester si chacune des composantes (tableau coefficient de summary()) ou si globalement chacune des variables (tableau obtenu avec anova()) a un effet significatif ou non dans le modèle. Le graphique correspondant est le même que si nous avions ajusté les 4 régressions linéaires indépendamment l’une de l’autre (mais les tests et les enveloppes de confiance diffèrent). chart(data = Babies, wt ~ wt1 %col=% smoke) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt ~ wt1 | smoke) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Comme toujours, lorsqu’un effet n’est pas siugnificatif, nous pouvons décider de simplifier le modèle. Mais attention ! Toujours considérer que les composantes sont interdépendantes. Donc, éliminer une composante du modèle peut avoir des effets parfois surprenants sur les autres. Voyons ce que cela donne si nous éliminons les interactions. Dans ce cas, nous ajustons des droites toutes parallèles avec uniquement un décalage de leur ordonnée à l’origine matérialisé par smoke1, smoke2 et smoke3 par rapport au modèle de référence ajusté pour la population smoke == 0 (notez l’utilisation, du signe + dans la formuile, là où nous utilisions le signe * dans la modèle précédent). # ANCOVA Babies_lm2 &lt;- lm(data = Babies, wt ~ smoke + wt1) summary(Babies_lm2) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.030052 0.092861 32.630 &lt; 2e-16 *** # smoke1 -0.237938 0.031816 -7.478 1.46e-13 *** # smoke2 0.022666 0.056508 0.401 0.688 # smoke3 0.035486 0.054068 0.656 0.512 # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 anova(Babies_lm2) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.887 1.285e-15 *** # wt1 1 6.162 6.1621 24.657 7.853e-07 *** # Residuals 1185 296.150 0.2499 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Hé, ça c’est intéressant ! Maintenant que nous avons éliminé les interactions qui apparaissent non pertinentes ici, nous avons toujours une régression significative entre wt et wt1 (mais avec un \\(R^2\\) très faible de 7,7%, attention), mais maintenant, nous faisons apparaitre un effet signicfication du contraste avec smoke1 au seuil alpha de 5%. Et du coup, les effets des deux variables deviennent plus clairs dans notre tableau de l’ANOVA. Le graphique correspondant est l’ajustement de droites parallèles les unes aux autres pour les 4 sous-populations en fonction de smoke. Ce graphique est difficile à réaliser. Il faut ruser, et les détails du code vont au delà de ce cours (il n’est pas nécessaire de les comprendre à ce stade). cols &lt;- iterators::iter(scales::hue_pal()(4)) # Get colors for lines chart(data = Babies, wt ~ wt1) + geom_point(aes(col = smoke)) + lapply(c(0, -0.238, 0.0227, 0.0355), function(offset) geom_smooth(method = lm, formula = y + offset ~ x, col = iterators::nextElem(cols))) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Voyons ce que donne l’analyse post hoc des comparaisons multiples (nous utilisons ici simplement le snippet disponible à partir de ... -&gt; hypothesis tests -&gt; hypothesis tests: means -&gt; hmanovamult : anova - multiple comparaisons [multcomp]) que nous avons déjà employé et qui reste valable ici. summary(anovaComp. &lt;- confint(multcomp::glht(Babies_lm2, linfct = multcomp::mcp(smoke = &quot;Tukey&quot;)))) # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lm(formula = wt ~ smoke + wt1, data = Babies) # # Linear Hypotheses: # Estimate Std. Error t value Pr(&gt;|t|) # 1 - 0 == 0 -0.23794 0.03182 -7.478 &lt;1e-04 *** # 2 - 0 == 0 0.02267 0.05651 0.401 0.977 # 3 - 0 == 0 0.03549 0.05407 0.656 0.908 # 2 - 1 == 0 0.26060 0.05704 4.568 &lt;1e-04 *** # 3 - 1 == 0 0.27342 0.05478 4.991 &lt;1e-04 *** # 3 - 2 == 0 0.01282 0.07199 0.178 0.998 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) .oma &lt;- par(oma = c(0, 5.1, 0, 0)); plot(anovaComp.); par(.oma); rm(.oma) Ici, comme nous testons tous les contrastes, nous pouvons dire que la population des mères qui ont fumé pendant la grossesse smoke == 1 donne des bébés significativement moins gros au seuil alpha de 5%, et ce, en comparaison de tous les autres niveaux (mère n’ayant jamais fumé, ou ayant fumé mais arrêté avant la grossesse, que ce soit longtemps avant ou juste avant). En conclusion de cette analyse, nous pouvons dire que la masse du bébé dépend de la masse de la mère, mais assez faiblement (seulement 7,7% de la variance totale expliquée). Par contre, nous pouvons aussi dire que le fait de fumer pendant la grossesse a un effet significatif sur la réduction de la masse du bébé à la naissance (en moyenne cette réduction est de 0,24kg pour une masse moyenne de 3,03kg, soit une réduction de 0,24 / 3,03 * 100 = 8%). Voilà, nous venons d’analyser et d’interpréter notre premier modèle linéaire sous forme d’une ANCOVA. A vous de jouer ! Après cette longue lecture avec énormément de nouvelles matières, nous vous proposons les exercices suivants : Répondez aux questions d’un learnr afin de vérifier vos acquis. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;03a_mod_lin&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Poursuivez l’analyse des données sur la biométrie des oursins en y intégrant vos nouvelles notions sur le modèle linéaire Reprenez votre travail sur la biométrie des oursins et appliquer les nouvelles notions vues Pour rappel, on utilise le signe + pour indiquer un modèle sans interactions et un signe *pour spécifier un modèle complet avec interactions entre les variables.↩ "],
["modele-lineaire-generalise.html", "3.5 Modèle linéaire généralisé", " 3.5 Modèle linéaire généralisé Le modèle linéaire nous a permis de généraliser la régression linéaire multiple (applicable seulement sur des variables quantitatives) à des variables réponses qualitatives grâce aux variables indicatrices \\(I_i\\). Le modèle linéaire généralisé reprend cette idée, mais permet en plus d’avoir d’autres variables dépendantes (ou réponses) que quantitatives, ou avec des distributions des résidus différentes. Dans R, c’est la fonction glm() qui se charge de calculer un modèle linéaire généralisé. Nous rajoutons une fonction de lien f(y) qui transforme la variable initiale en une variable quantitative dont la relation avec les variables explicatives est linéarisée : \\[ f(y) = \\beta_1 + \\beta_2 I_2 + \\beta_3 I_3 + ... + \\beta_k I_k + \\beta_l x_1 + \\beta_m x_2 + ... + \\epsilon \\] Par exemple, pour une variable réponse binaire (distribution binomiale), avec une réponse de type logistique \\[ y = 1/(1 + e^{- \\beta x}) \\] la transformation logit est une bonne fonction de lien : \\(\\ln(y / (1 - y)) = \\beta x\\). 3.5.1 Exemple Recherche d’effet de variables qualitatives et quantitatives sur une réponse binaire : SciViews:: R babies &lt;- read(&quot;babies&quot;, package = &quot;UsingR&quot;) # Transformation : garder aussi la variable &#39;gestation&#39; en jours # et &#39;ht&#39;, la taille de la mère en pouces à convertir en m (/ 39.37) babies %&gt;.% select(., gestation, smoke, wt1, ht) %&gt;.% filter(., gestation &lt; 999, smoke &lt; 9, wt1 &lt; 999, ht &lt; 999) %&gt;.% # Transformer wt1 en kg et ht en cm mutate(., wt1 = wt1 * 0.4536) %&gt;.% mutate(., ht = ht / 39.37) -&gt; Babies_prem Babies_prem$smoke &lt;- as.factor(Babies_prem$smoke) # Déterminer quels sont les enfants prématurés (nés avant 37 semaines) Babies_prem$premat &lt;- as.factor(as.numeric(Babies_prem$gestation &lt; 7*37)) # BMI peut être plus parlant que la masse pour la mère? Babies_prem %&gt;.% mutate(bmi = wt1 / ht^2) -&gt; Babies_prem # Modèle linéaire généralisé avec fonction de lien de type logit summary(glm(data = Babies_prem, premat ~ smoke + bmi, family = binomial(link = logit))) A vous de jouer ! Réalisez un rapport scientifique sur la maturation d’ovocytes. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/mXAIu4Ir Lisez le README afin de prendre connaissance de l’exercice "],
["reg-non-lin.html", "Module 4 Régression non linéaire", " Module 4 Régression non linéaire Objectifs TODO Prérequis TODO "],
["hierarchique.html", "Module 5 Classification hiérarchique", " Module 5 Classification hiérarchique Objectifs TODO Prérequis TODO "],
["k-moyenne-som.html", "Module 6 K-moyenne &amp; SOM", " Module 6 K-moyenne &amp; SOM Objectifs TODO Prérequis TODO "],
["k-moyennes.html", "6.1 K-moyennes", " 6.1 K-moyennes "],
["cartes-auto-adaptatives.html", "6.2 Cartes auto adaptatives", " 6.2 Cartes auto adaptatives La méthode des cartes auto-adaptatives se nomme self-organizing map (SOM) en anglais. "],
["acp-afc.html", "Module 7 ACP &amp; AFC", " Module 7 ACP &amp; AFC Objectifs TODO Prérequis TODO "],
["analyse-en-composantes-principales.html", "7.1 Analyse en composantes principales", " 7.1 Analyse en composantes principales "],
["analyse-factorielle-des-correspondances.html", "7.2 Analyse factorielle des correspondances", " 7.2 Analyse factorielle des correspondances "],
["afm.html", "Module 8 AFM", " Module 8 AFM Objectifs TODO Prérequis TODO "],
["analyse-factorielle-multiple-afm.html", "8.1 Analyse factorielle multiple (AFM)", " 8.1 Analyse factorielle multiple (AFM) L’analyse factorielle multiple (AFM) se nomme principal component analysis (PCA) en anglais. "],
["svbox.html", "A Installation de la SciViews Box", " A Installation de la SciViews Box Pour ce cours SDD 2, nous utiliserons la même SciViews Box que pour le cours 1… mais actualisée (version de l’année). Vous allez donc devoir installer la nouvelle version. La procédure n’a changé que sur des points de détails. Référez-vous à l’appendice A1 du cours SDD 1. Vous pouvez conserver l’ancienne SciViews Box en parallèle avec cette nouvelle version, mais vérifiez si vous avez assez d’espace sur le disque dur pour contenir les deux simultanément. Comptez par sécurité 20Go par version. Si vous manquez de place, vous pouvez éliminer l’ancienne version avant d’installer la nouvelle (vos projets ne seront pas effacés). "],
["migration-des-projets.html", "A.1 Migration des projets", " A.1 Migration des projets Concernant les projets réalisés dans une version précédente de la SciViews Box, ceux-ci restent disponibles, même si vous éliminez l’ancienne. Plusieurs cas de figure se présentent : Vous conserver deux ou plusieurs version de la SciViews Box en parallèle. Dans ce cas, nous conseillons fortement de garder chaque projet accessible à partir de la version dans laquelle il a été créé. Seulement les projets que vous décidez de migrer explicitement (voir ci-dessous) seront à déplacer dans le dossier shared de la nouvelle SciViews Box. Vous aurez à faire cette manipulation, par exemple, si vous devez recommencer un cours l’année suivante afin d’être en phase (même version de la svbox) par rapport à vos nouveaux collègues. Vous ne conservez que la dernière version de la SciViews Box, mais ne devez pas accéder fréquemment vos anciens projets, et dans ce cas, vous pouvez réinstaller temporairement l’ancienne version de svbox. Dans ce cas, ne migrez pas vos anciens projets. Éliminez simplement l’ancienne svbox, tout en laisant vos projets intacts dans son répertoire shared. Lors de la réinstallation de l’ancienne svbox, vous retrouverez alors tous vos anciens projets intactes. Vous ne conservez pas d’ancienne version de la svbox et vous ne souhaitez pas devoir la réinstaller. Il est possible de migrer vos anciens projets en les déplaçant de l’ancien répertoire shared vers le nouveau. Soyez toutefois conscients que vos documents R Markdown et scripts R ne fonctionneront pas forcément dans la nouvelle svbox et qu’une adaptation sera peut-être nécessaire ! "],
["configuration-git-et-github.html", "A.2 Configuration Git et Github", " A.2 Configuration Git et Github A chaque nouvelle installation de la SciViews Box, vous devez la reconfigurer via la boite de dialogue SciViews Box Configuration. En particulier, il est très important d’indiquer correctement votre identifiant et email Git (zone encadrée en rouge dans la copie d’écran ci-dessous). Assurez-vous (si ce n’est déjà fait) que vous possédez un compte Github valide. Vous pouvez cliquer sur le bouton Go to Github par facilté dans la même boite de dialogue. Choisissez de manière judicieuse votre login. Vous pourriez être amenés à l’utiliser bien plus longtemps que vous ne le pensez, y compris plus tard dans votre carrière. Donc, lisez les conseils ci-dessous (inspirés et adaptés de Happy Git and Github for the UseR - Register a Github Account : Incluez votre nom réel. Les gens aiment savoir à qui ils ont affaire. Rendez aussi votre nom/login facile à deviner et à retenir. Philippe Grosjean a comme login phgrosjean, par exemple. Vous pouvez réutiliser votre login d’autres contextes, par exemple Twitter ou Slack (ou Facebook). Choisissez un login que vous pourrez échanger de manière confortable avec votre futur boss. Un login plus court est préférable. Soyez unique dans votre login, mais à l’aide d’aussi peu de caractères que possible. Github propose parfois des logins en auto-complétion. Examinez ce qu’il propose. Rendez votre login invariable dans le temps. Par exemple, n’utilisez pas un login lié à votre université (numéro de matricule, ou nom de l’université inclue dans le login). Si tout va bien votre login vous suivra dans votre carrière, … donc, potentiellement loin de l’université où vous avez fait vos études. N’utilisez pas de logins qui sont aussi des mots ayant une signification particulière en programmation, par exemple, n’utilisez pas NA, même si c’est vos initiales ! Une fois votre compte Github créé, et votre login/email pour votre identification Git correctement enregistrés dans la SciViews Box, vous devez pouvoir travailler, faire des “pushs”, des “pulls” et des “commits”8. Cependant, RStudio vous demandera constamment vos logins et mots de passe… à la longue, c’est lassant ! La procédure ci-dessous vous enregistre une fois pour toutes sur votre compte Github dans RStudio. A.2.1 Compte Github dans RStudio RStudio offre la possibilité d’enregistrer une clé publique/privée dans votre SciViews Box afin de vous enregistrer sur Github de manière permanente. L’avantage, c’est que vous ne devrez plus constamment entrer votre login et mot de passe à chaque opération sur Github ! Nous vous le conseillons donc vivement. Entrez dans Rstudio Server, et allez dans le menu Tools -&gt; Global Options.... Ensuite, cliquez dans la rubrique Git/SVN dans la boite de dialogue. Ensuite, cliquez sur le bouton Create RSA key.... La phrase de passe n’est pas nécessaire (il est même préférable de la laisser vide si vous voulez utiliser Github sans rien devoir taper à chaque fois). Cliquez sur le bouton Create. Vous obtenez alors une fenêtre similaire à celle ci-dessous (bien sûr avec des données différentes). Ceci confirme que votre clé cryptographique a été créée localement. Fermez cette fenêtre pour revenir à la boite de dialogue de configuration de RStudio Server. Dans la boite de dialogue de configuration de RStudio Server, section Git/SVN cliquez sur le lien View public key qui apparait une fois la clé créée : La clé apparait dans une fenêtre, déjà présélectionnée. Copiez-là dans le presse-papier (Ctrl-C ou clic bouton droit et sélection de Copy dans le menu contextuel), puis fermez cette fenêtre. Dans votre navigateur web favori, naviguez vers https://github.com, loggez-vous, et accédez aux paramètres de votre compte Github (menu déroulant en haut à droite, entrée Settings) : Dans les paramètres de votre compte, cliquez sur la rubrique SSH and GPG keys, ensuite sur le bouton vert New SSH key Collez-y votre clé à partir du presse-papier dans la zone Key. Vous pouvez lui donner un nom évocateur dans le champ Title. Ensuite, cliquez sur Add SSH key. Déloggez, puis reloggez-vous dans RStudio Server pour que les changements soient pris en compte. La prochaine action sur Github depuis RStudio pourrait encore déclencher la demande de votre login et mot de passe, mais ensuite, les opérations devraient se faire directement. Si vous éprouvez toujours des difficultés à faire collaborer R et RStudio avec Git et Github, voyez https://happygitwithr.com (en anglais) qui explique les différentes procédures bien plus en détails. Vérifiez toujours lors de votre premier commit que Github vous reconnait bien. Pour cela, naviguez vers le dépôt où vous avez commité avec votre explorateur web, et vérifiez l’identité prise en compte lors de votre commit.↩ "],
["references.html", "Références", " Références "]
]
